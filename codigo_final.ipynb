{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLWcfT_pJB0E"
      },
      "source": [
        "#Importando dados\n",
        "\n",
        "Começamos importando todos os dados necessarios para nosso treinamento, assim como as bibliotecas que serão usadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 909,
      "metadata": {
        "id": "jkJ_qogiJB0K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "def normalize(a, axis=-1, order=2):\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2==0] = 1\n",
        "    return (a / np.expand_dims(l2, axis)).transpose()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 910,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v8WBF2pJB0M",
        "outputId": "0315d7f2-2b18-429f-840a-5703724f9101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantidade de dados:  2000\n",
            "Quantidade de dados de treino 500\n",
            "(2000, 144)\n",
            "(2000,)\n",
            "(2000,)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "scaler=StandardScaler()\n",
        "x_data = pd.read_csv(\"inputs_finais.csv\",skiprows=1,header=None)\n",
        "y_data = pd.read_csv(\"targets_finais.csv\")\n",
        "qte_total=np.shape(y_data)[0]\n",
        "qte_training=500\n",
        "x_data = np.array(x_data.iloc[:qte_total,:])\n",
        "x_data=scaler.fit_transform(x_data)\n",
        "#_data = (x_data - np.mean(x_data,axis=0))/np.std(x_data,axis=0)\n",
        "y_fopt = np.array(y_data.iloc[:,0])\n",
        "y_fwpt = np.array(y_data.iloc[:,1])\n",
        "plt.ecdf(y_fopt)\n",
        "\n",
        "y_fopt = (y_fopt - np.mean(y_fopt))/np.std(y_fopt)\n",
        "y_fwpt = (y_fwpt - np.mean(y_fwpt))/np.std(y_fwpt)\n",
        "\n",
        "\n",
        "#Vamos permutar as matrizes\n",
        "\n",
        "n_rows = qte_total\n",
        "permutation = np.random.permutation(n_rows)\n",
        "\n",
        "x_data=x_data[permutation]\n",
        "y_fopt = y_fopt[permutation]\n",
        "y_fwpt=y_fwpt[permutation]\n",
        "print(\"Quantidade de dados: \",qte_total)\n",
        "print(\"Quantidade de dados de treino\",qte_training)\n",
        "print(np.shape(x_data))\n",
        "print(np.shape(y_fopt))\n",
        "print(np.shape(y_fwpt))\n",
        "#print(x_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KE-zEAyJB0O"
      },
      "source": [
        "Separamos os dados de treino e de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 911,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKTUnt0NJB0O",
        "outputId": "2770b8d6-6593-4228-933a-87a2b311d0e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 144)\n",
            "(500,)\n",
            "(1500, 144)\n",
            "(1500,)\n",
            "0.6409747066651207\n",
            "(500, 1)\n",
            "[[ -0.78982123]\n",
            " [ 12.53557992]\n",
            " [ 12.53416558]\n",
            " ...\n",
            " [ -0.75931336]\n",
            " [ -0.78734605]\n",
            " [-11.02409121]]\n",
            "[[ 12.51405351]\n",
            " [ -0.78709725]\n",
            " [-10.98915686]\n",
            " [ -0.77582148]\n",
            " [ 12.54199702]\n",
            " [-10.99664781]\n",
            " [ -0.77956696]\n",
            " [ -0.78734605]\n",
            " [-11.03260364]\n",
            " [ -0.7942477 ]\n",
            " [ -0.77514049]\n",
            " [ -0.75556788]\n",
            " [-11.033949  ]\n",
            " [ -0.77207601]\n",
            " [-10.99392383]\n",
            " [ -0.77479999]\n",
            " [ -0.79458819]\n",
            " [ -0.78126944]\n",
            " [ 12.5256138 ]\n",
            " [ -0.77888596]\n",
            " [-11.03905647]\n",
            " [ -0.803084  ]\n",
            " [ -0.774119  ]\n",
            " [ -0.78948073]\n",
            " [ -0.78121709]\n",
            " [ 12.52118733]\n",
            " [-10.9976693 ]\n",
            " [-11.01009146]\n",
            " [ 12.52629479]\n",
            " [ -0.77990745]\n",
            " [-10.98915686]\n",
            " [ -0.79152372]\n",
            " [-10.98915686]\n",
            " [-11.03088453]\n",
            " [ 12.56191013]\n",
            " [-11.03224652]\n",
            " [-10.99494532]\n",
            " [-11.01792291]\n",
            " [-11.03735398]\n",
            " [ 12.53961354]\n",
            " [ 12.55646217]\n",
            " [ -0.7525034 ]\n",
            " [ -0.77684298]\n",
            " [ -0.77718347]\n",
            " [-11.01145345]\n",
            " [ -0.78092895]\n",
            " [ 12.54199702]\n",
            " [ 12.52018247]\n",
            " [ -0.79084272]\n",
            " [ -0.75522738]\n",
            " [-11.01514658]\n",
            " [-11.02613419]\n",
            " [ -0.80240301]\n",
            " [-11.01417743]\n",
            " [ -0.76101584]\n",
            " [ -0.78126944]\n",
            " [-11.0247722 ]\n",
            " [-10.98949736]\n",
            " [-11.0251127 ]\n",
            " [ 12.5256138 ]\n",
            " [ -0.78155759]\n",
            " [ 12.52740798]\n",
            " [ 12.53995404]\n",
            " [ 12.56259112]\n",
            " [-11.01792291]\n",
            " [ 12.53523942]\n",
            " [-11.03973746]\n",
            " [ 12.54267802]\n",
            " [-10.98813537]\n",
            " [-10.9976693 ]\n",
            " [ 12.52842947]\n",
            " [-10.99630731]\n",
            " [ -0.78845924]\n",
            " [ 12.53927304]\n",
            " [ 12.55339769]\n",
            " [-11.0342895 ]\n",
            " [-11.01383693]\n",
            " [-11.02715568]\n",
            " [ -0.7531844 ]\n",
            " [ -0.79389057]\n",
            " [ -0.79457156]\n",
            " [-11.00975096]\n",
            " [ -0.76067535]\n",
            " [ -0.7730975 ]\n",
            " [-11.02195653]\n",
            " [-11.02365901]\n",
            " [ -0.77275701]\n",
            " [-11.01485843]\n",
            " [ -0.78811874]\n",
            " [ -0.78948073]\n",
            " [ 12.53586807]\n",
            " [ -0.75216291]\n",
            " [-11.03054403]\n",
            " [ -0.7528439 ]\n",
            " [ 12.55748366]\n",
            " [ 12.55441918]\n",
            " [-11.03837547]\n",
            " [ 12.55612167]\n",
            " [-11.02647469]\n",
            " [ -0.78598406]\n",
            " [ -0.78675675]\n",
            " [ 12.51201053]\n",
            " [ -0.77990745]\n",
            " [-11.03224652]\n",
            " [-11.03871597]\n",
            " [ -0.77582148]\n",
            " [ -0.7808766 ]\n",
            " [ -0.75761087]\n",
            " [ -0.78326008]\n",
            " [ 12.53421793]\n",
            " [-11.03088453]\n",
            " [ 12.55918615]\n",
            " [ 12.55373819]\n",
            " [ -0.79595018]\n",
            " [ 12.55986714]\n",
            " [ -0.75999435]\n",
            " [ 12.51982534]\n",
            " [ -0.77917411]\n",
            " [ 12.53285594]\n",
            " [-11.01417743]\n",
            " [ 12.52052297]\n",
            " [ 12.51882048]\n",
            " [ -0.79288571]\n",
            " [-11.03735398]\n",
            " [ 12.55544068]\n",
            " [ -0.79933853]\n",
            " [ -0.77752397]\n",
            " [-11.01622042]\n",
            " [ -0.7531844 ]\n",
            " [ 12.53489893]\n",
            " [-11.01548707]\n",
            " [-11.03088453]\n",
            " [ 12.54063503]\n",
            " [-11.01145345]\n",
            " [-10.98881637]\n",
            " [ 12.51337252]\n",
            " [-11.03190602]\n",
            " [ 12.55748366]\n",
            " [ 12.515756  ]\n",
            " [ 12.51984197]\n",
            " [-11.033949  ]\n",
            " [-11.00872947]\n",
            " [-11.01991354]\n",
            " [ -0.75965385]\n",
            " [ 12.53353694]\n",
            " [ 12.52979146]\n",
            " [-11.00906997]\n",
            " [-11.02681519]\n",
            " [ 12.53183445]\n",
            " [ -0.77650248]\n",
            " [-11.01690141]\n",
            " [ 12.52391131]\n",
            " [ 12.52774848]\n",
            " [-11.03326801]\n",
            " [ -0.77241651]\n",
            " [-10.99085935]\n",
            " [ 12.51779899]\n",
            " [ -0.7525034 ]\n",
            " [ -0.80070052]\n",
            " [ -0.7932262 ]\n",
            " [ -0.78845924]\n",
            " [ 12.52425181]\n",
            " [-11.02409121]\n",
            " [ 12.52086346]\n",
            " [ 12.54199702]\n",
            " [ 12.51950147]\n",
            " [ 12.52323032]\n",
            " [-11.0254532 ]\n",
            " [ 12.51846335]\n",
            " [ -0.75454639]\n",
            " [ 12.52842947]\n",
            " [-11.01077246]\n",
            " [-11.03973746]\n",
            " [ -0.77582148]\n",
            " [ -0.79627405]\n",
            " [ 12.53421793]\n",
            " [-11.01417743]\n",
            " [-11.01111295]\n",
            " [ 12.55816466]\n",
            " [ -0.75624888]\n",
            " [-10.99051886]\n",
            " [ -0.78428157]\n",
            " [-11.0342895 ]\n",
            " [-10.99494532]\n",
            " [-11.01009146]\n",
            " [-11.03260364]\n",
            " [ 12.51132953]\n",
            " [ 12.55918615]\n",
            " [ -0.7528439 ]\n",
            " [ -0.79560969]\n",
            " [ 12.55816466]\n",
            " [-11.02409121]\n",
            " [ -0.77815262]\n",
            " [ -0.79525256]\n",
            " [ 12.51779899]\n",
            " [-11.02817718]\n",
            " [ 12.55612167]\n",
            " [ -0.77548099]\n",
            " [-11.01281544]\n",
            " [ 12.56191013]\n",
            " [ -0.77582148]\n",
            " [ 12.52050634]\n",
            " [ -0.77747162]\n",
            " [-11.02987966]\n",
            " [-11.02579369]\n",
            " [ -0.77990745]\n",
            " [ -0.79118322]\n",
            " [ 12.53013196]\n",
            " [-11.03871597]\n",
            " [-11.01043196]\n",
            " [-11.00975096]\n",
            " [-10.98983786]\n",
            " [-10.9976693 ]\n",
            " [-11.03905647]\n",
            " [ 12.53893255]\n",
            " [ -0.77479999]\n",
            " [-11.01383693]\n",
            " [ 12.51132953]\n",
            " [ 12.52945097]\n",
            " [ 12.51846335]\n",
            " [ 12.53518707]\n",
            " [-10.99188085]\n",
            " [-11.03463   ]\n",
            " [ -0.79559306]\n",
            " [ 12.51982534]\n",
            " [ 12.53421793]\n",
            " [-11.02229702]\n",
            " [ -0.80240301]\n",
            " [ 12.56191013]\n",
            " [ -0.78155759]\n",
            " [-10.99119985]\n",
            " [ 12.52018247]\n",
            " [ 12.53387743]\n",
            " [ 12.55339769]\n",
            " [-11.02409121]\n",
            " [ 12.51948484]\n",
            " [ 12.53183445]\n",
            " [ 12.55748366]\n",
            " [ -0.77479999]\n",
            " [ -0.78734605]\n",
            " [ -0.75590838]\n",
            " [ -0.78700555]\n",
            " [ 12.51916098]\n",
            " [ 12.5259543 ]\n",
            " [ 12.56156963]\n",
            " [ 12.54097553]\n",
            " [ 12.53251544]\n",
            " [ 12.55918615]\n",
            " [-10.99324284]\n",
            " [-11.03158215]\n",
            " [ -0.78121709]\n",
            " [ -0.80240301]\n",
            " [ -0.75522738]\n",
            " [-10.98881637]\n",
            " [ 12.55884565]\n",
            " [ -0.773438  ]\n",
            " [-11.03088453]\n",
            " [ -0.76033485]\n",
            " [ 12.53961354]\n",
            " [ -0.7528439 ]\n",
            " [ 12.56156963]\n",
            " [ 12.52050634]\n",
            " [-11.00941047]\n",
            " [-11.01957304]\n",
            " [ 12.53421793]\n",
            " [-10.99222134]\n",
            " [ -0.8027435 ]\n",
            " [ 12.53348459]\n",
            " [-11.03124165]\n",
            " [ -0.78189809]\n",
            " [-11.02715568]\n",
            " [ -0.78126944]\n",
            " [ -0.78675675]\n",
            " [-11.01514658]\n",
            " [-11.01077246]\n",
            " [ -0.78743774]\n",
            " [ -0.78743774]\n",
            " [-11.03260364]\n",
            " [ 12.53382508]\n",
            " [ 12.54029454]\n",
            " [ 12.52084683]\n",
            " [ 12.51711799]\n",
            " [-11.00941047]\n",
            " [ -0.78879973]\n",
            " [ 12.51201053]\n",
            " [-10.99698831]\n",
            " [ -0.75216291]\n",
            " [ 12.51337252]\n",
            " [ 12.55339769]\n",
            " [ -0.7808766 ]\n",
            " [ -0.77854546]\n",
            " [ -0.79254521]\n",
            " [-10.99426433]\n",
            " [-11.03158215]\n",
            " [ -0.79389057]\n",
            " [-11.01383693]\n",
            " [ -0.78126944]\n",
            " [ -0.77548099]\n",
            " [ 12.55339769]\n",
            " [ -0.79457156]\n",
            " [ -0.77479999]\n",
            " [-10.99698831]\n",
            " [-11.033949  ]\n",
            " [-11.02059454]\n",
            " [ 12.56156963]\n",
            " [ -0.75829186]\n",
            " [-11.01753006]\n",
            " [ 12.51984197]\n",
            " [-10.99392383]\n",
            " [-11.03973746]\n",
            " [ -0.75556788]\n",
            " [-11.01787056]\n",
            " [-11.01514658]\n",
            " [ -0.77883361]\n",
            " [-11.03260364]\n",
            " [-11.03701348]\n",
            " [ 12.52391131]\n",
            " [ 12.53387743]\n",
            " [-11.01758241]\n",
            " [-10.99698831]\n",
            " [ 12.53149395]\n",
            " [ -0.78121709]\n",
            " [ -0.7942477 ]\n",
            " [ 12.53251544]\n",
            " [ -0.79831704]\n",
            " [-11.02851767]\n",
            " [ -0.75829186]\n",
            " [-10.99664781]\n",
            " [ -0.78743774]\n",
            " [ 12.53416558]\n",
            " [ 12.515756  ]\n",
            " [-11.0244317 ]\n",
            " [ 12.51882048]\n",
            " [ -0.78058845]\n",
            " [ -0.75692987]\n",
            " [-11.01514658]\n",
            " [ -0.78700555]\n",
            " [-11.03531099]\n",
            " [ 12.55952665]\n",
            " [ -0.78845924]\n",
            " [-11.02919867]\n",
            " [ -0.77922646]\n",
            " [ -0.79491206]\n",
            " [-11.03122502]\n",
            " [-11.00975096]\n",
            " [ 12.54165653]\n",
            " [-11.02613419]\n",
            " [ 12.51846335]\n",
            " [ -0.77207601]\n",
            " [-11.01753006]\n",
            " [ 12.52629479]\n",
            " [ 12.52911047]\n",
            " [ -0.79595018]\n",
            " [ 12.52118733]\n",
            " [-11.03226315]\n",
            " [ -0.7935667 ]\n",
            " [ 12.51201053]\n",
            " [-11.01412508]\n",
            " [ -0.78189809]\n",
            " [-11.00906997]\n",
            " [-10.99732881]\n",
            " [-11.03565149]\n",
            " [-11.03939696]\n",
            " [ -0.79559306]\n",
            " [-11.03122502]\n",
            " [-11.01145345]\n",
            " [ 12.53592042]\n",
            " [-11.02399951]\n",
            " [ -0.75216291]\n",
            " [-11.03326801]\n",
            " [ 12.53421793]\n",
            " [ 12.56225063]\n",
            " [ 12.51880385]\n",
            " [ -0.7808766 ]\n",
            " [-10.99732881]\n",
            " [-11.01451793]\n",
            " [ 12.52876997]\n",
            " [ 12.54029454]\n",
            " [ 12.52629479]\n",
            " [ 12.53421793]\n",
            " [-11.01111295]\n",
            " [ -0.75454639]\n",
            " [ -0.77815262]\n",
            " [ -0.78291958]\n",
            " [ 12.515075  ]\n",
            " [ -0.78155759]\n",
            " [ -0.79729554]\n",
            " [ -0.79457156]\n",
            " [ 12.52731629]\n",
            " [ -0.80070052]\n",
            " [-11.03158215]\n",
            " [ 12.55510018]\n",
            " [-10.99051886]\n",
            " [-11.03122502]\n",
            " [-11.01582757]\n",
            " [ -0.79492869]\n",
            " [ -0.75624888]\n",
            " [ 12.52016584]\n",
            " [ -0.79831704]\n",
            " [-11.03803497]\n",
            " [ -0.75897286]\n",
            " [-11.01684906]\n",
            " [-11.01043196]\n",
            " [ 12.51779899]\n",
            " [ -0.77917411]\n",
            " [ 12.55714316]\n",
            " [ -0.78223859]\n",
            " [ -0.80001952]\n",
            " [ 12.51132953]\n",
            " [ -0.75148191]\n",
            " [ 12.51473451]\n",
            " [ -0.78879973]\n",
            " [-11.01383693]\n",
            " [ -0.7801956 ]\n",
            " [ 12.5252733 ]\n",
            " [ 12.52808898]\n",
            " [ 12.52629479]\n",
            " [-11.03531099]\n",
            " [ -0.80206251]\n",
            " [ -0.78777824]\n",
            " [ 12.52629479]\n",
            " [ -0.78155759]\n",
            " [-11.03124165]\n",
            " [ 12.55578117]\n",
            " [ -0.78257909]\n",
            " [ -0.7808766 ]\n",
            " [-10.99222134]\n",
            " [-11.03973746]\n",
            " [-11.01753006]\n",
            " [ -0.80172201]\n",
            " [ -0.803084  ]\n",
            " [ -0.77684298]\n",
            " [ 12.56259112]\n",
            " [ 12.51916098]\n",
            " [ -0.7801956 ]\n",
            " [ -0.79525256]\n",
            " [-11.02375071]\n",
            " [ 12.55952665]\n",
            " [ 12.52425181]\n",
            " [ 12.53183445]\n",
            " [ 12.53251544]\n",
            " [ 12.53149395]\n",
            " [ 12.55782416]\n",
            " [ -0.77888596]\n",
            " [ -0.75522738]\n",
            " [ 12.56191013]\n",
            " [ 12.55918615]\n",
            " [ 12.53592042]\n",
            " [ -0.75590838]\n",
            " [-11.02297802]\n",
            " [-10.99630731]\n",
            " [-11.02986303]\n",
            " [ 12.51439401]\n",
            " [-11.01923255]\n",
            " [ -0.7935667 ]\n",
            " [-11.02297802]\n",
            " [-11.01792291]\n",
            " [ -0.78092895]\n",
            " [-10.99051886]\n",
            " [-11.01650857]\n",
            " [ 12.53523942]\n",
            " [ -0.78496257]\n",
            " [ 12.52663529]\n",
            " [ -0.79288571]\n",
            " [ 12.53825155]\n",
            " [ 12.53518707]\n",
            " [ -0.78914023]\n",
            " [ 12.52254932]\n",
            " [ -0.79525256]\n",
            " [-11.03260364]\n",
            " [ -0.79560969]\n",
            " [ -0.77275701]\n",
            " [ 12.53416558]\n",
            " [-11.01616807]\n",
            " [ -0.7730975 ]\n",
            " [ 12.5252733 ]\n",
            " [ -0.78126944]\n",
            " [-11.01724191]\n",
            " [-10.99085935]\n",
            " [-10.99358333]\n",
            " [-11.01417743]\n",
            " [ 12.53557992]\n",
            " [ 12.52018247]\n",
            " [ -0.77548099]\n",
            " [-11.03260364]\n",
            " [-11.02272922]\n",
            " [ -0.79050222]\n",
            " [ -0.76101584]\n",
            " [-11.00941047]\n",
            " [ 12.56088864]\n",
            " [ 12.56122913]\n",
            " [ -0.80104102]\n",
            " [ 12.53859205]\n",
            " [-11.01519893]\n",
            " [ 12.53592042]\n",
            " [ 12.55339769]\n",
            " [ -0.78777824]\n",
            " [-11.03090116]\n",
            " [-11.02375071]\n",
            " [ 12.52018247]]\n"
          ]
        }
      ],
      "source": [
        "x_train_data = x_data[:qte_training]\n",
        "y_train_fopt = y_fopt[:qte_training]\n",
        "y_train_fwpt = y_fwpt[:qte_training]\n",
        "y_train_data = np.column_stack((y_train_fopt,y_train_fwpt))\n",
        "\n",
        "x_test_data = x_data[qte_training:]\n",
        "y_test_fopt = y_fopt[qte_training:]\n",
        "y_test_fwpt = y_fwpt[qte_training:]\n",
        "y_test_data = np.column_stack((y_test_fopt,y_test_fwpt))\n",
        "\n",
        "print(np.shape(x_train_data))\n",
        "print(np.shape(y_train_fopt))\n",
        "print(np.shape(x_test_data))\n",
        "print(np.shape(y_test_fopt))\n",
        "#Aqui retiramos apenas as características boas\n",
        "\n",
        "num_features=1\n",
        "pca=PCA(n_components=num_features)\n",
        "\n",
        "pca.fit(x_train_data)\n",
        "#print(pca.singular_values_)\n",
        "#vemos que so os primeiros 7 realmente tem alguma importancia na reprensetatividade do problema, logo:\n",
        "\n",
        "x_train_data=pca.fit_transform(x_train_data)\n",
        "x_test_data = pca.transform(x_test_data)\n",
        "print(sum(pca.explained_variance_ratio_))\n",
        "print(np.shape(x_train_data))\n",
        "#print(np.shape(x_transformed_pca))\n",
        "\n",
        "print(x_test_data)\n",
        "print(x_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2J0YLL1JB0P"
      },
      "source": [
        "#Fit do polinômio\n",
        "\n",
        "Agora criamos o polinômio que ajustaremos, com o mesmo formato daquele mencionado no relatório: um polinomio de grau 2 com interações entre os parâmetros.\n",
        "O metodo fit_transform ja faz a transformação nos inputs , nos dando um novo conjunto com os valores que usaremos no ajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 912,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZgFeyY1JB0P",
        "outputId": "6d4b4884-4bbb-4875-bdfc-7e5b40741b7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 2)"
            ]
          },
          "execution_count": 912,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "poly=PolynomialFeatures(2,include_bias=False)\n",
        "\n",
        "\n",
        "x_training_transformed=poly.fit_transform(x_train_data)\n",
        "#x_training_transformed = (x_training_transformed - np.mean(x_training_transformed))/np.std(x_training_transformed)\n",
        "np.shape(x_training_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kThLeTDJB0Q"
      },
      "source": [
        "Fazemos o ajuste, usando modelos com 50, 100 e 150 dados de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 913,
      "metadata": {
        "id": "45PFOuxsJB0Q"
      },
      "outputs": [],
      "source": [
        "poly_50_fopt = LinearRegression().fit(x_training_transformed[:qte_training//3],y_train_fopt[:qte_training//3])\n",
        "poly_100_fopt = LinearRegression().fit(x_training_transformed[:qte_training//2],y_train_fopt[:qte_training//2])\n",
        "poly_150_fopt = LinearRegression().fit(x_training_transformed[:qte_training],y_train_fopt[:qte_training])\n",
        "\n",
        "poly_50_fwpt = LinearRegression().fit(x_training_transformed[:qte_training//3],y_train_fwpt[:qte_training//3])\n",
        "poly_100_fwpt = LinearRegression().fit(x_training_transformed[:qte_training//2],y_train_fwpt[:qte_training//2])\n",
        "poly_150_fwpt = LinearRegression().fit(x_training_transformed[:qte_training],y_train_fwpt[:qte_training])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X9TMKj7JB0Q"
      },
      "source": [
        "Agora fazemos a predição, lembrando do fato de que precisamos transformar também os dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 914,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyKVUIfzJB0Q",
        "outputId": "d41df770-9533-4404-a6fd-d5810ce57c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00695586 -0.21953644 -0.21949652 ...  0.00677905  0.00694157\n",
            " -0.02224592]\n",
            "[ 0.14466313 -0.15421034 -0.26860183 ... -0.12361028  0.75641792\n",
            "  0.54658234]\n"
          ]
        }
      ],
      "source": [
        "x_test_transformed = poly.fit_transform(x_test_data)\n",
        "\n",
        "ypred_50_fopt=poly_50_fopt.predict(x_test_transformed)\n",
        "ypred_100_fopt=poly_100_fopt.predict(x_test_transformed)\n",
        "ypred_150_fopt=poly_150_fopt.predict(x_test_transformed)\n",
        "\n",
        "ypred_50_fwpt=poly_50_fwpt.predict(x_test_transformed)\n",
        "ypred_100_fwpt=poly_100_fwpt.predict(x_test_transformed)\n",
        "ypred_150_fwpt=poly_150_fwpt.predict(x_test_transformed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.shape(ypred_50_fopt)\n",
        "\n",
        "print(ypred_50_fopt)\n",
        "print(y_fopt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI2Bcw6KJB0R"
      },
      "source": [
        "##Gráficos e resultados do fit.\n",
        "Plotamos as curvas da função cumulativa e os desvios médios/máximos de cada polinomio preditor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 915,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "0SRhw0p3JB0R",
        "outputId": "d6ffd552-0dc0-43cd-a192-30fe186d82f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'CDF da Prod. Total de água')"
            ]
          },
          "execution_count": 915,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,4.8))\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "# axs[0].ecdf(y_test_fopt, label='sim')\n",
        "# axs[0].ecdf(ypred_50_fopt, label = 't50')\n",
        "# axs[0].ecdf(ypred_100_fopt,label = 't100')\n",
        "# axs[0].ecdf(ypred_150_fopt, label='t150')\n",
        "# axs[0].legend()\n",
        "# axs[0].grid()\n",
        "# axs[0].set_xlabel(\"FOPT \")\n",
        "# axs[0].set_ylabel(\"CDF\")\n",
        "# #axs[0].set_yticks([-0.2,1.1])\n",
        "# axs[0].set_title(\"CDF da Prod. Total de petróleo\")\n",
        "\n",
        "axs[0].scatter(np.sort(y_test_fopt),np.arange(len(np.sort(y_test_fopt)))/float(len(np.sort(y_test_fopt))), label='sim',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(ypred_50_fopt),np.arange(len(np.sort(ypred_50_fopt)))/float(len(np.sort(ypred_50_fopt))), label='t50',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(ypred_100_fopt),np.arange(len(np.sort(ypred_100_fopt)))/float(len(np.sort(ypred_100_fopt))), label='t100',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(ypred_150_fopt),np.arange(len(np.sort(ypred_150_fopt)))/float(len(np.sort(ypred_150_fopt))), label='t150',linewidths=0.3)\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "axs[0].set_xlabel(\"FOPT \")\n",
        "axs[0].set_ylabel(\"CDF\")\n",
        "axs[0].set_ylim([0,1])\n",
        "axs[0].set_xlim([-2,2])\n",
        "axs[0].set_title(\"CDF da Prod. Total de petróleo\")\n",
        "\n",
        "axs[1].scatter(np.sort(y_test_fwpt),np.arange(len(np.sort(y_test_fwpt)))/float(len(np.sort(y_test_fwpt))), label='sim',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(ypred_50_fwpt),np.arange(len(np.sort(ypred_50_fwpt)))/float(len(np.sort(ypred_50_fwpt))), label='t50',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(ypred_100_fwpt),np.arange(len(np.sort(ypred_100_fwpt)))/float(len(np.sort(ypred_100_fwpt))), label='t100',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(ypred_150_fwpt),np.arange(len(np.sort(ypred_150_fwpt)))/float(len(np.sort(ypred_150_fwpt))), label='t150',linewidths=0.3)\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "axs[1].set_xlabel(\"FWPT \")\n",
        "axs[1].set_ylabel(\"CDF\")\n",
        "axs[1].set_ylim([0,1])\n",
        "axs[1].set_title(\"CDF da Prod. Total de água\")\n",
        "\n",
        "# axs[1].ecdf(y_test_fwpt,label = 'sim')\n",
        "# axs[1].ecdf(ypred_50_fwpt,label = 't50')\n",
        "# axs[1].ecdf(ypred_100_fwpt,label = 't100')\n",
        "# axs[1].ecdf(ypred_150_fwpt,label = 't150')\n",
        "# axs[1].set_xlabel(\"FWTP\")\n",
        "# axs[1].set_title(\"CDF da Prod. Total de água\")\n",
        "# axs[1].legend()\n",
        "# axs[1].grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uYGbs3-JB0S"
      },
      "source": [
        "E o resultado do desvio que queremos é"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 916,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbfsw9RSJB0S",
        "outputId": "72b87a3f-42d0-45a1-be65-9054a73713ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O valor médio para o caso de 50 treinamentos é: 1.62%\n",
            "O valor médio para o caso de 50 treinamentos é: 1.59%\n",
            "O valor médio para o caso de 100 treinamentos é: 1.63%\n",
            "O valor médio para o caso de 100 treinamentos é: 3.98%\n",
            "O valor médio para o caso de 150 treinamentos é: 1.33%\n",
            "O valor médio para o caso de 150 treinamentos é: 2.13%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "RE_50_fopt =np.abs( np.abs(y_test_fopt - ypred_50_fopt) / y_test_fopt)\n",
        "RE_100_fopt = np.abs(np.abs(y_test_fopt - ypred_100_fopt) / y_test_fopt)\n",
        "RE_150_fopt =np.abs( np.abs(y_test_fopt - ypred_150_fopt) / y_test_fopt)\n",
        "\n",
        "RE_50_fwpt =np.abs( np.abs(y_test_fwpt - ypred_50_fwpt) / y_test_fwpt)\n",
        "RE_100_fwpt = np.abs(np.abs(y_test_fwpt - ypred_100_fwpt) / y_test_fwpt)\n",
        "RE_150_fwpt =np.abs( np.abs(y_test_fwpt - ypred_150_fwpt) / y_test_fwpt)\n",
        "\n",
        "\n",
        "print('O valor médio para o caso de 50 treinamentos é: {:.2f}%'.format(np.mean(RE_50_fopt)))\n",
        "print('O valor médio para o caso de 50 treinamentos é: {:.2f}%'.format(np.mean(RE_50_fwpt)))\n",
        "\n",
        "print('O valor médio para o caso de 100 treinamentos é: {:.2f}%'.format(np.mean(RE_100_fopt)))\n",
        "print('O valor médio para o caso de 100 treinamentos é: {:.2f}%'.format(np.mean(RE_100_fwpt)))\n",
        "\n",
        "print('O valor médio para o caso de 150 treinamentos é: {:.2f}%'.format(np.mean(RE_150_fopt)))\n",
        "print('O valor médio para o caso de 150 treinamentos é: {:.2f}%'.format(np.mean(RE_150_fwpt)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUniqTp9J9tF"
      },
      "source": [
        "#Rede Neural\n",
        "\n",
        "Começamos pela estrutuda da nossa rede. Importamos todos os pacotes necessários:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 917,
      "metadata": {
        "id": "GasBaD-eJPPy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import random\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.functional import kl_div, softmax, log_softmax\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 918,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxwtaxUoJjVO",
        "outputId": "af8ba51d-5e48-4283-85bb-3beeecf8c9e1"
      },
      "outputs": [],
      "source": [
        " qte_input_param=np.shape(x_data)[1]\n",
        "# print(qte_input_param)\n",
        "# class SimpleNetwork(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(SimpleNetwork,self).__init__()\n",
        "#         self.linear1=nn.Linear(qte_input_param, 200)\n",
        "#         self.linear2=nn.Linear(200,50)\n",
        "#         self.linear3=nn.Linear(50,10)\n",
        "#         self.linear4=nn.Linear(10,2)\n",
        "\n",
        "#     def forward(self,x):\n",
        "\n",
        "#         x=F.tanh(self.linear1(x))\n",
        "#         x=F.tanh(self.linear2(x))\n",
        "#         x=F.tanh(self.linear3(x))\n",
        "#         x=self.linear4(x)\n",
        "#         return x\n",
        "\n",
        "class SimpleNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNetwork,self).__init__()\n",
        "        self.linear1=nn.Linear(num_features,5)\n",
        "        self.linear2=nn.Linear(5,7)\n",
        "        self.linear3=nn.Linear(7,2)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        x=F.tanh(self.linear1(x))\n",
        "        x=F.tanh(self.linear2(x))\n",
        "        x=self.linear3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# class SimpleNetwork(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(SimpleNetwork,self).__init__()\n",
        "#         self.linear1=nn.Linear(qte_input_param, 10)\n",
        "#         #self.dropout1=nn.Dropout(p=0.5)\n",
        "#         self.linear2=nn.Linear(10,2)\n",
        "#         #self.dropout2=nn.Dropout(p=0.2)\n",
        "#         #self.linear3=nn.Linear(10,2)\n",
        "\n",
        "#     def forward(self,x):\n",
        "\n",
        "#         x=F.tanh(self.linear1(x))\n",
        "#         x=self.linear2(x)\n",
        "#         #x=self.dropout1(x)\n",
        "#         #x=F.relu(self.linear2(x))\n",
        "#         #x=self.dropout2(x)\n",
        "#         #x=self.linear3(x)\n",
        "#         return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z72MAr65MUya"
      },
      "source": [
        "Fazemos também nosso otimizador e a função de perda e outros himerparâmetros do processo de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 919,
      "metadata": {
        "id": "OP7-TisgMT4Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size=2\n",
        "lr=0.001\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#para variarmos o LR durante o treinamento, fazendo a rede trabalhar na regiao de\n",
        "#minimo da perda\n",
        "def learning_rate_scheduler(init,epoch):\n",
        "  optim_factor=0\n",
        "  if (epoch>50):\n",
        "    optim_factor=3\n",
        "  elif (epoch>20):\n",
        "    optim_factor=2\n",
        "  elif (epoch>10):\n",
        "    optim_factor=1\n",
        "  return init*math.pow(0.1,optim_factor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FeT8OTpKfoS"
      },
      "source": [
        "Temos que fazer também data loaders e uma função de treino. Também passamos os dados pra um formato convencional do PyTorch:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 920,
      "metadata": {
        "id": "JAi1AaiaKfEx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.77527663 -0.04434602]\n",
            " [ 0.19047397  2.29818459]\n",
            " [ 0.38399086  0.60202405]\n",
            " ...\n",
            " [-0.12361028 -0.63345603]\n",
            " [ 0.75641792  0.03295394]\n",
            " [ 0.54658234 -0.18232344]]\n"
          ]
        }
      ],
      "source": [
        "class CustomDataset(data.Dataset):\n",
        "    def __init__(self, feature_file,target_file, transform=None,phase='label'):\n",
        "        self.labels = target_file\n",
        "        self.features = feature_file\n",
        "        self.transform = transform\n",
        "        self.phase=phase\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.phase=='label': #caso de treino\n",
        "          feature=self.features[idx].astype(float)\n",
        "          label=self.labels[idx].astype(float)\n",
        "        else:\n",
        "          feature = self.features[idx].astype(float)\n",
        "        return feature.astype(np.float32), label.astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test(epoch,net,testloader):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    counter = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "\n",
        "            #inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = Variable(inputs), Variable(targets)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            counter.append(loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n| Validation Epoch #%d\\t\\t\\tLoss: %.4f \" %(epoch, loss.item()))\n",
        "    return 1,np.mean(counter)\n",
        "\n",
        "def train(epoch,net,trainloader,log_interval=5):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    counter=[]\n",
        "    print('\\n=> Training Epoch #%d, LR=%.4f' %(epoch, learning_rate_scheduler(lr, epoch)))\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate_scheduler(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "\n",
        "        #inputs, targets = inputs.cuda(), targets.cuda() # GPU settings\n",
        "        optimizer.zero_grad()\n",
        "        #inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)               # Forward Propagation\n",
        "        loss = criterion(outputs, targets)  # Loss\n",
        "        loss.backward()  # Backward Propagation\n",
        "        optimizer.step() # Optimizer update\n",
        "        counter.append(loss.item())\n",
        "        train_loss += loss.item()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('| Epoch [%3d/%3d] Iter[%3d/%3d]\\t\\tLoss: %.4f '\n",
        "                %(epoch, n_epochs, batch_idx+1,\n",
        "                    (len(trainloader.dataset)//batch_size)+1, loss.item()))\n",
        "    return np.mean(counter), 1\n",
        "print(y_test_data)\n",
        "train_loader_150=data.DataLoader(CustomDataset(x_train_data,y_train_data),\n",
        "                            batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_loader_100=data.DataLoader(CustomDataset(x_train_data[:qte_training//2],y_train_data[:qte_training//2]),\n",
        "                            batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_loader_50=data.DataLoader(CustomDataset(x_train_data[:qte_training//3],y_train_data[:qte_training//3]),\n",
        "                            batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader=data.DataLoader(CustomDataset(x_test_data,y_test_data),\n",
        "                            batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpJC2O41JLMn"
      },
      "source": [
        "#Treinamento da Rede\n",
        "Agora treinamos nosso modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 921,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTDa0wWRJKQ2",
        "outputId": "8368693f-170e-4b79-f7c2-d33ddd561292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=> Training Epoch #0, LR=0.0010\n",
            "| Epoch [  0/ 60] Iter[  1/251]\t\tLoss: 10.2532 \n",
            "| Epoch [  0/ 60] Iter[  6/251]\t\tLoss: 7.8384 \n",
            "| Epoch [  0/ 60] Iter[ 11/251]\t\tLoss: 7.7584 \n",
            "| Epoch [  0/ 60] Iter[ 16/251]\t\tLoss: 3.0244 \n",
            "| Epoch [  0/ 60] Iter[ 21/251]\t\tLoss: 3.8963 \n",
            "| Epoch [  0/ 60] Iter[ 26/251]\t\tLoss: 1.1536 \n",
            "| Epoch [  0/ 60] Iter[ 31/251]\t\tLoss: 1.9240 \n",
            "| Epoch [  0/ 60] Iter[ 36/251]\t\tLoss: 0.2783 \n",
            "| Epoch [  0/ 60] Iter[ 41/251]\t\tLoss: 0.5154 \n",
            "| Epoch [  0/ 60] Iter[ 46/251]\t\tLoss: 0.8106 \n",
            "| Epoch [  0/ 60] Iter[ 51/251]\t\tLoss: 0.3468 \n",
            "| Epoch [  0/ 60] Iter[ 56/251]\t\tLoss: 0.4440 \n",
            "| Epoch [  0/ 60] Iter[ 61/251]\t\tLoss: 0.0557 \n",
            "| Epoch [  0/ 60] Iter[ 66/251]\t\tLoss: 0.3906 \n",
            "| Epoch [  0/ 60] Iter[ 71/251]\t\tLoss: 1.2871 \n",
            "| Epoch [  0/ 60] Iter[ 76/251]\t\tLoss: 1.0629 \n",
            "| Epoch [  0/ 60] Iter[ 81/251]\t\tLoss: 0.2894 \n",
            "| Epoch [  0/ 60] Iter[ 86/251]\t\tLoss: 0.1409 \n",
            "| Epoch [  0/ 60] Iter[ 91/251]\t\tLoss: 1.3290 \n",
            "| Epoch [  0/ 60] Iter[ 96/251]\t\tLoss: 0.0787 \n",
            "| Epoch [  0/ 60] Iter[101/251]\t\tLoss: 0.3002 \n",
            "| Epoch [  0/ 60] Iter[106/251]\t\tLoss: 0.0681 \n",
            "| Epoch [  0/ 60] Iter[111/251]\t\tLoss: 0.1532 \n",
            "| Epoch [  0/ 60] Iter[116/251]\t\tLoss: 1.0986 \n",
            "| Epoch [  0/ 60] Iter[121/251]\t\tLoss: 0.8198 \n",
            "| Epoch [  0/ 60] Iter[126/251]\t\tLoss: 0.3083 \n",
            "| Epoch [  0/ 60] Iter[131/251]\t\tLoss: 0.0972 \n",
            "| Epoch [  0/ 60] Iter[136/251]\t\tLoss: 0.5259 \n",
            "| Epoch [  0/ 60] Iter[141/251]\t\tLoss: 0.0482 \n",
            "| Epoch [  0/ 60] Iter[146/251]\t\tLoss: 1.2413 \n",
            "| Epoch [  0/ 60] Iter[151/251]\t\tLoss: 0.7383 \n",
            "| Epoch [  0/ 60] Iter[156/251]\t\tLoss: 0.4176 \n",
            "| Epoch [  0/ 60] Iter[161/251]\t\tLoss: 0.3120 \n",
            "| Epoch [  0/ 60] Iter[166/251]\t\tLoss: 0.3633 \n",
            "| Epoch [  0/ 60] Iter[171/251]\t\tLoss: 3.4835 \n",
            "| Epoch [  0/ 60] Iter[176/251]\t\tLoss: 1.2121 \n",
            "| Epoch [  0/ 60] Iter[181/251]\t\tLoss: 0.3708 \n",
            "| Epoch [  0/ 60] Iter[186/251]\t\tLoss: 5.8773 \n",
            "| Epoch [  0/ 60] Iter[191/251]\t\tLoss: 0.6818 \n",
            "| Epoch [  0/ 60] Iter[196/251]\t\tLoss: 7.5321 \n",
            "| Epoch [  0/ 60] Iter[201/251]\t\tLoss: 0.2098 \n",
            "| Epoch [  0/ 60] Iter[206/251]\t\tLoss: 0.1232 \n",
            "| Epoch [  0/ 60] Iter[211/251]\t\tLoss: 0.4180 \n",
            "| Epoch [  0/ 60] Iter[216/251]\t\tLoss: 1.7998 \n",
            "| Epoch [  0/ 60] Iter[221/251]\t\tLoss: 0.1122 \n",
            "| Epoch [  0/ 60] Iter[226/251]\t\tLoss: 0.0747 \n",
            "| Epoch [  0/ 60] Iter[231/251]\t\tLoss: 0.0690 \n",
            "| Epoch [  0/ 60] Iter[236/251]\t\tLoss: 0.7392 \n",
            "| Epoch [  0/ 60] Iter[241/251]\t\tLoss: 0.4619 \n",
            "| Epoch [  0/ 60] Iter[246/251]\t\tLoss: 0.6009 \n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 0.1748 \n",
            "\n",
            "=> Training Epoch #1, LR=0.0010\n",
            "| Epoch [  1/ 60] Iter[  1/251]\t\tLoss: 2.8755 \n",
            "| Epoch [  1/ 60] Iter[  6/251]\t\tLoss: 0.5805 \n",
            "| Epoch [  1/ 60] Iter[ 11/251]\t\tLoss: 0.5588 \n",
            "| Epoch [  1/ 60] Iter[ 16/251]\t\tLoss: 5.7179 \n",
            "| Epoch [  1/ 60] Iter[ 21/251]\t\tLoss: 0.1080 \n",
            "| Epoch [  1/ 60] Iter[ 26/251]\t\tLoss: 6.2277 \n",
            "| Epoch [  1/ 60] Iter[ 31/251]\t\tLoss: 6.1245 \n",
            "| Epoch [  1/ 60] Iter[ 36/251]\t\tLoss: 0.5373 \n",
            "| Epoch [  1/ 60] Iter[ 41/251]\t\tLoss: 0.6720 \n",
            "| Epoch [  1/ 60] Iter[ 46/251]\t\tLoss: 0.1633 \n",
            "| Epoch [  1/ 60] Iter[ 51/251]\t\tLoss: 0.2399 \n",
            "| Epoch [  1/ 60] Iter[ 56/251]\t\tLoss: 0.0485 \n",
            "| Epoch [  1/ 60] Iter[ 61/251]\t\tLoss: 0.4337 \n",
            "| Epoch [  1/ 60] Iter[ 66/251]\t\tLoss: 0.2482 \n",
            "| Epoch [  1/ 60] Iter[ 71/251]\t\tLoss: 0.0190 \n",
            "| Epoch [  1/ 60] Iter[ 76/251]\t\tLoss: 0.2490 \n",
            "| Epoch [  1/ 60] Iter[ 81/251]\t\tLoss: 5.7452 \n",
            "| Epoch [  1/ 60] Iter[ 86/251]\t\tLoss: 0.2385 \n",
            "| Epoch [  1/ 60] Iter[ 91/251]\t\tLoss: 5.6574 \n",
            "| Epoch [  1/ 60] Iter[ 96/251]\t\tLoss: 0.7153 \n",
            "| Epoch [  1/ 60] Iter[101/251]\t\tLoss: 0.4624 \n",
            "| Epoch [  1/ 60] Iter[106/251]\t\tLoss: 0.0795 \n",
            "| Epoch [  1/ 60] Iter[111/251]\t\tLoss: 2.1085 \n",
            "| Epoch [  1/ 60] Iter[116/251]\t\tLoss: 0.5554 \n",
            "| Epoch [  1/ 60] Iter[121/251]\t\tLoss: 5.8093 \n",
            "| Epoch [  1/ 60] Iter[126/251]\t\tLoss: 0.4828 \n",
            "| Epoch [  1/ 60] Iter[131/251]\t\tLoss: 0.0458 \n",
            "| Epoch [  1/ 60] Iter[136/251]\t\tLoss: 0.0926 \n",
            "| Epoch [  1/ 60] Iter[141/251]\t\tLoss: 0.4044 \n",
            "| Epoch [  1/ 60] Iter[146/251]\t\tLoss: 0.2166 \n",
            "| Epoch [  1/ 60] Iter[151/251]\t\tLoss: 0.4065 \n",
            "| Epoch [  1/ 60] Iter[156/251]\t\tLoss: 0.7871 \n",
            "| Epoch [  1/ 60] Iter[161/251]\t\tLoss: 0.0479 \n",
            "| Epoch [  1/ 60] Iter[166/251]\t\tLoss: 0.3884 \n",
            "| Epoch [  1/ 60] Iter[171/251]\t\tLoss: 0.8123 \n",
            "| Epoch [  1/ 60] Iter[176/251]\t\tLoss: 1.1364 \n",
            "| Epoch [  1/ 60] Iter[181/251]\t\tLoss: 0.8490 \n",
            "| Epoch [  1/ 60] Iter[186/251]\t\tLoss: 0.2424 \n",
            "| Epoch [  1/ 60] Iter[191/251]\t\tLoss: 0.1628 \n",
            "| Epoch [  1/ 60] Iter[196/251]\t\tLoss: 0.3557 \n",
            "| Epoch [  1/ 60] Iter[201/251]\t\tLoss: 1.3119 \n",
            "| Epoch [  1/ 60] Iter[206/251]\t\tLoss: 0.4902 \n",
            "| Epoch [  1/ 60] Iter[211/251]\t\tLoss: 6.3603 \n",
            "| Epoch [  1/ 60] Iter[216/251]\t\tLoss: 0.2574 \n",
            "| Epoch [  1/ 60] Iter[221/251]\t\tLoss: 1.0014 \n",
            "| Epoch [  1/ 60] Iter[226/251]\t\tLoss: 0.5127 \n",
            "| Epoch [  1/ 60] Iter[231/251]\t\tLoss: 0.4438 \n",
            "| Epoch [  1/ 60] Iter[236/251]\t\tLoss: 0.0859 \n",
            "| Epoch [  1/ 60] Iter[241/251]\t\tLoss: 0.2263 \n",
            "| Epoch [  1/ 60] Iter[246/251]\t\tLoss: 0.3878 \n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 0.2009 \n",
            "\n",
            "=> Training Epoch #2, LR=0.0010\n",
            "| Epoch [  2/ 60] Iter[  1/251]\t\tLoss: 0.2081 \n",
            "| Epoch [  2/ 60] Iter[  6/251]\t\tLoss: 6.4663 \n",
            "| Epoch [  2/ 60] Iter[ 11/251]\t\tLoss: 12.1151 \n",
            "| Epoch [  2/ 60] Iter[ 16/251]\t\tLoss: 1.2091 \n",
            "| Epoch [  2/ 60] Iter[ 21/251]\t\tLoss: 0.1155 \n",
            "| Epoch [  2/ 60] Iter[ 26/251]\t\tLoss: 0.1121 \n",
            "| Epoch [  2/ 60] Iter[ 31/251]\t\tLoss: 7.1015 \n",
            "| Epoch [  2/ 60] Iter[ 36/251]\t\tLoss: 0.5430 \n",
            "| Epoch [  2/ 60] Iter[ 41/251]\t\tLoss: 0.4651 \n",
            "| Epoch [  2/ 60] Iter[ 46/251]\t\tLoss: 0.2422 \n",
            "| Epoch [  2/ 60] Iter[ 51/251]\t\tLoss: 0.4363 \n",
            "| Epoch [  2/ 60] Iter[ 56/251]\t\tLoss: 0.8726 \n",
            "| Epoch [  2/ 60] Iter[ 61/251]\t\tLoss: 0.3912 \n",
            "| Epoch [  2/ 60] Iter[ 66/251]\t\tLoss: 6.3066 \n",
            "| Epoch [  2/ 60] Iter[ 71/251]\t\tLoss: 0.2609 \n",
            "| Epoch [  2/ 60] Iter[ 76/251]\t\tLoss: 0.9658 \n",
            "| Epoch [  2/ 60] Iter[ 81/251]\t\tLoss: 0.5735 \n",
            "| Epoch [  2/ 60] Iter[ 86/251]\t\tLoss: 0.2306 \n",
            "| Epoch [  2/ 60] Iter[ 91/251]\t\tLoss: 0.4319 \n",
            "| Epoch [  2/ 60] Iter[ 96/251]\t\tLoss: 1.0086 \n",
            "| Epoch [  2/ 60] Iter[101/251]\t\tLoss: 0.6311 \n",
            "| Epoch [  2/ 60] Iter[106/251]\t\tLoss: 0.7988 \n",
            "| Epoch [  2/ 60] Iter[111/251]\t\tLoss: 0.1505 \n",
            "| Epoch [  2/ 60] Iter[116/251]\t\tLoss: 0.6471 \n",
            "| Epoch [  2/ 60] Iter[121/251]\t\tLoss: 0.3563 \n",
            "| Epoch [  2/ 60] Iter[126/251]\t\tLoss: 0.2056 \n",
            "| Epoch [  2/ 60] Iter[131/251]\t\tLoss: 0.2753 \n",
            "| Epoch [  2/ 60] Iter[136/251]\t\tLoss: 0.2753 \n",
            "| Epoch [  2/ 60] Iter[141/251]\t\tLoss: 6.0610 \n",
            "| Epoch [  2/ 60] Iter[146/251]\t\tLoss: 6.4663 \n",
            "| Epoch [  2/ 60] Iter[151/251]\t\tLoss: 6.1869 \n",
            "| Epoch [  2/ 60] Iter[156/251]\t\tLoss: 0.1475 \n",
            "| Epoch [  2/ 60] Iter[161/251]\t\tLoss: 0.1961 \n",
            "| Epoch [  2/ 60] Iter[166/251]\t\tLoss: 0.5314 \n",
            "| Epoch [  2/ 60] Iter[171/251]\t\tLoss: 0.2041 \n",
            "| Epoch [  2/ 60] Iter[176/251]\t\tLoss: 0.3315 \n",
            "| Epoch [  2/ 60] Iter[181/251]\t\tLoss: 0.9174 \n",
            "| Epoch [  2/ 60] Iter[186/251]\t\tLoss: 0.3506 \n",
            "| Epoch [  2/ 60] Iter[191/251]\t\tLoss: 0.2548 \n",
            "| Epoch [  2/ 60] Iter[196/251]\t\tLoss: 0.9572 \n",
            "| Epoch [  2/ 60] Iter[201/251]\t\tLoss: 1.4715 \n",
            "| Epoch [  2/ 60] Iter[206/251]\t\tLoss: 0.3123 \n",
            "| Epoch [  2/ 60] Iter[211/251]\t\tLoss: 0.7695 \n",
            "| Epoch [  2/ 60] Iter[216/251]\t\tLoss: 0.5012 \n",
            "| Epoch [  2/ 60] Iter[221/251]\t\tLoss: 0.3423 \n",
            "| Epoch [  2/ 60] Iter[226/251]\t\tLoss: 0.2557 \n",
            "| Epoch [  2/ 60] Iter[231/251]\t\tLoss: 0.3897 \n",
            "| Epoch [  2/ 60] Iter[236/251]\t\tLoss: 0.5725 \n",
            "| Epoch [  2/ 60] Iter[241/251]\t\tLoss: 0.1487 \n",
            "| Epoch [  2/ 60] Iter[246/251]\t\tLoss: 0.0696 \n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 0.1451 \n",
            "\n",
            "=> Training Epoch #3, LR=0.0010\n",
            "| Epoch [  3/ 60] Iter[  1/251]\t\tLoss: 0.3946 \n",
            "| Epoch [  3/ 60] Iter[  6/251]\t\tLoss: 0.0913 \n",
            "| Epoch [  3/ 60] Iter[ 11/251]\t\tLoss: 0.5349 \n",
            "| Epoch [  3/ 60] Iter[ 16/251]\t\tLoss: 0.2072 \n",
            "| Epoch [  3/ 60] Iter[ 21/251]\t\tLoss: 0.1496 \n",
            "| Epoch [  3/ 60] Iter[ 26/251]\t\tLoss: 0.4270 \n",
            "| Epoch [  3/ 60] Iter[ 31/251]\t\tLoss: 6.4518 \n",
            "| Epoch [  3/ 60] Iter[ 36/251]\t\tLoss: 1.0566 \n",
            "| Epoch [  3/ 60] Iter[ 41/251]\t\tLoss: 0.6300 \n",
            "| Epoch [  3/ 60] Iter[ 46/251]\t\tLoss: 0.9276 \n",
            "| Epoch [  3/ 60] Iter[ 51/251]\t\tLoss: 0.4349 \n",
            "| Epoch [  3/ 60] Iter[ 56/251]\t\tLoss: 0.3768 \n",
            "| Epoch [  3/ 60] Iter[ 61/251]\t\tLoss: 0.4990 \n",
            "| Epoch [  3/ 60] Iter[ 66/251]\t\tLoss: 0.5091 \n",
            "| Epoch [  3/ 60] Iter[ 71/251]\t\tLoss: 0.5037 \n",
            "| Epoch [  3/ 60] Iter[ 76/251]\t\tLoss: 0.2309 \n",
            "| Epoch [  3/ 60] Iter[ 81/251]\t\tLoss: 0.4945 \n",
            "| Epoch [  3/ 60] Iter[ 86/251]\t\tLoss: 0.3294 \n",
            "| Epoch [  3/ 60] Iter[ 91/251]\t\tLoss: 6.2127 \n",
            "| Epoch [  3/ 60] Iter[ 96/251]\t\tLoss: 6.7656 \n",
            "| Epoch [  3/ 60] Iter[101/251]\t\tLoss: 0.1194 \n",
            "| Epoch [  3/ 60] Iter[106/251]\t\tLoss: 6.6479 \n",
            "| Epoch [  3/ 60] Iter[111/251]\t\tLoss: 1.8543 \n",
            "| Epoch [  3/ 60] Iter[116/251]\t\tLoss: 0.2060 \n",
            "| Epoch [  3/ 60] Iter[121/251]\t\tLoss: 0.1578 \n",
            "| Epoch [  3/ 60] Iter[126/251]\t\tLoss: 0.6878 \n",
            "| Epoch [  3/ 60] Iter[131/251]\t\tLoss: 0.5822 \n",
            "| Epoch [  3/ 60] Iter[136/251]\t\tLoss: 0.3770 \n",
            "| Epoch [  3/ 60] Iter[141/251]\t\tLoss: 2.3407 \n",
            "| Epoch [  3/ 60] Iter[146/251]\t\tLoss: 0.3305 \n",
            "| Epoch [  3/ 60] Iter[151/251]\t\tLoss: 0.3375 \n",
            "| Epoch [  3/ 60] Iter[156/251]\t\tLoss: 6.2907 \n",
            "| Epoch [  3/ 60] Iter[161/251]\t\tLoss: 0.6591 \n",
            "| Epoch [  3/ 60] Iter[166/251]\t\tLoss: 4.7198 \n",
            "| Epoch [  3/ 60] Iter[171/251]\t\tLoss: 0.1590 \n",
            "| Epoch [  3/ 60] Iter[176/251]\t\tLoss: 1.8355 \n",
            "| Epoch [  3/ 60] Iter[181/251]\t\tLoss: 0.2720 \n",
            "| Epoch [  3/ 60] Iter[186/251]\t\tLoss: 0.6596 \n",
            "| Epoch [  3/ 60] Iter[191/251]\t\tLoss: 5.5402 \n",
            "| Epoch [  3/ 60] Iter[196/251]\t\tLoss: 0.2615 \n",
            "| Epoch [  3/ 60] Iter[201/251]\t\tLoss: 0.1236 \n",
            "| Epoch [  3/ 60] Iter[206/251]\t\tLoss: 0.4527 \n",
            "| Epoch [  3/ 60] Iter[211/251]\t\tLoss: 0.4426 \n",
            "| Epoch [  3/ 60] Iter[216/251]\t\tLoss: 0.4197 \n",
            "| Epoch [  3/ 60] Iter[221/251]\t\tLoss: 0.1033 \n",
            "| Epoch [  3/ 60] Iter[226/251]\t\tLoss: 2.7524 \n",
            "| Epoch [  3/ 60] Iter[231/251]\t\tLoss: 0.6115 \n",
            "| Epoch [  3/ 60] Iter[236/251]\t\tLoss: 0.0651 \n",
            "| Epoch [  3/ 60] Iter[241/251]\t\tLoss: 0.6463 \n",
            "| Epoch [  3/ 60] Iter[246/251]\t\tLoss: 0.2362 \n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 0.1698 \n",
            "\n",
            "=> Training Epoch #4, LR=0.0010\n",
            "| Epoch [  4/ 60] Iter[  1/251]\t\tLoss: 0.1803 \n",
            "| Epoch [  4/ 60] Iter[  6/251]\t\tLoss: 1.2788 \n",
            "| Epoch [  4/ 60] Iter[ 11/251]\t\tLoss: 2.6748 \n",
            "| Epoch [  4/ 60] Iter[ 16/251]\t\tLoss: 0.8870 \n",
            "| Epoch [  4/ 60] Iter[ 21/251]\t\tLoss: 0.7424 \n",
            "| Epoch [  4/ 60] Iter[ 26/251]\t\tLoss: 0.5830 \n",
            "| Epoch [  4/ 60] Iter[ 31/251]\t\tLoss: 0.2414 \n",
            "| Epoch [  4/ 60] Iter[ 36/251]\t\tLoss: 0.7553 \n",
            "| Epoch [  4/ 60] Iter[ 41/251]\t\tLoss: 0.0723 \n",
            "| Epoch [  4/ 60] Iter[ 46/251]\t\tLoss: 0.8606 \n",
            "| Epoch [  4/ 60] Iter[ 51/251]\t\tLoss: 0.2699 \n",
            "| Epoch [  4/ 60] Iter[ 56/251]\t\tLoss: 0.2752 \n",
            "| Epoch [  4/ 60] Iter[ 61/251]\t\tLoss: 0.4763 \n",
            "| Epoch [  4/ 60] Iter[ 66/251]\t\tLoss: 0.1143 \n",
            "| Epoch [  4/ 60] Iter[ 71/251]\t\tLoss: 1.1766 \n",
            "| Epoch [  4/ 60] Iter[ 76/251]\t\tLoss: 0.2065 \n",
            "| Epoch [  4/ 60] Iter[ 81/251]\t\tLoss: 0.3210 \n",
            "| Epoch [  4/ 60] Iter[ 86/251]\t\tLoss: 0.1591 \n",
            "| Epoch [  4/ 60] Iter[ 91/251]\t\tLoss: 0.5831 \n",
            "| Epoch [  4/ 60] Iter[ 96/251]\t\tLoss: 0.2391 \n",
            "| Epoch [  4/ 60] Iter[101/251]\t\tLoss: 0.9304 \n",
            "| Epoch [  4/ 60] Iter[106/251]\t\tLoss: 1.4577 \n",
            "| Epoch [  4/ 60] Iter[111/251]\t\tLoss: 0.4010 \n",
            "| Epoch [  4/ 60] Iter[116/251]\t\tLoss: 0.4280 \n",
            "| Epoch [  4/ 60] Iter[121/251]\t\tLoss: 0.2246 \n",
            "| Epoch [  4/ 60] Iter[126/251]\t\tLoss: 0.7511 \n",
            "| Epoch [  4/ 60] Iter[131/251]\t\tLoss: 0.3143 \n",
            "| Epoch [  4/ 60] Iter[136/251]\t\tLoss: 0.1025 \n",
            "| Epoch [  4/ 60] Iter[141/251]\t\tLoss: 0.2805 \n",
            "| Epoch [  4/ 60] Iter[146/251]\t\tLoss: 0.6225 \n",
            "| Epoch [  4/ 60] Iter[151/251]\t\tLoss: 0.5492 \n",
            "| Epoch [  4/ 60] Iter[156/251]\t\tLoss: 0.4628 \n",
            "| Epoch [  4/ 60] Iter[161/251]\t\tLoss: 1.3586 \n",
            "| Epoch [  4/ 60] Iter[166/251]\t\tLoss: 0.5317 \n",
            "| Epoch [  4/ 60] Iter[171/251]\t\tLoss: 0.1434 \n",
            "| Epoch [  4/ 60] Iter[176/251]\t\tLoss: 6.5742 \n",
            "| Epoch [  4/ 60] Iter[181/251]\t\tLoss: 0.3689 \n",
            "| Epoch [  4/ 60] Iter[186/251]\t\tLoss: 1.0842 \n",
            "| Epoch [  4/ 60] Iter[191/251]\t\tLoss: 0.9941 \n",
            "| Epoch [  4/ 60] Iter[196/251]\t\tLoss: 7.1116 \n",
            "| Epoch [  4/ 60] Iter[201/251]\t\tLoss: 0.2438 \n",
            "| Epoch [  4/ 60] Iter[206/251]\t\tLoss: 0.8413 \n",
            "| Epoch [  4/ 60] Iter[211/251]\t\tLoss: 0.5822 \n",
            "| Epoch [  4/ 60] Iter[216/251]\t\tLoss: 0.4544 \n",
            "| Epoch [  4/ 60] Iter[221/251]\t\tLoss: 0.2432 \n",
            "| Epoch [  4/ 60] Iter[226/251]\t\tLoss: 0.6345 \n",
            "| Epoch [  4/ 60] Iter[231/251]\t\tLoss: 0.4105 \n",
            "| Epoch [  4/ 60] Iter[236/251]\t\tLoss: 0.1819 \n",
            "| Epoch [  4/ 60] Iter[241/251]\t\tLoss: 0.4233 \n",
            "| Epoch [  4/ 60] Iter[246/251]\t\tLoss: 0.1069 \n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 0.1300 \n",
            "\n",
            "=> Training Epoch #5, LR=0.0010\n",
            "| Epoch [  5/ 60] Iter[  1/251]\t\tLoss: 0.5013 \n",
            "| Epoch [  5/ 60] Iter[  6/251]\t\tLoss: 0.7512 \n",
            "| Epoch [  5/ 60] Iter[ 11/251]\t\tLoss: 0.6713 \n",
            "| Epoch [  5/ 60] Iter[ 16/251]\t\tLoss: 0.2547 \n",
            "| Epoch [  5/ 60] Iter[ 21/251]\t\tLoss: 0.1286 \n",
            "| Epoch [  5/ 60] Iter[ 26/251]\t\tLoss: 0.2858 \n",
            "| Epoch [  5/ 60] Iter[ 31/251]\t\tLoss: 0.0964 \n",
            "| Epoch [  5/ 60] Iter[ 36/251]\t\tLoss: 0.3991 \n",
            "| Epoch [  5/ 60] Iter[ 41/251]\t\tLoss: 1.1388 \n",
            "| Epoch [  5/ 60] Iter[ 46/251]\t\tLoss: 0.1735 \n",
            "| Epoch [  5/ 60] Iter[ 51/251]\t\tLoss: 1.4225 \n",
            "| Epoch [  5/ 60] Iter[ 56/251]\t\tLoss: 0.3106 \n",
            "| Epoch [  5/ 60] Iter[ 61/251]\t\tLoss: 0.1751 \n",
            "| Epoch [  5/ 60] Iter[ 66/251]\t\tLoss: 0.2275 \n",
            "| Epoch [  5/ 60] Iter[ 71/251]\t\tLoss: 0.3708 \n",
            "| Epoch [  5/ 60] Iter[ 76/251]\t\tLoss: 0.4955 \n",
            "| Epoch [  5/ 60] Iter[ 81/251]\t\tLoss: 0.2254 \n",
            "| Epoch [  5/ 60] Iter[ 86/251]\t\tLoss: 0.7767 \n",
            "| Epoch [  5/ 60] Iter[ 91/251]\t\tLoss: 0.8191 \n",
            "| Epoch [  5/ 60] Iter[ 96/251]\t\tLoss: 6.5414 \n",
            "| Epoch [  5/ 60] Iter[101/251]\t\tLoss: 0.5769 \n",
            "| Epoch [  5/ 60] Iter[106/251]\t\tLoss: 0.2288 \n",
            "| Epoch [  5/ 60] Iter[111/251]\t\tLoss: 0.1703 \n",
            "| Epoch [  5/ 60] Iter[116/251]\t\tLoss: 0.9035 \n",
            "| Epoch [  5/ 60] Iter[121/251]\t\tLoss: 0.3717 \n",
            "| Epoch [  5/ 60] Iter[126/251]\t\tLoss: 0.8179 \n",
            "| Epoch [  5/ 60] Iter[131/251]\t\tLoss: 1.1752 \n",
            "| Epoch [  5/ 60] Iter[136/251]\t\tLoss: 0.4994 \n",
            "| Epoch [  5/ 60] Iter[141/251]\t\tLoss: 0.6832 \n",
            "| Epoch [  5/ 60] Iter[146/251]\t\tLoss: 0.7775 \n",
            "| Epoch [  5/ 60] Iter[151/251]\t\tLoss: 0.6461 \n",
            "| Epoch [  5/ 60] Iter[156/251]\t\tLoss: 0.0252 \n",
            "| Epoch [  5/ 60] Iter[161/251]\t\tLoss: 0.3386 \n",
            "| Epoch [  5/ 60] Iter[166/251]\t\tLoss: 6.8460 \n",
            "| Epoch [  5/ 60] Iter[171/251]\t\tLoss: 0.6132 \n",
            "| Epoch [  5/ 60] Iter[176/251]\t\tLoss: 1.0914 \n",
            "| Epoch [  5/ 60] Iter[181/251]\t\tLoss: 0.1110 \n",
            "| Epoch [  5/ 60] Iter[186/251]\t\tLoss: 6.3613 \n",
            "| Epoch [  5/ 60] Iter[191/251]\t\tLoss: 0.3250 \n",
            "| Epoch [  5/ 60] Iter[196/251]\t\tLoss: 0.4985 \n",
            "| Epoch [  5/ 60] Iter[201/251]\t\tLoss: 0.2305 \n",
            "| Epoch [  5/ 60] Iter[206/251]\t\tLoss: 0.4312 \n",
            "| Epoch [  5/ 60] Iter[211/251]\t\tLoss: 0.2466 \n",
            "| Epoch [  5/ 60] Iter[216/251]\t\tLoss: 0.5324 \n",
            "| Epoch [  5/ 60] Iter[221/251]\t\tLoss: 0.8934 \n",
            "| Epoch [  5/ 60] Iter[226/251]\t\tLoss: 0.5804 \n",
            "| Epoch [  5/ 60] Iter[231/251]\t\tLoss: 2.1904 \n",
            "| Epoch [  5/ 60] Iter[236/251]\t\tLoss: 1.4467 \n",
            "| Epoch [  5/ 60] Iter[241/251]\t\tLoss: 0.4266 \n",
            "| Epoch [  5/ 60] Iter[246/251]\t\tLoss: 0.4589 \n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 0.2686 \n",
            "\n",
            "=> Training Epoch #6, LR=0.0010\n",
            "| Epoch [  6/ 60] Iter[  1/251]\t\tLoss: 0.4500 \n",
            "| Epoch [  6/ 60] Iter[  6/251]\t\tLoss: 0.3095 \n",
            "| Epoch [  6/ 60] Iter[ 11/251]\t\tLoss: 0.3159 \n",
            "| Epoch [  6/ 60] Iter[ 16/251]\t\tLoss: 0.6183 \n",
            "| Epoch [  6/ 60] Iter[ 21/251]\t\tLoss: 0.6923 \n",
            "| Epoch [  6/ 60] Iter[ 26/251]\t\tLoss: 0.1119 \n",
            "| Epoch [  6/ 60] Iter[ 31/251]\t\tLoss: 0.7636 \n",
            "| Epoch [  6/ 60] Iter[ 36/251]\t\tLoss: 0.0157 \n",
            "| Epoch [  6/ 60] Iter[ 41/251]\t\tLoss: 0.3531 \n",
            "| Epoch [  6/ 60] Iter[ 46/251]\t\tLoss: 6.9202 \n",
            "| Epoch [  6/ 60] Iter[ 51/251]\t\tLoss: 1.0871 \n",
            "| Epoch [  6/ 60] Iter[ 56/251]\t\tLoss: 0.3822 \n",
            "| Epoch [  6/ 60] Iter[ 61/251]\t\tLoss: 0.2774 \n",
            "| Epoch [  6/ 60] Iter[ 66/251]\t\tLoss: 0.1473 \n",
            "| Epoch [  6/ 60] Iter[ 71/251]\t\tLoss: 0.3502 \n",
            "| Epoch [  6/ 60] Iter[ 76/251]\t\tLoss: 1.0093 \n",
            "| Epoch [  6/ 60] Iter[ 81/251]\t\tLoss: 1.4543 \n",
            "| Epoch [  6/ 60] Iter[ 86/251]\t\tLoss: 0.2094 \n",
            "| Epoch [  6/ 60] Iter[ 91/251]\t\tLoss: 0.3451 \n",
            "| Epoch [  6/ 60] Iter[ 96/251]\t\tLoss: 1.0831 \n",
            "| Epoch [  6/ 60] Iter[101/251]\t\tLoss: 6.2123 \n",
            "| Epoch [  6/ 60] Iter[106/251]\t\tLoss: 0.4431 \n",
            "| Epoch [  6/ 60] Iter[111/251]\t\tLoss: 1.5211 \n",
            "| Epoch [  6/ 60] Iter[116/251]\t\tLoss: 0.7700 \n",
            "| Epoch [  6/ 60] Iter[121/251]\t\tLoss: 0.6192 \n",
            "| Epoch [  6/ 60] Iter[126/251]\t\tLoss: 0.6504 \n",
            "| Epoch [  6/ 60] Iter[131/251]\t\tLoss: 0.4018 \n",
            "| Epoch [  6/ 60] Iter[136/251]\t\tLoss: 0.4090 \n",
            "| Epoch [  6/ 60] Iter[141/251]\t\tLoss: 6.1888 \n",
            "| Epoch [  6/ 60] Iter[146/251]\t\tLoss: 0.2751 \n",
            "| Epoch [  6/ 60] Iter[151/251]\t\tLoss: 0.8358 \n",
            "| Epoch [  6/ 60] Iter[156/251]\t\tLoss: 0.6476 \n",
            "| Epoch [  6/ 60] Iter[161/251]\t\tLoss: 0.3653 \n",
            "| Epoch [  6/ 60] Iter[166/251]\t\tLoss: 0.5666 \n",
            "| Epoch [  6/ 60] Iter[171/251]\t\tLoss: 0.6699 \n",
            "| Epoch [  6/ 60] Iter[176/251]\t\tLoss: 0.0716 \n",
            "| Epoch [  6/ 60] Iter[181/251]\t\tLoss: 1.0330 \n",
            "| Epoch [  6/ 60] Iter[186/251]\t\tLoss: 0.5027 \n",
            "| Epoch [  6/ 60] Iter[191/251]\t\tLoss: 0.1397 \n",
            "| Epoch [  6/ 60] Iter[196/251]\t\tLoss: 0.3338 \n",
            "| Epoch [  6/ 60] Iter[201/251]\t\tLoss: 1.3214 \n",
            "| Epoch [  6/ 60] Iter[206/251]\t\tLoss: 0.8141 \n",
            "| Epoch [  6/ 60] Iter[211/251]\t\tLoss: 0.1063 \n",
            "| Epoch [  6/ 60] Iter[216/251]\t\tLoss: 0.2456 \n",
            "| Epoch [  6/ 60] Iter[221/251]\t\tLoss: 0.2003 \n",
            "| Epoch [  6/ 60] Iter[226/251]\t\tLoss: 0.1447 \n",
            "| Epoch [  6/ 60] Iter[231/251]\t\tLoss: 0.1849 \n",
            "| Epoch [  6/ 60] Iter[236/251]\t\tLoss: 0.3794 \n",
            "| Epoch [  6/ 60] Iter[241/251]\t\tLoss: 0.1446 \n",
            "| Epoch [  6/ 60] Iter[246/251]\t\tLoss: 0.1087 \n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 0.1911 \n",
            "\n",
            "=> Training Epoch #7, LR=0.0010\n",
            "| Epoch [  7/ 60] Iter[  1/251]\t\tLoss: 0.3546 \n",
            "| Epoch [  7/ 60] Iter[  6/251]\t\tLoss: 0.4106 \n",
            "| Epoch [  7/ 60] Iter[ 11/251]\t\tLoss: 6.0084 \n",
            "| Epoch [  7/ 60] Iter[ 16/251]\t\tLoss: 1.0517 \n",
            "| Epoch [  7/ 60] Iter[ 21/251]\t\tLoss: 0.0733 \n",
            "| Epoch [  7/ 60] Iter[ 26/251]\t\tLoss: 0.9700 \n",
            "| Epoch [  7/ 60] Iter[ 31/251]\t\tLoss: 0.5250 \n",
            "| Epoch [  7/ 60] Iter[ 36/251]\t\tLoss: 0.7408 \n",
            "| Epoch [  7/ 60] Iter[ 41/251]\t\tLoss: 0.7592 \n",
            "| Epoch [  7/ 60] Iter[ 46/251]\t\tLoss: 0.7647 \n",
            "| Epoch [  7/ 60] Iter[ 51/251]\t\tLoss: 6.0359 \n",
            "| Epoch [  7/ 60] Iter[ 56/251]\t\tLoss: 0.9826 \n",
            "| Epoch [  7/ 60] Iter[ 61/251]\t\tLoss: 0.4534 \n",
            "| Epoch [  7/ 60] Iter[ 66/251]\t\tLoss: 0.6057 \n",
            "| Epoch [  7/ 60] Iter[ 71/251]\t\tLoss: 0.4323 \n",
            "| Epoch [  7/ 60] Iter[ 76/251]\t\tLoss: 0.6035 \n",
            "| Epoch [  7/ 60] Iter[ 81/251]\t\tLoss: 0.9696 \n",
            "| Epoch [  7/ 60] Iter[ 86/251]\t\tLoss: 0.1407 \n",
            "| Epoch [  7/ 60] Iter[ 91/251]\t\tLoss: 0.1398 \n",
            "| Epoch [  7/ 60] Iter[ 96/251]\t\tLoss: 0.0886 \n",
            "| Epoch [  7/ 60] Iter[101/251]\t\tLoss: 0.7975 \n",
            "| Epoch [  7/ 60] Iter[106/251]\t\tLoss: 0.9885 \n",
            "| Epoch [  7/ 60] Iter[111/251]\t\tLoss: 0.1522 \n",
            "| Epoch [  7/ 60] Iter[116/251]\t\tLoss: 0.1020 \n",
            "| Epoch [  7/ 60] Iter[121/251]\t\tLoss: 0.0749 \n",
            "| Epoch [  7/ 60] Iter[126/251]\t\tLoss: 7.5672 \n",
            "| Epoch [  7/ 60] Iter[131/251]\t\tLoss: 0.4702 \n",
            "| Epoch [  7/ 60] Iter[136/251]\t\tLoss: 0.1128 \n",
            "| Epoch [  7/ 60] Iter[141/251]\t\tLoss: 1.3058 \n",
            "| Epoch [  7/ 60] Iter[146/251]\t\tLoss: 5.8987 \n",
            "| Epoch [  7/ 60] Iter[151/251]\t\tLoss: 0.1294 \n",
            "| Epoch [  7/ 60] Iter[156/251]\t\tLoss: 0.5683 \n",
            "| Epoch [  7/ 60] Iter[161/251]\t\tLoss: 0.5044 \n",
            "| Epoch [  7/ 60] Iter[166/251]\t\tLoss: 0.2433 \n",
            "| Epoch [  7/ 60] Iter[171/251]\t\tLoss: 0.9242 \n",
            "| Epoch [  7/ 60] Iter[176/251]\t\tLoss: 0.1076 \n",
            "| Epoch [  7/ 60] Iter[181/251]\t\tLoss: 0.6506 \n",
            "| Epoch [  7/ 60] Iter[186/251]\t\tLoss: 0.6948 \n",
            "| Epoch [  7/ 60] Iter[191/251]\t\tLoss: 0.6748 \n",
            "| Epoch [  7/ 60] Iter[196/251]\t\tLoss: 0.0134 \n",
            "| Epoch [  7/ 60] Iter[201/251]\t\tLoss: 0.4708 \n",
            "| Epoch [  7/ 60] Iter[206/251]\t\tLoss: 0.4399 \n",
            "| Epoch [  7/ 60] Iter[211/251]\t\tLoss: 0.4125 \n",
            "| Epoch [  7/ 60] Iter[216/251]\t\tLoss: 0.2747 \n",
            "| Epoch [  7/ 60] Iter[221/251]\t\tLoss: 0.4736 \n",
            "| Epoch [  7/ 60] Iter[226/251]\t\tLoss: 0.6647 \n",
            "| Epoch [  7/ 60] Iter[231/251]\t\tLoss: 0.3666 \n",
            "| Epoch [  7/ 60] Iter[236/251]\t\tLoss: 0.3908 \n",
            "| Epoch [  7/ 60] Iter[241/251]\t\tLoss: 6.3379 \n",
            "| Epoch [  7/ 60] Iter[246/251]\t\tLoss: 0.2487 \n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 0.2434 \n",
            "\n",
            "=> Training Epoch #8, LR=0.0010\n",
            "| Epoch [  8/ 60] Iter[  1/251]\t\tLoss: 0.4361 \n",
            "| Epoch [  8/ 60] Iter[  6/251]\t\tLoss: 0.2443 \n",
            "| Epoch [  8/ 60] Iter[ 11/251]\t\tLoss: 0.7607 \n",
            "| Epoch [  8/ 60] Iter[ 16/251]\t\tLoss: 0.6807 \n",
            "| Epoch [  8/ 60] Iter[ 21/251]\t\tLoss: 0.6429 \n",
            "| Epoch [  8/ 60] Iter[ 26/251]\t\tLoss: 6.4913 \n",
            "| Epoch [  8/ 60] Iter[ 31/251]\t\tLoss: 5.8739 \n",
            "| Epoch [  8/ 60] Iter[ 36/251]\t\tLoss: 5.7201 \n",
            "| Epoch [  8/ 60] Iter[ 41/251]\t\tLoss: 0.1204 \n",
            "| Epoch [  8/ 60] Iter[ 46/251]\t\tLoss: 0.5108 \n",
            "| Epoch [  8/ 60] Iter[ 51/251]\t\tLoss: 0.4043 \n",
            "| Epoch [  8/ 60] Iter[ 56/251]\t\tLoss: 0.3475 \n",
            "| Epoch [  8/ 60] Iter[ 61/251]\t\tLoss: 0.7286 \n",
            "| Epoch [  8/ 60] Iter[ 66/251]\t\tLoss: 0.7274 \n",
            "| Epoch [  8/ 60] Iter[ 71/251]\t\tLoss: 1.5375 \n",
            "| Epoch [  8/ 60] Iter[ 76/251]\t\tLoss: 0.3293 \n",
            "| Epoch [  8/ 60] Iter[ 81/251]\t\tLoss: 0.7399 \n",
            "| Epoch [  8/ 60] Iter[ 86/251]\t\tLoss: 0.7354 \n",
            "| Epoch [  8/ 60] Iter[ 91/251]\t\tLoss: 0.3018 \n",
            "| Epoch [  8/ 60] Iter[ 96/251]\t\tLoss: 0.1901 \n",
            "| Epoch [  8/ 60] Iter[101/251]\t\tLoss: 0.3059 \n",
            "| Epoch [  8/ 60] Iter[106/251]\t\tLoss: 0.2373 \n",
            "| Epoch [  8/ 60] Iter[111/251]\t\tLoss: 0.3193 \n",
            "| Epoch [  8/ 60] Iter[116/251]\t\tLoss: 0.3090 \n",
            "| Epoch [  8/ 60] Iter[121/251]\t\tLoss: 0.7873 \n",
            "| Epoch [  8/ 60] Iter[126/251]\t\tLoss: 0.9875 \n",
            "| Epoch [  8/ 60] Iter[131/251]\t\tLoss: 1.0699 \n",
            "| Epoch [  8/ 60] Iter[136/251]\t\tLoss: 0.1227 \n",
            "| Epoch [  8/ 60] Iter[141/251]\t\tLoss: 5.7490 \n",
            "| Epoch [  8/ 60] Iter[146/251]\t\tLoss: 0.2971 \n",
            "| Epoch [  8/ 60] Iter[151/251]\t\tLoss: 0.5283 \n",
            "| Epoch [  8/ 60] Iter[156/251]\t\tLoss: 0.4316 \n",
            "| Epoch [  8/ 60] Iter[161/251]\t\tLoss: 0.1641 \n",
            "| Epoch [  8/ 60] Iter[166/251]\t\tLoss: 0.6894 \n",
            "| Epoch [  8/ 60] Iter[171/251]\t\tLoss: 0.2409 \n",
            "| Epoch [  8/ 60] Iter[176/251]\t\tLoss: 1.0910 \n",
            "| Epoch [  8/ 60] Iter[181/251]\t\tLoss: 0.5044 \n",
            "| Epoch [  8/ 60] Iter[186/251]\t\tLoss: 0.2333 \n",
            "| Epoch [  8/ 60] Iter[191/251]\t\tLoss: 0.3347 \n",
            "| Epoch [  8/ 60] Iter[196/251]\t\tLoss: 6.4464 \n",
            "| Epoch [  8/ 60] Iter[201/251]\t\tLoss: 0.3508 \n",
            "| Epoch [  8/ 60] Iter[206/251]\t\tLoss: 0.2573 \n",
            "| Epoch [  8/ 60] Iter[211/251]\t\tLoss: 0.9274 \n",
            "| Epoch [  8/ 60] Iter[216/251]\t\tLoss: 1.0425 \n",
            "| Epoch [  8/ 60] Iter[221/251]\t\tLoss: 0.5834 \n",
            "| Epoch [  8/ 60] Iter[226/251]\t\tLoss: 0.1683 \n",
            "| Epoch [  8/ 60] Iter[231/251]\t\tLoss: 0.5067 \n",
            "| Epoch [  8/ 60] Iter[236/251]\t\tLoss: 0.6048 \n",
            "| Epoch [  8/ 60] Iter[241/251]\t\tLoss: 0.4648 \n",
            "| Epoch [  8/ 60] Iter[246/251]\t\tLoss: 1.3130 \n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 0.2701 \n",
            "\n",
            "=> Training Epoch #9, LR=0.0010\n",
            "| Epoch [  9/ 60] Iter[  1/251]\t\tLoss: 0.6957 \n",
            "| Epoch [  9/ 60] Iter[  6/251]\t\tLoss: 0.9946 \n",
            "| Epoch [  9/ 60] Iter[ 11/251]\t\tLoss: 0.5702 \n",
            "| Epoch [  9/ 60] Iter[ 16/251]\t\tLoss: 0.8238 \n",
            "| Epoch [  9/ 60] Iter[ 21/251]\t\tLoss: 0.1878 \n",
            "| Epoch [  9/ 60] Iter[ 26/251]\t\tLoss: 0.3237 \n",
            "| Epoch [  9/ 60] Iter[ 31/251]\t\tLoss: 0.3488 \n",
            "| Epoch [  9/ 60] Iter[ 36/251]\t\tLoss: 0.2821 \n",
            "| Epoch [  9/ 60] Iter[ 41/251]\t\tLoss: 1.0843 \n",
            "| Epoch [  9/ 60] Iter[ 46/251]\t\tLoss: 0.5539 \n",
            "| Epoch [  9/ 60] Iter[ 51/251]\t\tLoss: 0.4556 \n",
            "| Epoch [  9/ 60] Iter[ 56/251]\t\tLoss: 0.2807 \n",
            "| Epoch [  9/ 60] Iter[ 61/251]\t\tLoss: 6.3511 \n",
            "| Epoch [  9/ 60] Iter[ 66/251]\t\tLoss: 0.2980 \n",
            "| Epoch [  9/ 60] Iter[ 71/251]\t\tLoss: 0.3665 \n",
            "| Epoch [  9/ 60] Iter[ 76/251]\t\tLoss: 1.6076 \n",
            "| Epoch [  9/ 60] Iter[ 81/251]\t\tLoss: 1.4505 \n",
            "| Epoch [  9/ 60] Iter[ 86/251]\t\tLoss: 1.0376 \n",
            "| Epoch [  9/ 60] Iter[ 91/251]\t\tLoss: 0.4533 \n",
            "| Epoch [  9/ 60] Iter[ 96/251]\t\tLoss: 0.2695 \n",
            "| Epoch [  9/ 60] Iter[101/251]\t\tLoss: 1.2038 \n",
            "| Epoch [  9/ 60] Iter[106/251]\t\tLoss: 0.4042 \n",
            "| Epoch [  9/ 60] Iter[111/251]\t\tLoss: 1.2911 \n",
            "| Epoch [  9/ 60] Iter[116/251]\t\tLoss: 0.4571 \n",
            "| Epoch [  9/ 60] Iter[121/251]\t\tLoss: 0.0760 \n",
            "| Epoch [  9/ 60] Iter[126/251]\t\tLoss: 0.4210 \n",
            "| Epoch [  9/ 60] Iter[131/251]\t\tLoss: 0.5679 \n",
            "| Epoch [  9/ 60] Iter[136/251]\t\tLoss: 0.2793 \n",
            "| Epoch [  9/ 60] Iter[141/251]\t\tLoss: 1.4305 \n",
            "| Epoch [  9/ 60] Iter[146/251]\t\tLoss: 0.7006 \n",
            "| Epoch [  9/ 60] Iter[151/251]\t\tLoss: 0.4005 \n",
            "| Epoch [  9/ 60] Iter[156/251]\t\tLoss: 0.2538 \n",
            "| Epoch [  9/ 60] Iter[161/251]\t\tLoss: 0.3545 \n",
            "| Epoch [  9/ 60] Iter[166/251]\t\tLoss: 0.7834 \n",
            "| Epoch [  9/ 60] Iter[171/251]\t\tLoss: 0.4230 \n",
            "| Epoch [  9/ 60] Iter[176/251]\t\tLoss: 0.4976 \n",
            "| Epoch [  9/ 60] Iter[181/251]\t\tLoss: 0.2968 \n",
            "| Epoch [  9/ 60] Iter[186/251]\t\tLoss: 0.7583 \n",
            "| Epoch [  9/ 60] Iter[191/251]\t\tLoss: 0.3517 \n",
            "| Epoch [  9/ 60] Iter[196/251]\t\tLoss: 0.2189 \n",
            "| Epoch [  9/ 60] Iter[201/251]\t\tLoss: 0.0833 \n",
            "| Epoch [  9/ 60] Iter[206/251]\t\tLoss: 0.2948 \n",
            "| Epoch [  9/ 60] Iter[211/251]\t\tLoss: 1.5064 \n",
            "| Epoch [  9/ 60] Iter[216/251]\t\tLoss: 0.2594 \n",
            "| Epoch [  9/ 60] Iter[221/251]\t\tLoss: 0.1798 \n",
            "| Epoch [  9/ 60] Iter[226/251]\t\tLoss: 0.1710 \n",
            "| Epoch [  9/ 60] Iter[231/251]\t\tLoss: 0.2710 \n",
            "| Epoch [  9/ 60] Iter[236/251]\t\tLoss: 0.9353 \n",
            "| Epoch [  9/ 60] Iter[241/251]\t\tLoss: 0.0383 \n",
            "| Epoch [  9/ 60] Iter[246/251]\t\tLoss: 0.5546 \n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 0.1495 \n",
            "\n",
            "=> Training Epoch #10, LR=0.0010\n",
            "| Epoch [ 10/ 60] Iter[  1/251]\t\tLoss: 0.5033 \n",
            "| Epoch [ 10/ 60] Iter[  6/251]\t\tLoss: 2.5506 \n",
            "| Epoch [ 10/ 60] Iter[ 11/251]\t\tLoss: 0.5483 \n",
            "| Epoch [ 10/ 60] Iter[ 16/251]\t\tLoss: 0.2153 \n",
            "| Epoch [ 10/ 60] Iter[ 21/251]\t\tLoss: 1.0099 \n",
            "| Epoch [ 10/ 60] Iter[ 26/251]\t\tLoss: 0.3920 \n",
            "| Epoch [ 10/ 60] Iter[ 31/251]\t\tLoss: 0.2713 \n",
            "| Epoch [ 10/ 60] Iter[ 36/251]\t\tLoss: 0.2195 \n",
            "| Epoch [ 10/ 60] Iter[ 41/251]\t\tLoss: 0.5152 \n",
            "| Epoch [ 10/ 60] Iter[ 46/251]\t\tLoss: 1.2015 \n",
            "| Epoch [ 10/ 60] Iter[ 51/251]\t\tLoss: 0.4865 \n",
            "| Epoch [ 10/ 60] Iter[ 56/251]\t\tLoss: 6.5458 \n",
            "| Epoch [ 10/ 60] Iter[ 61/251]\t\tLoss: 1.3833 \n",
            "| Epoch [ 10/ 60] Iter[ 66/251]\t\tLoss: 0.1714 \n",
            "| Epoch [ 10/ 60] Iter[ 71/251]\t\tLoss: 0.9572 \n",
            "| Epoch [ 10/ 60] Iter[ 76/251]\t\tLoss: 1.0695 \n",
            "| Epoch [ 10/ 60] Iter[ 81/251]\t\tLoss: 0.1710 \n",
            "| Epoch [ 10/ 60] Iter[ 86/251]\t\tLoss: 0.4675 \n",
            "| Epoch [ 10/ 60] Iter[ 91/251]\t\tLoss: 0.3384 \n",
            "| Epoch [ 10/ 60] Iter[ 96/251]\t\tLoss: 0.5260 \n",
            "| Epoch [ 10/ 60] Iter[101/251]\t\tLoss: 0.4066 \n",
            "| Epoch [ 10/ 60] Iter[106/251]\t\tLoss: 1.0505 \n",
            "| Epoch [ 10/ 60] Iter[111/251]\t\tLoss: 1.3156 \n",
            "| Epoch [ 10/ 60] Iter[116/251]\t\tLoss: 0.5063 \n",
            "| Epoch [ 10/ 60] Iter[121/251]\t\tLoss: 0.4638 \n",
            "| Epoch [ 10/ 60] Iter[126/251]\t\tLoss: 0.2497 \n",
            "| Epoch [ 10/ 60] Iter[131/251]\t\tLoss: 0.3082 \n",
            "| Epoch [ 10/ 60] Iter[136/251]\t\tLoss: 0.2755 \n",
            "| Epoch [ 10/ 60] Iter[141/251]\t\tLoss: 0.4004 \n",
            "| Epoch [ 10/ 60] Iter[146/251]\t\tLoss: 0.3439 \n",
            "| Epoch [ 10/ 60] Iter[151/251]\t\tLoss: 6.0938 \n",
            "| Epoch [ 10/ 60] Iter[156/251]\t\tLoss: 0.1826 \n",
            "| Epoch [ 10/ 60] Iter[161/251]\t\tLoss: 0.0981 \n",
            "| Epoch [ 10/ 60] Iter[166/251]\t\tLoss: 0.2735 \n",
            "| Epoch [ 10/ 60] Iter[171/251]\t\tLoss: 0.7248 \n",
            "| Epoch [ 10/ 60] Iter[176/251]\t\tLoss: 0.3293 \n",
            "| Epoch [ 10/ 60] Iter[181/251]\t\tLoss: 0.2349 \n",
            "| Epoch [ 10/ 60] Iter[186/251]\t\tLoss: 0.5760 \n",
            "| Epoch [ 10/ 60] Iter[191/251]\t\tLoss: 0.3286 \n",
            "| Epoch [ 10/ 60] Iter[196/251]\t\tLoss: 0.2314 \n",
            "| Epoch [ 10/ 60] Iter[201/251]\t\tLoss: 0.4012 \n",
            "| Epoch [ 10/ 60] Iter[206/251]\t\tLoss: 0.1017 \n",
            "| Epoch [ 10/ 60] Iter[211/251]\t\tLoss: 1.8020 \n",
            "| Epoch [ 10/ 60] Iter[216/251]\t\tLoss: 0.3972 \n",
            "| Epoch [ 10/ 60] Iter[221/251]\t\tLoss: 6.7437 \n",
            "| Epoch [ 10/ 60] Iter[226/251]\t\tLoss: 0.1431 \n",
            "| Epoch [ 10/ 60] Iter[231/251]\t\tLoss: 0.5719 \n",
            "| Epoch [ 10/ 60] Iter[236/251]\t\tLoss: 0.1818 \n",
            "| Epoch [ 10/ 60] Iter[241/251]\t\tLoss: 0.1340 \n",
            "| Epoch [ 10/ 60] Iter[246/251]\t\tLoss: 0.8366 \n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 0.2416 \n",
            "\n",
            "=> Training Epoch #11, LR=0.0001\n",
            "| Epoch [ 11/ 60] Iter[  1/251]\t\tLoss: 0.7484 \n",
            "| Epoch [ 11/ 60] Iter[  6/251]\t\tLoss: 0.6016 \n",
            "| Epoch [ 11/ 60] Iter[ 11/251]\t\tLoss: 0.3368 \n",
            "| Epoch [ 11/ 60] Iter[ 16/251]\t\tLoss: 1.1967 \n",
            "| Epoch [ 11/ 60] Iter[ 21/251]\t\tLoss: 0.2186 \n",
            "| Epoch [ 11/ 60] Iter[ 26/251]\t\tLoss: 0.1679 \n",
            "| Epoch [ 11/ 60] Iter[ 31/251]\t\tLoss: 0.9194 \n",
            "| Epoch [ 11/ 60] Iter[ 36/251]\t\tLoss: 0.3663 \n",
            "| Epoch [ 11/ 60] Iter[ 41/251]\t\tLoss: 0.0400 \n",
            "| Epoch [ 11/ 60] Iter[ 46/251]\t\tLoss: 0.3275 \n",
            "| Epoch [ 11/ 60] Iter[ 51/251]\t\tLoss: 0.6911 \n",
            "| Epoch [ 11/ 60] Iter[ 56/251]\t\tLoss: 1.0229 \n",
            "| Epoch [ 11/ 60] Iter[ 61/251]\t\tLoss: 0.3105 \n",
            "| Epoch [ 11/ 60] Iter[ 66/251]\t\tLoss: 0.6951 \n",
            "| Epoch [ 11/ 60] Iter[ 71/251]\t\tLoss: 0.5179 \n",
            "| Epoch [ 11/ 60] Iter[ 76/251]\t\tLoss: 0.0702 \n",
            "| Epoch [ 11/ 60] Iter[ 81/251]\t\tLoss: 0.4871 \n",
            "| Epoch [ 11/ 60] Iter[ 86/251]\t\tLoss: 5.8631 \n",
            "| Epoch [ 11/ 60] Iter[ 91/251]\t\tLoss: 0.5578 \n",
            "| Epoch [ 11/ 60] Iter[ 96/251]\t\tLoss: 0.8711 \n",
            "| Epoch [ 11/ 60] Iter[101/251]\t\tLoss: 0.2333 \n",
            "| Epoch [ 11/ 60] Iter[106/251]\t\tLoss: 0.6564 \n",
            "| Epoch [ 11/ 60] Iter[111/251]\t\tLoss: 0.6818 \n",
            "| Epoch [ 11/ 60] Iter[116/251]\t\tLoss: 1.0223 \n",
            "| Epoch [ 11/ 60] Iter[121/251]\t\tLoss: 0.1170 \n",
            "| Epoch [ 11/ 60] Iter[126/251]\t\tLoss: 1.1663 \n",
            "| Epoch [ 11/ 60] Iter[131/251]\t\tLoss: 0.2926 \n",
            "| Epoch [ 11/ 60] Iter[136/251]\t\tLoss: 0.8643 \n",
            "| Epoch [ 11/ 60] Iter[141/251]\t\tLoss: 0.4224 \n",
            "| Epoch [ 11/ 60] Iter[146/251]\t\tLoss: 6.4809 \n",
            "| Epoch [ 11/ 60] Iter[151/251]\t\tLoss: 0.2600 \n",
            "| Epoch [ 11/ 60] Iter[156/251]\t\tLoss: 0.1578 \n",
            "| Epoch [ 11/ 60] Iter[161/251]\t\tLoss: 0.4656 \n",
            "| Epoch [ 11/ 60] Iter[166/251]\t\tLoss: 0.0492 \n",
            "| Epoch [ 11/ 60] Iter[171/251]\t\tLoss: 0.2772 \n",
            "| Epoch [ 11/ 60] Iter[176/251]\t\tLoss: 0.5607 \n",
            "| Epoch [ 11/ 60] Iter[181/251]\t\tLoss: 0.6554 \n",
            "| Epoch [ 11/ 60] Iter[186/251]\t\tLoss: 0.2353 \n",
            "| Epoch [ 11/ 60] Iter[191/251]\t\tLoss: 0.1745 \n",
            "| Epoch [ 11/ 60] Iter[196/251]\t\tLoss: 0.4758 \n",
            "| Epoch [ 11/ 60] Iter[201/251]\t\tLoss: 0.5159 \n",
            "| Epoch [ 11/ 60] Iter[206/251]\t\tLoss: 0.4222 \n",
            "| Epoch [ 11/ 60] Iter[211/251]\t\tLoss: 0.9077 \n",
            "| Epoch [ 11/ 60] Iter[216/251]\t\tLoss: 0.4111 \n",
            "| Epoch [ 11/ 60] Iter[221/251]\t\tLoss: 0.2985 \n",
            "| Epoch [ 11/ 60] Iter[226/251]\t\tLoss: 2.8215 \n",
            "| Epoch [ 11/ 60] Iter[231/251]\t\tLoss: 0.8381 \n",
            "| Epoch [ 11/ 60] Iter[236/251]\t\tLoss: 1.0459 \n",
            "| Epoch [ 11/ 60] Iter[241/251]\t\tLoss: 0.8277 \n",
            "| Epoch [ 11/ 60] Iter[246/251]\t\tLoss: 1.3874 \n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 0.2458 \n",
            "\n",
            "=> Training Epoch #12, LR=0.0001\n",
            "| Epoch [ 12/ 60] Iter[  1/251]\t\tLoss: 0.6886 \n",
            "| Epoch [ 12/ 60] Iter[  6/251]\t\tLoss: 0.1126 \n",
            "| Epoch [ 12/ 60] Iter[ 11/251]\t\tLoss: 1.4064 \n",
            "| Epoch [ 12/ 60] Iter[ 16/251]\t\tLoss: 0.2904 \n",
            "| Epoch [ 12/ 60] Iter[ 21/251]\t\tLoss: 0.7138 \n",
            "| Epoch [ 12/ 60] Iter[ 26/251]\t\tLoss: 0.3141 \n",
            "| Epoch [ 12/ 60] Iter[ 31/251]\t\tLoss: 0.7550 \n",
            "| Epoch [ 12/ 60] Iter[ 36/251]\t\tLoss: 0.1176 \n",
            "| Epoch [ 12/ 60] Iter[ 41/251]\t\tLoss: 0.4025 \n",
            "| Epoch [ 12/ 60] Iter[ 46/251]\t\tLoss: 0.9708 \n",
            "| Epoch [ 12/ 60] Iter[ 51/251]\t\tLoss: 0.5337 \n",
            "| Epoch [ 12/ 60] Iter[ 56/251]\t\tLoss: 0.2366 \n",
            "| Epoch [ 12/ 60] Iter[ 61/251]\t\tLoss: 0.4782 \n",
            "| Epoch [ 12/ 60] Iter[ 66/251]\t\tLoss: 0.4494 \n",
            "| Epoch [ 12/ 60] Iter[ 71/251]\t\tLoss: 0.6565 \n",
            "| Epoch [ 12/ 60] Iter[ 76/251]\t\tLoss: 0.4587 \n",
            "| Epoch [ 12/ 60] Iter[ 81/251]\t\tLoss: 0.5272 \n",
            "| Epoch [ 12/ 60] Iter[ 86/251]\t\tLoss: 3.5621 \n",
            "| Epoch [ 12/ 60] Iter[ 91/251]\t\tLoss: 0.2521 \n",
            "| Epoch [ 12/ 60] Iter[ 96/251]\t\tLoss: 0.6852 \n",
            "| Epoch [ 12/ 60] Iter[101/251]\t\tLoss: 3.1350 \n",
            "| Epoch [ 12/ 60] Iter[106/251]\t\tLoss: 1.0876 \n",
            "| Epoch [ 12/ 60] Iter[111/251]\t\tLoss: 2.1035 \n",
            "| Epoch [ 12/ 60] Iter[116/251]\t\tLoss: 0.8751 \n",
            "| Epoch [ 12/ 60] Iter[121/251]\t\tLoss: 0.7265 \n",
            "| Epoch [ 12/ 60] Iter[126/251]\t\tLoss: 0.3332 \n",
            "| Epoch [ 12/ 60] Iter[131/251]\t\tLoss: 0.5799 \n",
            "| Epoch [ 12/ 60] Iter[136/251]\t\tLoss: 0.3012 \n",
            "| Epoch [ 12/ 60] Iter[141/251]\t\tLoss: 0.3774 \n",
            "| Epoch [ 12/ 60] Iter[146/251]\t\tLoss: 0.4930 \n",
            "| Epoch [ 12/ 60] Iter[151/251]\t\tLoss: 0.1445 \n",
            "| Epoch [ 12/ 60] Iter[156/251]\t\tLoss: 0.9713 \n",
            "| Epoch [ 12/ 60] Iter[161/251]\t\tLoss: 0.2458 \n",
            "| Epoch [ 12/ 60] Iter[166/251]\t\tLoss: 0.2875 \n",
            "| Epoch [ 12/ 60] Iter[171/251]\t\tLoss: 0.5396 \n",
            "| Epoch [ 12/ 60] Iter[176/251]\t\tLoss: 0.1463 \n",
            "| Epoch [ 12/ 60] Iter[181/251]\t\tLoss: 0.1560 \n",
            "| Epoch [ 12/ 60] Iter[186/251]\t\tLoss: 0.1585 \n",
            "| Epoch [ 12/ 60] Iter[191/251]\t\tLoss: 0.1657 \n",
            "| Epoch [ 12/ 60] Iter[196/251]\t\tLoss: 0.2471 \n",
            "| Epoch [ 12/ 60] Iter[201/251]\t\tLoss: 0.5961 \n",
            "| Epoch [ 12/ 60] Iter[206/251]\t\tLoss: 0.6768 \n",
            "| Epoch [ 12/ 60] Iter[211/251]\t\tLoss: 0.7422 \n",
            "| Epoch [ 12/ 60] Iter[216/251]\t\tLoss: 0.5481 \n",
            "| Epoch [ 12/ 60] Iter[221/251]\t\tLoss: 0.1801 \n",
            "| Epoch [ 12/ 60] Iter[226/251]\t\tLoss: 0.2405 \n",
            "| Epoch [ 12/ 60] Iter[231/251]\t\tLoss: 0.3435 \n",
            "| Epoch [ 12/ 60] Iter[236/251]\t\tLoss: 0.6519 \n",
            "| Epoch [ 12/ 60] Iter[241/251]\t\tLoss: 6.4215 \n",
            "| Epoch [ 12/ 60] Iter[246/251]\t\tLoss: 0.2893 \n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 0.2421 \n",
            "\n",
            "=> Training Epoch #13, LR=0.0001\n",
            "| Epoch [ 13/ 60] Iter[  1/251]\t\tLoss: 0.1670 \n",
            "| Epoch [ 13/ 60] Iter[  6/251]\t\tLoss: 0.5509 \n",
            "| Epoch [ 13/ 60] Iter[ 11/251]\t\tLoss: 0.3194 \n",
            "| Epoch [ 13/ 60] Iter[ 16/251]\t\tLoss: 0.2229 \n",
            "| Epoch [ 13/ 60] Iter[ 21/251]\t\tLoss: 0.2008 \n",
            "| Epoch [ 13/ 60] Iter[ 26/251]\t\tLoss: 0.6099 \n",
            "| Epoch [ 13/ 60] Iter[ 31/251]\t\tLoss: 0.1031 \n",
            "| Epoch [ 13/ 60] Iter[ 36/251]\t\tLoss: 6.0077 \n",
            "| Epoch [ 13/ 60] Iter[ 41/251]\t\tLoss: 1.3623 \n",
            "| Epoch [ 13/ 60] Iter[ 46/251]\t\tLoss: 0.6117 \n",
            "| Epoch [ 13/ 60] Iter[ 51/251]\t\tLoss: 0.0598 \n",
            "| Epoch [ 13/ 60] Iter[ 56/251]\t\tLoss: 0.0950 \n",
            "| Epoch [ 13/ 60] Iter[ 61/251]\t\tLoss: 0.6750 \n",
            "| Epoch [ 13/ 60] Iter[ 66/251]\t\tLoss: 0.5130 \n",
            "| Epoch [ 13/ 60] Iter[ 71/251]\t\tLoss: 0.7528 \n",
            "| Epoch [ 13/ 60] Iter[ 76/251]\t\tLoss: 0.1452 \n",
            "| Epoch [ 13/ 60] Iter[ 81/251]\t\tLoss: 0.5104 \n",
            "| Epoch [ 13/ 60] Iter[ 86/251]\t\tLoss: 7.2185 \n",
            "| Epoch [ 13/ 60] Iter[ 91/251]\t\tLoss: 0.2599 \n",
            "| Epoch [ 13/ 60] Iter[ 96/251]\t\tLoss: 0.5936 \n",
            "| Epoch [ 13/ 60] Iter[101/251]\t\tLoss: 0.2953 \n",
            "| Epoch [ 13/ 60] Iter[106/251]\t\tLoss: 4.2079 \n",
            "| Epoch [ 13/ 60] Iter[111/251]\t\tLoss: 0.3581 \n",
            "| Epoch [ 13/ 60] Iter[116/251]\t\tLoss: 0.2439 \n",
            "| Epoch [ 13/ 60] Iter[121/251]\t\tLoss: 5.9689 \n",
            "| Epoch [ 13/ 60] Iter[126/251]\t\tLoss: 0.0624 \n",
            "| Epoch [ 13/ 60] Iter[131/251]\t\tLoss: 0.0823 \n",
            "| Epoch [ 13/ 60] Iter[136/251]\t\tLoss: 0.3300 \n",
            "| Epoch [ 13/ 60] Iter[141/251]\t\tLoss: 0.2537 \n",
            "| Epoch [ 13/ 60] Iter[146/251]\t\tLoss: 0.2345 \n",
            "| Epoch [ 13/ 60] Iter[151/251]\t\tLoss: 0.2633 \n",
            "| Epoch [ 13/ 60] Iter[156/251]\t\tLoss: 0.1137 \n",
            "| Epoch [ 13/ 60] Iter[161/251]\t\tLoss: 1.0804 \n",
            "| Epoch [ 13/ 60] Iter[166/251]\t\tLoss: 0.5759 \n",
            "| Epoch [ 13/ 60] Iter[171/251]\t\tLoss: 0.4052 \n",
            "| Epoch [ 13/ 60] Iter[176/251]\t\tLoss: 0.2259 \n",
            "| Epoch [ 13/ 60] Iter[181/251]\t\tLoss: 0.4781 \n",
            "| Epoch [ 13/ 60] Iter[186/251]\t\tLoss: 0.4155 \n",
            "| Epoch [ 13/ 60] Iter[191/251]\t\tLoss: 0.2320 \n",
            "| Epoch [ 13/ 60] Iter[196/251]\t\tLoss: 0.5838 \n",
            "| Epoch [ 13/ 60] Iter[201/251]\t\tLoss: 0.6447 \n",
            "| Epoch [ 13/ 60] Iter[206/251]\t\tLoss: 0.5235 \n",
            "| Epoch [ 13/ 60] Iter[211/251]\t\tLoss: 0.2900 \n",
            "| Epoch [ 13/ 60] Iter[216/251]\t\tLoss: 0.4625 \n",
            "| Epoch [ 13/ 60] Iter[221/251]\t\tLoss: 0.1325 \n",
            "| Epoch [ 13/ 60] Iter[226/251]\t\tLoss: 0.1375 \n",
            "| Epoch [ 13/ 60] Iter[231/251]\t\tLoss: 1.2705 \n",
            "| Epoch [ 13/ 60] Iter[236/251]\t\tLoss: 0.7375 \n",
            "| Epoch [ 13/ 60] Iter[241/251]\t\tLoss: 1.0200 \n",
            "| Epoch [ 13/ 60] Iter[246/251]\t\tLoss: 0.2595 \n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 0.2296 \n",
            "\n",
            "=> Training Epoch #14, LR=0.0001\n",
            "| Epoch [ 14/ 60] Iter[  1/251]\t\tLoss: 0.6786 \n",
            "| Epoch [ 14/ 60] Iter[  6/251]\t\tLoss: 0.5338 \n",
            "| Epoch [ 14/ 60] Iter[ 11/251]\t\tLoss: 0.7087 \n",
            "| Epoch [ 14/ 60] Iter[ 16/251]\t\tLoss: 0.9290 \n",
            "| Epoch [ 14/ 60] Iter[ 21/251]\t\tLoss: 0.2719 \n",
            "| Epoch [ 14/ 60] Iter[ 26/251]\t\tLoss: 0.2359 \n",
            "| Epoch [ 14/ 60] Iter[ 31/251]\t\tLoss: 0.5279 \n",
            "| Epoch [ 14/ 60] Iter[ 36/251]\t\tLoss: 0.5791 \n",
            "| Epoch [ 14/ 60] Iter[ 41/251]\t\tLoss: 0.3095 \n",
            "| Epoch [ 14/ 60] Iter[ 46/251]\t\tLoss: 0.9993 \n",
            "| Epoch [ 14/ 60] Iter[ 51/251]\t\tLoss: 0.5956 \n",
            "| Epoch [ 14/ 60] Iter[ 56/251]\t\tLoss: 6.3442 \n",
            "| Epoch [ 14/ 60] Iter[ 61/251]\t\tLoss: 0.8323 \n",
            "| Epoch [ 14/ 60] Iter[ 66/251]\t\tLoss: 0.2611 \n",
            "| Epoch [ 14/ 60] Iter[ 71/251]\t\tLoss: 0.3671 \n",
            "| Epoch [ 14/ 60] Iter[ 76/251]\t\tLoss: 0.1549 \n",
            "| Epoch [ 14/ 60] Iter[ 81/251]\t\tLoss: 11.3268 \n",
            "| Epoch [ 14/ 60] Iter[ 86/251]\t\tLoss: 0.5140 \n",
            "| Epoch [ 14/ 60] Iter[ 91/251]\t\tLoss: 0.7163 \n",
            "| Epoch [ 14/ 60] Iter[ 96/251]\t\tLoss: 1.0736 \n",
            "| Epoch [ 14/ 60] Iter[101/251]\t\tLoss: 5.8983 \n",
            "| Epoch [ 14/ 60] Iter[106/251]\t\tLoss: 0.3604 \n",
            "| Epoch [ 14/ 60] Iter[111/251]\t\tLoss: 0.2635 \n",
            "| Epoch [ 14/ 60] Iter[116/251]\t\tLoss: 0.4281 \n",
            "| Epoch [ 14/ 60] Iter[121/251]\t\tLoss: 0.3580 \n",
            "| Epoch [ 14/ 60] Iter[126/251]\t\tLoss: 0.1763 \n",
            "| Epoch [ 14/ 60] Iter[131/251]\t\tLoss: 0.3953 \n",
            "| Epoch [ 14/ 60] Iter[136/251]\t\tLoss: 0.4636 \n",
            "| Epoch [ 14/ 60] Iter[141/251]\t\tLoss: 6.0427 \n",
            "| Epoch [ 14/ 60] Iter[146/251]\t\tLoss: 2.7178 \n",
            "| Epoch [ 14/ 60] Iter[151/251]\t\tLoss: 0.4018 \n",
            "| Epoch [ 14/ 60] Iter[156/251]\t\tLoss: 1.3522 \n",
            "| Epoch [ 14/ 60] Iter[161/251]\t\tLoss: 0.3433 \n",
            "| Epoch [ 14/ 60] Iter[166/251]\t\tLoss: 0.1326 \n",
            "| Epoch [ 14/ 60] Iter[171/251]\t\tLoss: 0.2522 \n",
            "| Epoch [ 14/ 60] Iter[176/251]\t\tLoss: 0.1382 \n",
            "| Epoch [ 14/ 60] Iter[181/251]\t\tLoss: 0.4875 \n",
            "| Epoch [ 14/ 60] Iter[186/251]\t\tLoss: 0.3781 \n",
            "| Epoch [ 14/ 60] Iter[191/251]\t\tLoss: 0.4254 \n",
            "| Epoch [ 14/ 60] Iter[196/251]\t\tLoss: 0.0873 \n",
            "| Epoch [ 14/ 60] Iter[201/251]\t\tLoss: 0.4359 \n",
            "| Epoch [ 14/ 60] Iter[206/251]\t\tLoss: 1.1521 \n",
            "| Epoch [ 14/ 60] Iter[211/251]\t\tLoss: 0.0858 \n",
            "| Epoch [ 14/ 60] Iter[216/251]\t\tLoss: 0.3126 \n",
            "| Epoch [ 14/ 60] Iter[221/251]\t\tLoss: 0.3703 \n",
            "| Epoch [ 14/ 60] Iter[226/251]\t\tLoss: 0.2322 \n",
            "| Epoch [ 14/ 60] Iter[231/251]\t\tLoss: 0.5977 \n",
            "| Epoch [ 14/ 60] Iter[236/251]\t\tLoss: 0.2894 \n",
            "| Epoch [ 14/ 60] Iter[241/251]\t\tLoss: 0.3417 \n",
            "| Epoch [ 14/ 60] Iter[246/251]\t\tLoss: 0.3120 \n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 0.2267 \n",
            "\n",
            "=> Training Epoch #15, LR=0.0001\n",
            "| Epoch [ 15/ 60] Iter[  1/251]\t\tLoss: 6.0789 \n",
            "| Epoch [ 15/ 60] Iter[  6/251]\t\tLoss: 0.5184 \n",
            "| Epoch [ 15/ 60] Iter[ 11/251]\t\tLoss: 1.3801 \n",
            "| Epoch [ 15/ 60] Iter[ 16/251]\t\tLoss: 0.2902 \n",
            "| Epoch [ 15/ 60] Iter[ 21/251]\t\tLoss: 0.3431 \n",
            "| Epoch [ 15/ 60] Iter[ 26/251]\t\tLoss: 0.7364 \n",
            "| Epoch [ 15/ 60] Iter[ 31/251]\t\tLoss: 0.7708 \n",
            "| Epoch [ 15/ 60] Iter[ 36/251]\t\tLoss: 0.0545 \n",
            "| Epoch [ 15/ 60] Iter[ 41/251]\t\tLoss: 0.9647 \n",
            "| Epoch [ 15/ 60] Iter[ 46/251]\t\tLoss: 0.1699 \n",
            "| Epoch [ 15/ 60] Iter[ 51/251]\t\tLoss: 0.2004 \n",
            "| Epoch [ 15/ 60] Iter[ 56/251]\t\tLoss: 0.2928 \n",
            "| Epoch [ 15/ 60] Iter[ 61/251]\t\tLoss: 0.4766 \n",
            "| Epoch [ 15/ 60] Iter[ 66/251]\t\tLoss: 0.4059 \n",
            "| Epoch [ 15/ 60] Iter[ 71/251]\t\tLoss: 0.5637 \n",
            "| Epoch [ 15/ 60] Iter[ 76/251]\t\tLoss: 0.3934 \n",
            "| Epoch [ 15/ 60] Iter[ 81/251]\t\tLoss: 0.3691 \n",
            "| Epoch [ 15/ 60] Iter[ 86/251]\t\tLoss: 6.3340 \n",
            "| Epoch [ 15/ 60] Iter[ 91/251]\t\tLoss: 6.5287 \n",
            "| Epoch [ 15/ 60] Iter[ 96/251]\t\tLoss: 0.3037 \n",
            "| Epoch [ 15/ 60] Iter[101/251]\t\tLoss: 0.4835 \n",
            "| Epoch [ 15/ 60] Iter[106/251]\t\tLoss: 0.4049 \n",
            "| Epoch [ 15/ 60] Iter[111/251]\t\tLoss: 0.2417 \n",
            "| Epoch [ 15/ 60] Iter[116/251]\t\tLoss: 0.6842 \n",
            "| Epoch [ 15/ 60] Iter[121/251]\t\tLoss: 0.6336 \n",
            "| Epoch [ 15/ 60] Iter[126/251]\t\tLoss: 0.2978 \n",
            "| Epoch [ 15/ 60] Iter[131/251]\t\tLoss: 0.4662 \n",
            "| Epoch [ 15/ 60] Iter[136/251]\t\tLoss: 0.3223 \n",
            "| Epoch [ 15/ 60] Iter[141/251]\t\tLoss: 0.1993 \n",
            "| Epoch [ 15/ 60] Iter[146/251]\t\tLoss: 0.5909 \n",
            "| Epoch [ 15/ 60] Iter[151/251]\t\tLoss: 0.4392 \n",
            "| Epoch [ 15/ 60] Iter[156/251]\t\tLoss: 0.2904 \n",
            "| Epoch [ 15/ 60] Iter[161/251]\t\tLoss: 1.1272 \n",
            "| Epoch [ 15/ 60] Iter[166/251]\t\tLoss: 0.3575 \n",
            "| Epoch [ 15/ 60] Iter[171/251]\t\tLoss: 6.1554 \n",
            "| Epoch [ 15/ 60] Iter[176/251]\t\tLoss: 0.4720 \n",
            "| Epoch [ 15/ 60] Iter[181/251]\t\tLoss: 0.3520 \n",
            "| Epoch [ 15/ 60] Iter[186/251]\t\tLoss: 0.2172 \n",
            "| Epoch [ 15/ 60] Iter[191/251]\t\tLoss: 3.8530 \n",
            "| Epoch [ 15/ 60] Iter[196/251]\t\tLoss: 1.0259 \n",
            "| Epoch [ 15/ 60] Iter[201/251]\t\tLoss: 0.4954 \n",
            "| Epoch [ 15/ 60] Iter[206/251]\t\tLoss: 0.5563 \n",
            "| Epoch [ 15/ 60] Iter[211/251]\t\tLoss: 6.0593 \n",
            "| Epoch [ 15/ 60] Iter[216/251]\t\tLoss: 6.2758 \n",
            "| Epoch [ 15/ 60] Iter[221/251]\t\tLoss: 0.5519 \n",
            "| Epoch [ 15/ 60] Iter[226/251]\t\tLoss: 2.8830 \n",
            "| Epoch [ 15/ 60] Iter[231/251]\t\tLoss: 1.3008 \n",
            "| Epoch [ 15/ 60] Iter[236/251]\t\tLoss: 1.0398 \n",
            "| Epoch [ 15/ 60] Iter[241/251]\t\tLoss: 0.5384 \n",
            "| Epoch [ 15/ 60] Iter[246/251]\t\tLoss: 0.3257 \n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 0.2196 \n",
            "\n",
            "=> Training Epoch #16, LR=0.0001\n",
            "| Epoch [ 16/ 60] Iter[  1/251]\t\tLoss: 0.4152 \n",
            "| Epoch [ 16/ 60] Iter[  6/251]\t\tLoss: 0.5660 \n",
            "| Epoch [ 16/ 60] Iter[ 11/251]\t\tLoss: 0.3931 \n",
            "| Epoch [ 16/ 60] Iter[ 16/251]\t\tLoss: 0.1449 \n",
            "| Epoch [ 16/ 60] Iter[ 21/251]\t\tLoss: 0.8617 \n",
            "| Epoch [ 16/ 60] Iter[ 26/251]\t\tLoss: 1.6139 \n",
            "| Epoch [ 16/ 60] Iter[ 31/251]\t\tLoss: 0.4146 \n",
            "| Epoch [ 16/ 60] Iter[ 36/251]\t\tLoss: 0.7455 \n",
            "| Epoch [ 16/ 60] Iter[ 41/251]\t\tLoss: 6.3569 \n",
            "| Epoch [ 16/ 60] Iter[ 46/251]\t\tLoss: 0.4381 \n",
            "| Epoch [ 16/ 60] Iter[ 51/251]\t\tLoss: 0.3088 \n",
            "| Epoch [ 16/ 60] Iter[ 56/251]\t\tLoss: 0.4956 \n",
            "| Epoch [ 16/ 60] Iter[ 61/251]\t\tLoss: 0.4504 \n",
            "| Epoch [ 16/ 60] Iter[ 66/251]\t\tLoss: 0.6652 \n",
            "| Epoch [ 16/ 60] Iter[ 71/251]\t\tLoss: 0.4898 \n",
            "| Epoch [ 16/ 60] Iter[ 76/251]\t\tLoss: 0.1932 \n",
            "| Epoch [ 16/ 60] Iter[ 81/251]\t\tLoss: 0.5946 \n",
            "| Epoch [ 16/ 60] Iter[ 86/251]\t\tLoss: 0.4076 \n",
            "| Epoch [ 16/ 60] Iter[ 91/251]\t\tLoss: 0.4941 \n",
            "| Epoch [ 16/ 60] Iter[ 96/251]\t\tLoss: 0.5211 \n",
            "| Epoch [ 16/ 60] Iter[101/251]\t\tLoss: 0.3404 \n",
            "| Epoch [ 16/ 60] Iter[106/251]\t\tLoss: 0.5500 \n",
            "| Epoch [ 16/ 60] Iter[111/251]\t\tLoss: 0.7579 \n",
            "| Epoch [ 16/ 60] Iter[116/251]\t\tLoss: 6.1476 \n",
            "| Epoch [ 16/ 60] Iter[121/251]\t\tLoss: 6.3473 \n",
            "| Epoch [ 16/ 60] Iter[126/251]\t\tLoss: 0.3972 \n",
            "| Epoch [ 16/ 60] Iter[131/251]\t\tLoss: 0.1651 \n",
            "| Epoch [ 16/ 60] Iter[136/251]\t\tLoss: 0.5541 \n",
            "| Epoch [ 16/ 60] Iter[141/251]\t\tLoss: 0.6627 \n",
            "| Epoch [ 16/ 60] Iter[146/251]\t\tLoss: 0.9109 \n",
            "| Epoch [ 16/ 60] Iter[151/251]\t\tLoss: 0.4583 \n",
            "| Epoch [ 16/ 60] Iter[156/251]\t\tLoss: 1.3097 \n",
            "| Epoch [ 16/ 60] Iter[161/251]\t\tLoss: 0.7819 \n",
            "| Epoch [ 16/ 60] Iter[166/251]\t\tLoss: 0.4312 \n",
            "| Epoch [ 16/ 60] Iter[171/251]\t\tLoss: 0.2776 \n",
            "| Epoch [ 16/ 60] Iter[176/251]\t\tLoss: 0.3007 \n",
            "| Epoch [ 16/ 60] Iter[181/251]\t\tLoss: 0.8762 \n",
            "| Epoch [ 16/ 60] Iter[186/251]\t\tLoss: 0.6693 \n",
            "| Epoch [ 16/ 60] Iter[191/251]\t\tLoss: 0.3665 \n",
            "| Epoch [ 16/ 60] Iter[196/251]\t\tLoss: 0.2480 \n",
            "| Epoch [ 16/ 60] Iter[201/251]\t\tLoss: 6.0578 \n",
            "| Epoch [ 16/ 60] Iter[206/251]\t\tLoss: 0.2893 \n",
            "| Epoch [ 16/ 60] Iter[211/251]\t\tLoss: 0.2152 \n",
            "| Epoch [ 16/ 60] Iter[216/251]\t\tLoss: 0.2068 \n",
            "| Epoch [ 16/ 60] Iter[221/251]\t\tLoss: 0.6389 \n",
            "| Epoch [ 16/ 60] Iter[226/251]\t\tLoss: 0.2541 \n",
            "| Epoch [ 16/ 60] Iter[231/251]\t\tLoss: 0.4738 \n",
            "| Epoch [ 16/ 60] Iter[236/251]\t\tLoss: 0.1651 \n",
            "| Epoch [ 16/ 60] Iter[241/251]\t\tLoss: 1.2167 \n",
            "| Epoch [ 16/ 60] Iter[246/251]\t\tLoss: 1.5922 \n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 0.2325 \n",
            "\n",
            "=> Training Epoch #17, LR=0.0001\n",
            "| Epoch [ 17/ 60] Iter[  1/251]\t\tLoss: 0.4707 \n",
            "| Epoch [ 17/ 60] Iter[  6/251]\t\tLoss: 0.9564 \n",
            "| Epoch [ 17/ 60] Iter[ 11/251]\t\tLoss: 0.9843 \n",
            "| Epoch [ 17/ 60] Iter[ 16/251]\t\tLoss: 0.9894 \n",
            "| Epoch [ 17/ 60] Iter[ 21/251]\t\tLoss: 0.3394 \n",
            "| Epoch [ 17/ 60] Iter[ 26/251]\t\tLoss: 0.4135 \n",
            "| Epoch [ 17/ 60] Iter[ 31/251]\t\tLoss: 1.0811 \n",
            "| Epoch [ 17/ 60] Iter[ 36/251]\t\tLoss: 0.2739 \n",
            "| Epoch [ 17/ 60] Iter[ 41/251]\t\tLoss: 0.2344 \n",
            "| Epoch [ 17/ 60] Iter[ 46/251]\t\tLoss: 0.0663 \n",
            "| Epoch [ 17/ 60] Iter[ 51/251]\t\tLoss: 0.2370 \n",
            "| Epoch [ 17/ 60] Iter[ 56/251]\t\tLoss: 0.2559 \n",
            "| Epoch [ 17/ 60] Iter[ 61/251]\t\tLoss: 0.3919 \n",
            "| Epoch [ 17/ 60] Iter[ 66/251]\t\tLoss: 7.1502 \n",
            "| Epoch [ 17/ 60] Iter[ 71/251]\t\tLoss: 0.0479 \n",
            "| Epoch [ 17/ 60] Iter[ 76/251]\t\tLoss: 0.3567 \n",
            "| Epoch [ 17/ 60] Iter[ 81/251]\t\tLoss: 0.5354 \n",
            "| Epoch [ 17/ 60] Iter[ 86/251]\t\tLoss: 0.9749 \n",
            "| Epoch [ 17/ 60] Iter[ 91/251]\t\tLoss: 0.3538 \n",
            "| Epoch [ 17/ 60] Iter[ 96/251]\t\tLoss: 1.3302 \n",
            "| Epoch [ 17/ 60] Iter[101/251]\t\tLoss: 0.2909 \n",
            "| Epoch [ 17/ 60] Iter[106/251]\t\tLoss: 5.8751 \n",
            "| Epoch [ 17/ 60] Iter[111/251]\t\tLoss: 0.2809 \n",
            "| Epoch [ 17/ 60] Iter[116/251]\t\tLoss: 0.2895 \n",
            "| Epoch [ 17/ 60] Iter[121/251]\t\tLoss: 0.6526 \n",
            "| Epoch [ 17/ 60] Iter[126/251]\t\tLoss: 0.4245 \n",
            "| Epoch [ 17/ 60] Iter[131/251]\t\tLoss: 6.1974 \n",
            "| Epoch [ 17/ 60] Iter[136/251]\t\tLoss: 0.6232 \n",
            "| Epoch [ 17/ 60] Iter[141/251]\t\tLoss: 0.5689 \n",
            "| Epoch [ 17/ 60] Iter[146/251]\t\tLoss: 0.9162 \n",
            "| Epoch [ 17/ 60] Iter[151/251]\t\tLoss: 0.4588 \n",
            "| Epoch [ 17/ 60] Iter[156/251]\t\tLoss: 0.4237 \n",
            "| Epoch [ 17/ 60] Iter[161/251]\t\tLoss: 0.4055 \n",
            "| Epoch [ 17/ 60] Iter[166/251]\t\tLoss: 0.2430 \n",
            "| Epoch [ 17/ 60] Iter[171/251]\t\tLoss: 0.7108 \n",
            "| Epoch [ 17/ 60] Iter[176/251]\t\tLoss: 0.6692 \n",
            "| Epoch [ 17/ 60] Iter[181/251]\t\tLoss: 0.1026 \n",
            "| Epoch [ 17/ 60] Iter[186/251]\t\tLoss: 6.7160 \n",
            "| Epoch [ 17/ 60] Iter[191/251]\t\tLoss: 0.1527 \n",
            "| Epoch [ 17/ 60] Iter[196/251]\t\tLoss: 0.1750 \n",
            "| Epoch [ 17/ 60] Iter[201/251]\t\tLoss: 1.5171 \n",
            "| Epoch [ 17/ 60] Iter[206/251]\t\tLoss: 0.2817 \n",
            "| Epoch [ 17/ 60] Iter[211/251]\t\tLoss: 0.4464 \n",
            "| Epoch [ 17/ 60] Iter[216/251]\t\tLoss: 0.2341 \n",
            "| Epoch [ 17/ 60] Iter[221/251]\t\tLoss: 0.4373 \n",
            "| Epoch [ 17/ 60] Iter[226/251]\t\tLoss: 0.2655 \n",
            "| Epoch [ 17/ 60] Iter[231/251]\t\tLoss: 0.1609 \n",
            "| Epoch [ 17/ 60] Iter[236/251]\t\tLoss: 6.6277 \n",
            "| Epoch [ 17/ 60] Iter[241/251]\t\tLoss: 0.1099 \n",
            "| Epoch [ 17/ 60] Iter[246/251]\t\tLoss: 0.2900 \n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 0.2291 \n",
            "\n",
            "=> Training Epoch #18, LR=0.0001\n",
            "| Epoch [ 18/ 60] Iter[  1/251]\t\tLoss: 6.6673 \n",
            "| Epoch [ 18/ 60] Iter[  6/251]\t\tLoss: 0.5426 \n",
            "| Epoch [ 18/ 60] Iter[ 11/251]\t\tLoss: 0.3416 \n",
            "| Epoch [ 18/ 60] Iter[ 16/251]\t\tLoss: 0.9541 \n",
            "| Epoch [ 18/ 60] Iter[ 21/251]\t\tLoss: 0.4945 \n",
            "| Epoch [ 18/ 60] Iter[ 26/251]\t\tLoss: 6.8919 \n",
            "| Epoch [ 18/ 60] Iter[ 31/251]\t\tLoss: 1.6388 \n",
            "| Epoch [ 18/ 60] Iter[ 36/251]\t\tLoss: 0.1803 \n",
            "| Epoch [ 18/ 60] Iter[ 41/251]\t\tLoss: 0.9187 \n",
            "| Epoch [ 18/ 60] Iter[ 46/251]\t\tLoss: 0.9384 \n",
            "| Epoch [ 18/ 60] Iter[ 51/251]\t\tLoss: 0.1256 \n",
            "| Epoch [ 18/ 60] Iter[ 56/251]\t\tLoss: 0.6662 \n",
            "| Epoch [ 18/ 60] Iter[ 61/251]\t\tLoss: 0.2399 \n",
            "| Epoch [ 18/ 60] Iter[ 66/251]\t\tLoss: 0.2856 \n",
            "| Epoch [ 18/ 60] Iter[ 71/251]\t\tLoss: 0.0921 \n",
            "| Epoch [ 18/ 60] Iter[ 76/251]\t\tLoss: 1.0325 \n",
            "| Epoch [ 18/ 60] Iter[ 81/251]\t\tLoss: 6.1612 \n",
            "| Epoch [ 18/ 60] Iter[ 86/251]\t\tLoss: 0.3742 \n",
            "| Epoch [ 18/ 60] Iter[ 91/251]\t\tLoss: 0.0909 \n",
            "| Epoch [ 18/ 60] Iter[ 96/251]\t\tLoss: 0.5075 \n",
            "| Epoch [ 18/ 60] Iter[101/251]\t\tLoss: 0.4139 \n",
            "| Epoch [ 18/ 60] Iter[106/251]\t\tLoss: 0.3785 \n",
            "| Epoch [ 18/ 60] Iter[111/251]\t\tLoss: 0.0437 \n",
            "| Epoch [ 18/ 60] Iter[116/251]\t\tLoss: 0.2275 \n",
            "| Epoch [ 18/ 60] Iter[121/251]\t\tLoss: 6.4381 \n",
            "| Epoch [ 18/ 60] Iter[126/251]\t\tLoss: 0.2160 \n",
            "| Epoch [ 18/ 60] Iter[131/251]\t\tLoss: 6.5790 \n",
            "| Epoch [ 18/ 60] Iter[136/251]\t\tLoss: 0.1080 \n",
            "| Epoch [ 18/ 60] Iter[141/251]\t\tLoss: 1.0443 \n",
            "| Epoch [ 18/ 60] Iter[146/251]\t\tLoss: 0.3208 \n",
            "| Epoch [ 18/ 60] Iter[151/251]\t\tLoss: 0.5439 \n",
            "| Epoch [ 18/ 60] Iter[156/251]\t\tLoss: 0.5290 \n",
            "| Epoch [ 18/ 60] Iter[161/251]\t\tLoss: 0.1528 \n",
            "| Epoch [ 18/ 60] Iter[166/251]\t\tLoss: 0.1704 \n",
            "| Epoch [ 18/ 60] Iter[171/251]\t\tLoss: 0.6929 \n",
            "| Epoch [ 18/ 60] Iter[176/251]\t\tLoss: 0.7379 \n",
            "| Epoch [ 18/ 60] Iter[181/251]\t\tLoss: 0.5015 \n",
            "| Epoch [ 18/ 60] Iter[186/251]\t\tLoss: 6.3063 \n",
            "| Epoch [ 18/ 60] Iter[191/251]\t\tLoss: 1.1997 \n",
            "| Epoch [ 18/ 60] Iter[196/251]\t\tLoss: 0.3013 \n",
            "| Epoch [ 18/ 60] Iter[201/251]\t\tLoss: 0.9336 \n",
            "| Epoch [ 18/ 60] Iter[206/251]\t\tLoss: 0.2200 \n",
            "| Epoch [ 18/ 60] Iter[211/251]\t\tLoss: 0.8771 \n",
            "| Epoch [ 18/ 60] Iter[216/251]\t\tLoss: 0.2463 \n",
            "| Epoch [ 18/ 60] Iter[221/251]\t\tLoss: 0.3956 \n",
            "| Epoch [ 18/ 60] Iter[226/251]\t\tLoss: 0.4145 \n",
            "| Epoch [ 18/ 60] Iter[231/251]\t\tLoss: 2.8656 \n",
            "| Epoch [ 18/ 60] Iter[236/251]\t\tLoss: 0.1611 \n",
            "| Epoch [ 18/ 60] Iter[241/251]\t\tLoss: 0.1616 \n",
            "| Epoch [ 18/ 60] Iter[246/251]\t\tLoss: 0.6469 \n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 0.2190 \n",
            "\n",
            "=> Training Epoch #19, LR=0.0001\n",
            "| Epoch [ 19/ 60] Iter[  1/251]\t\tLoss: 0.9819 \n",
            "| Epoch [ 19/ 60] Iter[  6/251]\t\tLoss: 0.4455 \n",
            "| Epoch [ 19/ 60] Iter[ 11/251]\t\tLoss: 1.3810 \n",
            "| Epoch [ 19/ 60] Iter[ 16/251]\t\tLoss: 0.4120 \n",
            "| Epoch [ 19/ 60] Iter[ 21/251]\t\tLoss: 0.6243 \n",
            "| Epoch [ 19/ 60] Iter[ 26/251]\t\tLoss: 0.2947 \n",
            "| Epoch [ 19/ 60] Iter[ 31/251]\t\tLoss: 0.4445 \n",
            "| Epoch [ 19/ 60] Iter[ 36/251]\t\tLoss: 0.5754 \n",
            "| Epoch [ 19/ 60] Iter[ 41/251]\t\tLoss: 0.0884 \n",
            "| Epoch [ 19/ 60] Iter[ 46/251]\t\tLoss: 0.2250 \n",
            "| Epoch [ 19/ 60] Iter[ 51/251]\t\tLoss: 0.9606 \n",
            "| Epoch [ 19/ 60] Iter[ 56/251]\t\tLoss: 0.0542 \n",
            "| Epoch [ 19/ 60] Iter[ 61/251]\t\tLoss: 0.3733 \n",
            "| Epoch [ 19/ 60] Iter[ 66/251]\t\tLoss: 0.3552 \n",
            "| Epoch [ 19/ 60] Iter[ 71/251]\t\tLoss: 0.2299 \n",
            "| Epoch [ 19/ 60] Iter[ 76/251]\t\tLoss: 0.5107 \n",
            "| Epoch [ 19/ 60] Iter[ 81/251]\t\tLoss: 1.3112 \n",
            "| Epoch [ 19/ 60] Iter[ 86/251]\t\tLoss: 0.2301 \n",
            "| Epoch [ 19/ 60] Iter[ 91/251]\t\tLoss: 0.9095 \n",
            "| Epoch [ 19/ 60] Iter[ 96/251]\t\tLoss: 0.2647 \n",
            "| Epoch [ 19/ 60] Iter[101/251]\t\tLoss: 0.2540 \n",
            "| Epoch [ 19/ 60] Iter[106/251]\t\tLoss: 6.2043 \n",
            "| Epoch [ 19/ 60] Iter[111/251]\t\tLoss: 0.5203 \n",
            "| Epoch [ 19/ 60] Iter[116/251]\t\tLoss: 0.2650 \n",
            "| Epoch [ 19/ 60] Iter[121/251]\t\tLoss: 0.2008 \n",
            "| Epoch [ 19/ 60] Iter[126/251]\t\tLoss: 0.2014 \n",
            "| Epoch [ 19/ 60] Iter[131/251]\t\tLoss: 7.3822 \n",
            "| Epoch [ 19/ 60] Iter[136/251]\t\tLoss: 0.1732 \n",
            "| Epoch [ 19/ 60] Iter[141/251]\t\tLoss: 6.0427 \n",
            "| Epoch [ 19/ 60] Iter[146/251]\t\tLoss: 6.7411 \n",
            "| Epoch [ 19/ 60] Iter[151/251]\t\tLoss: 0.7228 \n",
            "| Epoch [ 19/ 60] Iter[156/251]\t\tLoss: 0.4043 \n",
            "| Epoch [ 19/ 60] Iter[161/251]\t\tLoss: 0.4766 \n",
            "| Epoch [ 19/ 60] Iter[166/251]\t\tLoss: 0.3479 \n",
            "| Epoch [ 19/ 60] Iter[171/251]\t\tLoss: 0.8127 \n",
            "| Epoch [ 19/ 60] Iter[176/251]\t\tLoss: 0.3755 \n",
            "| Epoch [ 19/ 60] Iter[181/251]\t\tLoss: 0.0502 \n",
            "| Epoch [ 19/ 60] Iter[186/251]\t\tLoss: 0.6452 \n",
            "| Epoch [ 19/ 60] Iter[191/251]\t\tLoss: 0.4767 \n",
            "| Epoch [ 19/ 60] Iter[196/251]\t\tLoss: 0.2116 \n",
            "| Epoch [ 19/ 60] Iter[201/251]\t\tLoss: 0.4368 \n",
            "| Epoch [ 19/ 60] Iter[206/251]\t\tLoss: 0.1567 \n",
            "| Epoch [ 19/ 60] Iter[211/251]\t\tLoss: 0.5074 \n",
            "| Epoch [ 19/ 60] Iter[216/251]\t\tLoss: 0.2593 \n",
            "| Epoch [ 19/ 60] Iter[221/251]\t\tLoss: 0.1714 \n",
            "| Epoch [ 19/ 60] Iter[226/251]\t\tLoss: 0.7777 \n",
            "| Epoch [ 19/ 60] Iter[231/251]\t\tLoss: 0.7421 \n",
            "| Epoch [ 19/ 60] Iter[236/251]\t\tLoss: 0.7106 \n",
            "| Epoch [ 19/ 60] Iter[241/251]\t\tLoss: 0.9381 \n",
            "| Epoch [ 19/ 60] Iter[246/251]\t\tLoss: 1.3098 \n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 0.2132 \n",
            "\n",
            "=> Training Epoch #20, LR=0.0001\n",
            "| Epoch [ 20/ 60] Iter[  1/251]\t\tLoss: 0.2479 \n",
            "| Epoch [ 20/ 60] Iter[  6/251]\t\tLoss: 0.1595 \n",
            "| Epoch [ 20/ 60] Iter[ 11/251]\t\tLoss: 0.2381 \n",
            "| Epoch [ 20/ 60] Iter[ 16/251]\t\tLoss: 0.1895 \n",
            "| Epoch [ 20/ 60] Iter[ 21/251]\t\tLoss: 0.1418 \n",
            "| Epoch [ 20/ 60] Iter[ 26/251]\t\tLoss: 0.3089 \n",
            "| Epoch [ 20/ 60] Iter[ 31/251]\t\tLoss: 0.7831 \n",
            "| Epoch [ 20/ 60] Iter[ 36/251]\t\tLoss: 0.3525 \n",
            "| Epoch [ 20/ 60] Iter[ 41/251]\t\tLoss: 0.7901 \n",
            "| Epoch [ 20/ 60] Iter[ 46/251]\t\tLoss: 0.2591 \n",
            "| Epoch [ 20/ 60] Iter[ 51/251]\t\tLoss: 1.1318 \n",
            "| Epoch [ 20/ 60] Iter[ 56/251]\t\tLoss: 0.5634 \n",
            "| Epoch [ 20/ 60] Iter[ 61/251]\t\tLoss: 1.0259 \n",
            "| Epoch [ 20/ 60] Iter[ 66/251]\t\tLoss: 6.9966 \n",
            "| Epoch [ 20/ 60] Iter[ 71/251]\t\tLoss: 0.3853 \n",
            "| Epoch [ 20/ 60] Iter[ 76/251]\t\tLoss: 0.6062 \n",
            "| Epoch [ 20/ 60] Iter[ 81/251]\t\tLoss: 0.1514 \n",
            "| Epoch [ 20/ 60] Iter[ 86/251]\t\tLoss: 0.9512 \n",
            "| Epoch [ 20/ 60] Iter[ 91/251]\t\tLoss: 6.1944 \n",
            "| Epoch [ 20/ 60] Iter[ 96/251]\t\tLoss: 0.7560 \n",
            "| Epoch [ 20/ 60] Iter[101/251]\t\tLoss: 0.2692 \n",
            "| Epoch [ 20/ 60] Iter[106/251]\t\tLoss: 0.4371 \n",
            "| Epoch [ 20/ 60] Iter[111/251]\t\tLoss: 0.3303 \n",
            "| Epoch [ 20/ 60] Iter[116/251]\t\tLoss: 0.4416 \n",
            "| Epoch [ 20/ 60] Iter[121/251]\t\tLoss: 0.1772 \n",
            "| Epoch [ 20/ 60] Iter[126/251]\t\tLoss: 0.3854 \n",
            "| Epoch [ 20/ 60] Iter[131/251]\t\tLoss: 0.1759 \n",
            "| Epoch [ 20/ 60] Iter[136/251]\t\tLoss: 0.3684 \n",
            "| Epoch [ 20/ 60] Iter[141/251]\t\tLoss: 0.8938 \n",
            "| Epoch [ 20/ 60] Iter[146/251]\t\tLoss: 0.4761 \n",
            "| Epoch [ 20/ 60] Iter[151/251]\t\tLoss: 1.2239 \n",
            "| Epoch [ 20/ 60] Iter[156/251]\t\tLoss: 0.8571 \n",
            "| Epoch [ 20/ 60] Iter[161/251]\t\tLoss: 0.4891 \n",
            "| Epoch [ 20/ 60] Iter[166/251]\t\tLoss: 0.6994 \n",
            "| Epoch [ 20/ 60] Iter[171/251]\t\tLoss: 0.5369 \n",
            "| Epoch [ 20/ 60] Iter[176/251]\t\tLoss: 0.2889 \n",
            "| Epoch [ 20/ 60] Iter[181/251]\t\tLoss: 0.1615 \n",
            "| Epoch [ 20/ 60] Iter[186/251]\t\tLoss: 0.6111 \n",
            "| Epoch [ 20/ 60] Iter[191/251]\t\tLoss: 0.3669 \n",
            "| Epoch [ 20/ 60] Iter[196/251]\t\tLoss: 0.3668 \n",
            "| Epoch [ 20/ 60] Iter[201/251]\t\tLoss: 0.7229 \n",
            "| Epoch [ 20/ 60] Iter[206/251]\t\tLoss: 2.4338 \n",
            "| Epoch [ 20/ 60] Iter[211/251]\t\tLoss: 0.2113 \n",
            "| Epoch [ 20/ 60] Iter[216/251]\t\tLoss: 0.2880 \n",
            "| Epoch [ 20/ 60] Iter[221/251]\t\tLoss: 0.1500 \n",
            "| Epoch [ 20/ 60] Iter[226/251]\t\tLoss: 0.2032 \n",
            "| Epoch [ 20/ 60] Iter[231/251]\t\tLoss: 0.5531 \n",
            "| Epoch [ 20/ 60] Iter[236/251]\t\tLoss: 0.4489 \n",
            "| Epoch [ 20/ 60] Iter[241/251]\t\tLoss: 0.0486 \n",
            "| Epoch [ 20/ 60] Iter[246/251]\t\tLoss: 0.3411 \n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 0.2148 \n",
            "\n",
            "=> Training Epoch #21, LR=0.0000\n",
            "| Epoch [ 21/ 60] Iter[  1/251]\t\tLoss: 0.6324 \n",
            "| Epoch [ 21/ 60] Iter[  6/251]\t\tLoss: 6.1900 \n",
            "| Epoch [ 21/ 60] Iter[ 11/251]\t\tLoss: 0.5253 \n",
            "| Epoch [ 21/ 60] Iter[ 16/251]\t\tLoss: 0.9523 \n",
            "| Epoch [ 21/ 60] Iter[ 21/251]\t\tLoss: 0.3781 \n",
            "| Epoch [ 21/ 60] Iter[ 26/251]\t\tLoss: 0.2412 \n",
            "| Epoch [ 21/ 60] Iter[ 31/251]\t\tLoss: 0.5723 \n",
            "| Epoch [ 21/ 60] Iter[ 36/251]\t\tLoss: 0.6184 \n",
            "| Epoch [ 21/ 60] Iter[ 41/251]\t\tLoss: 0.1274 \n",
            "| Epoch [ 21/ 60] Iter[ 46/251]\t\tLoss: 0.2263 \n",
            "| Epoch [ 21/ 60] Iter[ 51/251]\t\tLoss: 0.6927 \n",
            "| Epoch [ 21/ 60] Iter[ 56/251]\t\tLoss: 0.5455 \n",
            "| Epoch [ 21/ 60] Iter[ 61/251]\t\tLoss: 0.0696 \n",
            "| Epoch [ 21/ 60] Iter[ 66/251]\t\tLoss: 0.4466 \n",
            "| Epoch [ 21/ 60] Iter[ 71/251]\t\tLoss: 0.4776 \n",
            "| Epoch [ 21/ 60] Iter[ 76/251]\t\tLoss: 0.5938 \n",
            "| Epoch [ 21/ 60] Iter[ 81/251]\t\tLoss: 0.1425 \n",
            "| Epoch [ 21/ 60] Iter[ 86/251]\t\tLoss: 0.0856 \n",
            "| Epoch [ 21/ 60] Iter[ 91/251]\t\tLoss: 0.4481 \n",
            "| Epoch [ 21/ 60] Iter[ 96/251]\t\tLoss: 6.1690 \n",
            "| Epoch [ 21/ 60] Iter[101/251]\t\tLoss: 0.6335 \n",
            "| Epoch [ 21/ 60] Iter[106/251]\t\tLoss: 0.4632 \n",
            "| Epoch [ 21/ 60] Iter[111/251]\t\tLoss: 1.0297 \n",
            "| Epoch [ 21/ 60] Iter[116/251]\t\tLoss: 0.2665 \n",
            "| Epoch [ 21/ 60] Iter[121/251]\t\tLoss: 1.4658 \n",
            "| Epoch [ 21/ 60] Iter[126/251]\t\tLoss: 1.1820 \n",
            "| Epoch [ 21/ 60] Iter[131/251]\t\tLoss: 0.3548 \n",
            "| Epoch [ 21/ 60] Iter[136/251]\t\tLoss: 0.4154 \n",
            "| Epoch [ 21/ 60] Iter[141/251]\t\tLoss: 0.8514 \n",
            "| Epoch [ 21/ 60] Iter[146/251]\t\tLoss: 0.4902 \n",
            "| Epoch [ 21/ 60] Iter[151/251]\t\tLoss: 0.2668 \n",
            "| Epoch [ 21/ 60] Iter[156/251]\t\tLoss: 0.7888 \n",
            "| Epoch [ 21/ 60] Iter[161/251]\t\tLoss: 0.2944 \n",
            "| Epoch [ 21/ 60] Iter[166/251]\t\tLoss: 0.3002 \n",
            "| Epoch [ 21/ 60] Iter[171/251]\t\tLoss: 0.1694 \n",
            "| Epoch [ 21/ 60] Iter[176/251]\t\tLoss: 0.3604 \n",
            "| Epoch [ 21/ 60] Iter[181/251]\t\tLoss: 0.9328 \n",
            "| Epoch [ 21/ 60] Iter[186/251]\t\tLoss: 0.2385 \n",
            "| Epoch [ 21/ 60] Iter[191/251]\t\tLoss: 0.2192 \n",
            "| Epoch [ 21/ 60] Iter[196/251]\t\tLoss: 0.2899 \n",
            "| Epoch [ 21/ 60] Iter[201/251]\t\tLoss: 0.0719 \n",
            "| Epoch [ 21/ 60] Iter[206/251]\t\tLoss: 0.2923 \n",
            "| Epoch [ 21/ 60] Iter[211/251]\t\tLoss: 0.6308 \n",
            "| Epoch [ 21/ 60] Iter[216/251]\t\tLoss: 0.5777 \n",
            "| Epoch [ 21/ 60] Iter[221/251]\t\tLoss: 6.4863 \n",
            "| Epoch [ 21/ 60] Iter[226/251]\t\tLoss: 0.5731 \n",
            "| Epoch [ 21/ 60] Iter[231/251]\t\tLoss: 0.3751 \n",
            "| Epoch [ 21/ 60] Iter[236/251]\t\tLoss: 0.3115 \n",
            "| Epoch [ 21/ 60] Iter[241/251]\t\tLoss: 0.6895 \n",
            "| Epoch [ 21/ 60] Iter[246/251]\t\tLoss: 0.4520 \n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 0.2157 \n",
            "\n",
            "=> Training Epoch #22, LR=0.0000\n",
            "| Epoch [ 22/ 60] Iter[  1/251]\t\tLoss: 6.2146 \n",
            "| Epoch [ 22/ 60] Iter[  6/251]\t\tLoss: 0.3905 \n",
            "| Epoch [ 22/ 60] Iter[ 11/251]\t\tLoss: 0.4458 \n",
            "| Epoch [ 22/ 60] Iter[ 16/251]\t\tLoss: 0.2298 \n",
            "| Epoch [ 22/ 60] Iter[ 21/251]\t\tLoss: 0.7531 \n",
            "| Epoch [ 22/ 60] Iter[ 26/251]\t\tLoss: 0.1982 \n",
            "| Epoch [ 22/ 60] Iter[ 31/251]\t\tLoss: 6.3515 \n",
            "| Epoch [ 22/ 60] Iter[ 36/251]\t\tLoss: 0.1098 \n",
            "| Epoch [ 22/ 60] Iter[ 41/251]\t\tLoss: 1.0162 \n",
            "| Epoch [ 22/ 60] Iter[ 46/251]\t\tLoss: 0.4701 \n",
            "| Epoch [ 22/ 60] Iter[ 51/251]\t\tLoss: 0.4894 \n",
            "| Epoch [ 22/ 60] Iter[ 56/251]\t\tLoss: 0.1749 \n",
            "| Epoch [ 22/ 60] Iter[ 61/251]\t\tLoss: 0.4680 \n",
            "| Epoch [ 22/ 60] Iter[ 66/251]\t\tLoss: 0.3015 \n",
            "| Epoch [ 22/ 60] Iter[ 71/251]\t\tLoss: 0.3401 \n",
            "| Epoch [ 22/ 60] Iter[ 76/251]\t\tLoss: 1.0153 \n",
            "| Epoch [ 22/ 60] Iter[ 81/251]\t\tLoss: 0.0739 \n",
            "| Epoch [ 22/ 60] Iter[ 86/251]\t\tLoss: 0.3142 \n",
            "| Epoch [ 22/ 60] Iter[ 91/251]\t\tLoss: 5.7789 \n",
            "| Epoch [ 22/ 60] Iter[ 96/251]\t\tLoss: 0.1045 \n",
            "| Epoch [ 22/ 60] Iter[101/251]\t\tLoss: 0.4139 \n",
            "| Epoch [ 22/ 60] Iter[106/251]\t\tLoss: 0.4203 \n",
            "| Epoch [ 22/ 60] Iter[111/251]\t\tLoss: 0.2454 \n",
            "| Epoch [ 22/ 60] Iter[116/251]\t\tLoss: 5.9843 \n",
            "| Epoch [ 22/ 60] Iter[121/251]\t\tLoss: 0.6563 \n",
            "| Epoch [ 22/ 60] Iter[126/251]\t\tLoss: 0.2830 \n",
            "| Epoch [ 22/ 60] Iter[131/251]\t\tLoss: 0.1909 \n",
            "| Epoch [ 22/ 60] Iter[136/251]\t\tLoss: 0.5894 \n",
            "| Epoch [ 22/ 60] Iter[141/251]\t\tLoss: 5.9625 \n",
            "| Epoch [ 22/ 60] Iter[146/251]\t\tLoss: 1.0452 \n",
            "| Epoch [ 22/ 60] Iter[151/251]\t\tLoss: 0.7983 \n",
            "| Epoch [ 22/ 60] Iter[156/251]\t\tLoss: 0.3170 \n",
            "| Epoch [ 22/ 60] Iter[161/251]\t\tLoss: 0.1158 \n",
            "| Epoch [ 22/ 60] Iter[166/251]\t\tLoss: 0.4476 \n",
            "| Epoch [ 22/ 60] Iter[171/251]\t\tLoss: 0.6156 \n",
            "| Epoch [ 22/ 60] Iter[176/251]\t\tLoss: 0.4396 \n",
            "| Epoch [ 22/ 60] Iter[181/251]\t\tLoss: 0.9487 \n",
            "| Epoch [ 22/ 60] Iter[186/251]\t\tLoss: 0.3480 \n",
            "| Epoch [ 22/ 60] Iter[191/251]\t\tLoss: 0.4186 \n",
            "| Epoch [ 22/ 60] Iter[196/251]\t\tLoss: 0.2762 \n",
            "| Epoch [ 22/ 60] Iter[201/251]\t\tLoss: 0.5565 \n",
            "| Epoch [ 22/ 60] Iter[206/251]\t\tLoss: 0.4077 \n",
            "| Epoch [ 22/ 60] Iter[211/251]\t\tLoss: 0.8113 \n",
            "| Epoch [ 22/ 60] Iter[216/251]\t\tLoss: 6.1186 \n",
            "| Epoch [ 22/ 60] Iter[221/251]\t\tLoss: 0.5879 \n",
            "| Epoch [ 22/ 60] Iter[226/251]\t\tLoss: 0.7403 \n",
            "| Epoch [ 22/ 60] Iter[231/251]\t\tLoss: 0.2568 \n",
            "| Epoch [ 22/ 60] Iter[236/251]\t\tLoss: 0.9924 \n",
            "| Epoch [ 22/ 60] Iter[241/251]\t\tLoss: 0.4293 \n",
            "| Epoch [ 22/ 60] Iter[246/251]\t\tLoss: 1.2202 \n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 0.2159 \n",
            "\n",
            "=> Training Epoch #23, LR=0.0000\n",
            "| Epoch [ 23/ 60] Iter[  1/251]\t\tLoss: 0.1343 \n",
            "| Epoch [ 23/ 60] Iter[  6/251]\t\tLoss: 0.1908 \n",
            "| Epoch [ 23/ 60] Iter[ 11/251]\t\tLoss: 0.5112 \n",
            "| Epoch [ 23/ 60] Iter[ 16/251]\t\tLoss: 6.5075 \n",
            "| Epoch [ 23/ 60] Iter[ 21/251]\t\tLoss: 0.8525 \n",
            "| Epoch [ 23/ 60] Iter[ 26/251]\t\tLoss: 0.3461 \n",
            "| Epoch [ 23/ 60] Iter[ 31/251]\t\tLoss: 0.1515 \n",
            "| Epoch [ 23/ 60] Iter[ 36/251]\t\tLoss: 0.2242 \n",
            "| Epoch [ 23/ 60] Iter[ 41/251]\t\tLoss: 1.0753 \n",
            "| Epoch [ 23/ 60] Iter[ 46/251]\t\tLoss: 0.6607 \n",
            "| Epoch [ 23/ 60] Iter[ 51/251]\t\tLoss: 0.4770 \n",
            "| Epoch [ 23/ 60] Iter[ 56/251]\t\tLoss: 0.1202 \n",
            "| Epoch [ 23/ 60] Iter[ 61/251]\t\tLoss: 0.3880 \n",
            "| Epoch [ 23/ 60] Iter[ 66/251]\t\tLoss: 0.4033 \n",
            "| Epoch [ 23/ 60] Iter[ 71/251]\t\tLoss: 0.3934 \n",
            "| Epoch [ 23/ 60] Iter[ 76/251]\t\tLoss: 0.5256 \n",
            "| Epoch [ 23/ 60] Iter[ 81/251]\t\tLoss: 0.5056 \n",
            "| Epoch [ 23/ 60] Iter[ 86/251]\t\tLoss: 0.2103 \n",
            "| Epoch [ 23/ 60] Iter[ 91/251]\t\tLoss: 0.9192 \n",
            "| Epoch [ 23/ 60] Iter[ 96/251]\t\tLoss: 0.3738 \n",
            "| Epoch [ 23/ 60] Iter[101/251]\t\tLoss: 0.2738 \n",
            "| Epoch [ 23/ 60] Iter[106/251]\t\tLoss: 5.9552 \n",
            "| Epoch [ 23/ 60] Iter[111/251]\t\tLoss: 2.5698 \n",
            "| Epoch [ 23/ 60] Iter[116/251]\t\tLoss: 0.3233 \n",
            "| Epoch [ 23/ 60] Iter[121/251]\t\tLoss: 0.1267 \n",
            "| Epoch [ 23/ 60] Iter[126/251]\t\tLoss: 0.6671 \n",
            "| Epoch [ 23/ 60] Iter[131/251]\t\tLoss: 0.2739 \n",
            "| Epoch [ 23/ 60] Iter[136/251]\t\tLoss: 0.7077 \n",
            "| Epoch [ 23/ 60] Iter[141/251]\t\tLoss: 1.0281 \n",
            "| Epoch [ 23/ 60] Iter[146/251]\t\tLoss: 0.1693 \n",
            "| Epoch [ 23/ 60] Iter[151/251]\t\tLoss: 0.8647 \n",
            "| Epoch [ 23/ 60] Iter[156/251]\t\tLoss: 0.2490 \n",
            "| Epoch [ 23/ 60] Iter[161/251]\t\tLoss: 0.8321 \n",
            "| Epoch [ 23/ 60] Iter[166/251]\t\tLoss: 0.4986 \n",
            "| Epoch [ 23/ 60] Iter[171/251]\t\tLoss: 0.3146 \n",
            "| Epoch [ 23/ 60] Iter[176/251]\t\tLoss: 0.5195 \n",
            "| Epoch [ 23/ 60] Iter[181/251]\t\tLoss: 0.3880 \n",
            "| Epoch [ 23/ 60] Iter[186/251]\t\tLoss: 0.3464 \n",
            "| Epoch [ 23/ 60] Iter[191/251]\t\tLoss: 0.4323 \n",
            "| Epoch [ 23/ 60] Iter[196/251]\t\tLoss: 1.7709 \n",
            "| Epoch [ 23/ 60] Iter[201/251]\t\tLoss: 0.5467 \n",
            "| Epoch [ 23/ 60] Iter[206/251]\t\tLoss: 0.2225 \n",
            "| Epoch [ 23/ 60] Iter[211/251]\t\tLoss: 0.1294 \n",
            "| Epoch [ 23/ 60] Iter[216/251]\t\tLoss: 0.6138 \n",
            "| Epoch [ 23/ 60] Iter[221/251]\t\tLoss: 0.2821 \n",
            "| Epoch [ 23/ 60] Iter[226/251]\t\tLoss: 0.6769 \n",
            "| Epoch [ 23/ 60] Iter[231/251]\t\tLoss: 0.3852 \n",
            "| Epoch [ 23/ 60] Iter[236/251]\t\tLoss: 0.2160 \n",
            "| Epoch [ 23/ 60] Iter[241/251]\t\tLoss: 6.1167 \n",
            "| Epoch [ 23/ 60] Iter[246/251]\t\tLoss: 0.7284 \n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 0.2165 \n",
            "\n",
            "=> Training Epoch #24, LR=0.0000\n",
            "| Epoch [ 24/ 60] Iter[  1/251]\t\tLoss: 0.2213 \n",
            "| Epoch [ 24/ 60] Iter[  6/251]\t\tLoss: 0.1813 \n",
            "| Epoch [ 24/ 60] Iter[ 11/251]\t\tLoss: 0.6229 \n",
            "| Epoch [ 24/ 60] Iter[ 16/251]\t\tLoss: 0.6593 \n",
            "| Epoch [ 24/ 60] Iter[ 21/251]\t\tLoss: 6.0785 \n",
            "| Epoch [ 24/ 60] Iter[ 26/251]\t\tLoss: 0.2876 \n",
            "| Epoch [ 24/ 60] Iter[ 31/251]\t\tLoss: 0.0699 \n",
            "| Epoch [ 24/ 60] Iter[ 36/251]\t\tLoss: 0.4248 \n",
            "| Epoch [ 24/ 60] Iter[ 41/251]\t\tLoss: 1.2604 \n",
            "| Epoch [ 24/ 60] Iter[ 46/251]\t\tLoss: 0.6524 \n",
            "| Epoch [ 24/ 60] Iter[ 51/251]\t\tLoss: 0.5416 \n",
            "| Epoch [ 24/ 60] Iter[ 56/251]\t\tLoss: 0.1269 \n",
            "| Epoch [ 24/ 60] Iter[ 61/251]\t\tLoss: 1.3780 \n",
            "| Epoch [ 24/ 60] Iter[ 66/251]\t\tLoss: 0.2629 \n",
            "| Epoch [ 24/ 60] Iter[ 71/251]\t\tLoss: 0.6627 \n",
            "| Epoch [ 24/ 60] Iter[ 76/251]\t\tLoss: 0.8625 \n",
            "| Epoch [ 24/ 60] Iter[ 81/251]\t\tLoss: 0.4381 \n",
            "| Epoch [ 24/ 60] Iter[ 86/251]\t\tLoss: 0.6417 \n",
            "| Epoch [ 24/ 60] Iter[ 91/251]\t\tLoss: 0.1144 \n",
            "| Epoch [ 24/ 60] Iter[ 96/251]\t\tLoss: 0.8869 \n",
            "| Epoch [ 24/ 60] Iter[101/251]\t\tLoss: 0.1641 \n",
            "| Epoch [ 24/ 60] Iter[106/251]\t\tLoss: 0.4320 \n",
            "| Epoch [ 24/ 60] Iter[111/251]\t\tLoss: 0.5098 \n",
            "| Epoch [ 24/ 60] Iter[116/251]\t\tLoss: 0.1880 \n",
            "| Epoch [ 24/ 60] Iter[121/251]\t\tLoss: 0.2818 \n",
            "| Epoch [ 24/ 60] Iter[126/251]\t\tLoss: 2.5926 \n",
            "| Epoch [ 24/ 60] Iter[131/251]\t\tLoss: 5.8841 \n",
            "| Epoch [ 24/ 60] Iter[136/251]\t\tLoss: 0.6367 \n",
            "| Epoch [ 24/ 60] Iter[141/251]\t\tLoss: 0.6371 \n",
            "| Epoch [ 24/ 60] Iter[146/251]\t\tLoss: 1.2597 \n",
            "| Epoch [ 24/ 60] Iter[151/251]\t\tLoss: 0.6320 \n",
            "| Epoch [ 24/ 60] Iter[156/251]\t\tLoss: 0.3849 \n",
            "| Epoch [ 24/ 60] Iter[161/251]\t\tLoss: 0.0788 \n",
            "| Epoch [ 24/ 60] Iter[166/251]\t\tLoss: 0.8362 \n",
            "| Epoch [ 24/ 60] Iter[171/251]\t\tLoss: 0.0615 \n",
            "| Epoch [ 24/ 60] Iter[176/251]\t\tLoss: 0.7854 \n",
            "| Epoch [ 24/ 60] Iter[181/251]\t\tLoss: 6.3308 \n",
            "| Epoch [ 24/ 60] Iter[186/251]\t\tLoss: 5.6030 \n",
            "| Epoch [ 24/ 60] Iter[191/251]\t\tLoss: 0.4824 \n",
            "| Epoch [ 24/ 60] Iter[196/251]\t\tLoss: 0.0063 \n",
            "| Epoch [ 24/ 60] Iter[201/251]\t\tLoss: 0.3627 \n",
            "| Epoch [ 24/ 60] Iter[206/251]\t\tLoss: 0.2698 \n",
            "| Epoch [ 24/ 60] Iter[211/251]\t\tLoss: 0.5703 \n",
            "| Epoch [ 24/ 60] Iter[216/251]\t\tLoss: 0.2792 \n",
            "| Epoch [ 24/ 60] Iter[221/251]\t\tLoss: 6.0465 \n",
            "| Epoch [ 24/ 60] Iter[226/251]\t\tLoss: 0.9859 \n",
            "| Epoch [ 24/ 60] Iter[231/251]\t\tLoss: 1.1546 \n",
            "| Epoch [ 24/ 60] Iter[236/251]\t\tLoss: 6.5430 \n",
            "| Epoch [ 24/ 60] Iter[241/251]\t\tLoss: 0.3540 \n",
            "| Epoch [ 24/ 60] Iter[246/251]\t\tLoss: 0.9088 \n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 0.2175 \n",
            "\n",
            "=> Training Epoch #25, LR=0.0000\n",
            "| Epoch [ 25/ 60] Iter[  1/251]\t\tLoss: 0.9588 \n",
            "| Epoch [ 25/ 60] Iter[  6/251]\t\tLoss: 0.2007 \n",
            "| Epoch [ 25/ 60] Iter[ 11/251]\t\tLoss: 0.3383 \n",
            "| Epoch [ 25/ 60] Iter[ 16/251]\t\tLoss: 0.7965 \n",
            "| Epoch [ 25/ 60] Iter[ 21/251]\t\tLoss: 0.4137 \n",
            "| Epoch [ 25/ 60] Iter[ 26/251]\t\tLoss: 0.8196 \n",
            "| Epoch [ 25/ 60] Iter[ 31/251]\t\tLoss: 0.0555 \n",
            "| Epoch [ 25/ 60] Iter[ 36/251]\t\tLoss: 0.1365 \n",
            "| Epoch [ 25/ 60] Iter[ 41/251]\t\tLoss: 0.5649 \n",
            "| Epoch [ 25/ 60] Iter[ 46/251]\t\tLoss: 0.5891 \n",
            "| Epoch [ 25/ 60] Iter[ 51/251]\t\tLoss: 0.4368 \n",
            "| Epoch [ 25/ 60] Iter[ 56/251]\t\tLoss: 1.0761 \n",
            "| Epoch [ 25/ 60] Iter[ 61/251]\t\tLoss: 0.3511 \n",
            "| Epoch [ 25/ 60] Iter[ 66/251]\t\tLoss: 0.1164 \n",
            "| Epoch [ 25/ 60] Iter[ 71/251]\t\tLoss: 0.3938 \n",
            "| Epoch [ 25/ 60] Iter[ 76/251]\t\tLoss: 0.4295 \n",
            "| Epoch [ 25/ 60] Iter[ 81/251]\t\tLoss: 0.4294 \n",
            "| Epoch [ 25/ 60] Iter[ 86/251]\t\tLoss: 0.3170 \n",
            "| Epoch [ 25/ 60] Iter[ 91/251]\t\tLoss: 0.2372 \n",
            "| Epoch [ 25/ 60] Iter[ 96/251]\t\tLoss: 6.0914 \n",
            "| Epoch [ 25/ 60] Iter[101/251]\t\tLoss: 0.4932 \n",
            "| Epoch [ 25/ 60] Iter[106/251]\t\tLoss: 0.1679 \n",
            "| Epoch [ 25/ 60] Iter[111/251]\t\tLoss: 0.5783 \n",
            "| Epoch [ 25/ 60] Iter[116/251]\t\tLoss: 0.7363 \n",
            "| Epoch [ 25/ 60] Iter[121/251]\t\tLoss: 0.6334 \n",
            "| Epoch [ 25/ 60] Iter[126/251]\t\tLoss: 0.8381 \n",
            "| Epoch [ 25/ 60] Iter[131/251]\t\tLoss: 1.2099 \n",
            "| Epoch [ 25/ 60] Iter[136/251]\t\tLoss: 0.1349 \n",
            "| Epoch [ 25/ 60] Iter[141/251]\t\tLoss: 0.1188 \n",
            "| Epoch [ 25/ 60] Iter[146/251]\t\tLoss: 0.2501 \n",
            "| Epoch [ 25/ 60] Iter[151/251]\t\tLoss: 0.3791 \n",
            "| Epoch [ 25/ 60] Iter[156/251]\t\tLoss: 0.2083 \n",
            "| Epoch [ 25/ 60] Iter[161/251]\t\tLoss: 1.1897 \n",
            "| Epoch [ 25/ 60] Iter[166/251]\t\tLoss: 6.9139 \n",
            "| Epoch [ 25/ 60] Iter[171/251]\t\tLoss: 0.4467 \n",
            "| Epoch [ 25/ 60] Iter[176/251]\t\tLoss: 0.2236 \n",
            "| Epoch [ 25/ 60] Iter[181/251]\t\tLoss: 0.8129 \n",
            "| Epoch [ 25/ 60] Iter[186/251]\t\tLoss: 5.6034 \n",
            "| Epoch [ 25/ 60] Iter[191/251]\t\tLoss: 0.5765 \n",
            "| Epoch [ 25/ 60] Iter[196/251]\t\tLoss: 0.8149 \n",
            "| Epoch [ 25/ 60] Iter[201/251]\t\tLoss: 0.3053 \n",
            "| Epoch [ 25/ 60] Iter[206/251]\t\tLoss: 0.9169 \n",
            "| Epoch [ 25/ 60] Iter[211/251]\t\tLoss: 0.2654 \n",
            "| Epoch [ 25/ 60] Iter[216/251]\t\tLoss: 0.0453 \n",
            "| Epoch [ 25/ 60] Iter[221/251]\t\tLoss: 1.5372 \n",
            "| Epoch [ 25/ 60] Iter[226/251]\t\tLoss: 0.8438 \n",
            "| Epoch [ 25/ 60] Iter[231/251]\t\tLoss: 0.0660 \n",
            "| Epoch [ 25/ 60] Iter[236/251]\t\tLoss: 0.0613 \n",
            "| Epoch [ 25/ 60] Iter[241/251]\t\tLoss: 0.0475 \n",
            "| Epoch [ 25/ 60] Iter[246/251]\t\tLoss: 1.2889 \n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 0.2180 \n",
            "\n",
            "=> Training Epoch #26, LR=0.0000\n",
            "| Epoch [ 26/ 60] Iter[  1/251]\t\tLoss: 6.9937 \n",
            "| Epoch [ 26/ 60] Iter[  6/251]\t\tLoss: 0.2350 \n",
            "| Epoch [ 26/ 60] Iter[ 11/251]\t\tLoss: 0.6595 \n",
            "| Epoch [ 26/ 60] Iter[ 16/251]\t\tLoss: 0.4268 \n",
            "| Epoch [ 26/ 60] Iter[ 21/251]\t\tLoss: 0.4160 \n",
            "| Epoch [ 26/ 60] Iter[ 26/251]\t\tLoss: 0.4056 \n",
            "| Epoch [ 26/ 60] Iter[ 31/251]\t\tLoss: 0.1507 \n",
            "| Epoch [ 26/ 60] Iter[ 36/251]\t\tLoss: 6.1070 \n",
            "| Epoch [ 26/ 60] Iter[ 41/251]\t\tLoss: 1.3612 \n",
            "| Epoch [ 26/ 60] Iter[ 46/251]\t\tLoss: 1.3802 \n",
            "| Epoch [ 26/ 60] Iter[ 51/251]\t\tLoss: 0.1224 \n",
            "| Epoch [ 26/ 60] Iter[ 56/251]\t\tLoss: 0.1951 \n",
            "| Epoch [ 26/ 60] Iter[ 61/251]\t\tLoss: 0.8749 \n",
            "| Epoch [ 26/ 60] Iter[ 66/251]\t\tLoss: 0.5512 \n",
            "| Epoch [ 26/ 60] Iter[ 71/251]\t\tLoss: 0.3878 \n",
            "| Epoch [ 26/ 60] Iter[ 76/251]\t\tLoss: 0.3367 \n",
            "| Epoch [ 26/ 60] Iter[ 81/251]\t\tLoss: 0.1830 \n",
            "| Epoch [ 26/ 60] Iter[ 86/251]\t\tLoss: 0.4654 \n",
            "| Epoch [ 26/ 60] Iter[ 91/251]\t\tLoss: 0.7803 \n",
            "| Epoch [ 26/ 60] Iter[ 96/251]\t\tLoss: 0.4138 \n",
            "| Epoch [ 26/ 60] Iter[101/251]\t\tLoss: 0.0842 \n",
            "| Epoch [ 26/ 60] Iter[106/251]\t\tLoss: 0.2497 \n",
            "| Epoch [ 26/ 60] Iter[111/251]\t\tLoss: 0.1430 \n",
            "| Epoch [ 26/ 60] Iter[116/251]\t\tLoss: 0.4712 \n",
            "| Epoch [ 26/ 60] Iter[121/251]\t\tLoss: 0.6038 \n",
            "| Epoch [ 26/ 60] Iter[126/251]\t\tLoss: 0.3695 \n",
            "| Epoch [ 26/ 60] Iter[131/251]\t\tLoss: 6.8510 \n",
            "| Epoch [ 26/ 60] Iter[136/251]\t\tLoss: 0.5332 \n",
            "| Epoch [ 26/ 60] Iter[141/251]\t\tLoss: 0.5282 \n",
            "| Epoch [ 26/ 60] Iter[146/251]\t\tLoss: 0.7388 \n",
            "| Epoch [ 26/ 60] Iter[151/251]\t\tLoss: 0.0094 \n",
            "| Epoch [ 26/ 60] Iter[156/251]\t\tLoss: 0.1549 \n",
            "| Epoch [ 26/ 60] Iter[161/251]\t\tLoss: 0.2563 \n",
            "| Epoch [ 26/ 60] Iter[166/251]\t\tLoss: 1.3417 \n",
            "| Epoch [ 26/ 60] Iter[171/251]\t\tLoss: 1.1708 \n",
            "| Epoch [ 26/ 60] Iter[176/251]\t\tLoss: 0.1233 \n",
            "| Epoch [ 26/ 60] Iter[181/251]\t\tLoss: 0.6667 \n",
            "| Epoch [ 26/ 60] Iter[186/251]\t\tLoss: 0.3953 \n",
            "| Epoch [ 26/ 60] Iter[191/251]\t\tLoss: 0.1326 \n",
            "| Epoch [ 26/ 60] Iter[196/251]\t\tLoss: 0.3717 \n",
            "| Epoch [ 26/ 60] Iter[201/251]\t\tLoss: 0.4680 \n",
            "| Epoch [ 26/ 60] Iter[206/251]\t\tLoss: 0.4276 \n",
            "| Epoch [ 26/ 60] Iter[211/251]\t\tLoss: 0.7458 \n",
            "| Epoch [ 26/ 60] Iter[216/251]\t\tLoss: 0.1330 \n",
            "| Epoch [ 26/ 60] Iter[221/251]\t\tLoss: 0.3018 \n",
            "| Epoch [ 26/ 60] Iter[226/251]\t\tLoss: 0.5204 \n",
            "| Epoch [ 26/ 60] Iter[231/251]\t\tLoss: 0.1567 \n",
            "| Epoch [ 26/ 60] Iter[236/251]\t\tLoss: 0.6347 \n",
            "| Epoch [ 26/ 60] Iter[241/251]\t\tLoss: 0.7225 \n",
            "| Epoch [ 26/ 60] Iter[246/251]\t\tLoss: 2.6957 \n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 0.2182 \n",
            "\n",
            "=> Training Epoch #27, LR=0.0000\n",
            "| Epoch [ 27/ 60] Iter[  1/251]\t\tLoss: 0.4255 \n",
            "| Epoch [ 27/ 60] Iter[  6/251]\t\tLoss: 0.9217 \n",
            "| Epoch [ 27/ 60] Iter[ 11/251]\t\tLoss: 0.3188 \n",
            "| Epoch [ 27/ 60] Iter[ 16/251]\t\tLoss: 0.9376 \n",
            "| Epoch [ 27/ 60] Iter[ 21/251]\t\tLoss: 0.2926 \n",
            "| Epoch [ 27/ 60] Iter[ 26/251]\t\tLoss: 0.4772 \n",
            "| Epoch [ 27/ 60] Iter[ 31/251]\t\tLoss: 0.7603 \n",
            "| Epoch [ 27/ 60] Iter[ 36/251]\t\tLoss: 0.6071 \n",
            "| Epoch [ 27/ 60] Iter[ 41/251]\t\tLoss: 0.8848 \n",
            "| Epoch [ 27/ 60] Iter[ 46/251]\t\tLoss: 0.2896 \n",
            "| Epoch [ 27/ 60] Iter[ 51/251]\t\tLoss: 0.2124 \n",
            "| Epoch [ 27/ 60] Iter[ 56/251]\t\tLoss: 0.3348 \n",
            "| Epoch [ 27/ 60] Iter[ 61/251]\t\tLoss: 0.8553 \n",
            "| Epoch [ 27/ 60] Iter[ 66/251]\t\tLoss: 0.5753 \n",
            "| Epoch [ 27/ 60] Iter[ 71/251]\t\tLoss: 0.8646 \n",
            "| Epoch [ 27/ 60] Iter[ 76/251]\t\tLoss: 0.6138 \n",
            "| Epoch [ 27/ 60] Iter[ 81/251]\t\tLoss: 2.7974 \n",
            "| Epoch [ 27/ 60] Iter[ 86/251]\t\tLoss: 0.7977 \n",
            "| Epoch [ 27/ 60] Iter[ 91/251]\t\tLoss: 0.4949 \n",
            "| Epoch [ 27/ 60] Iter[ 96/251]\t\tLoss: 2.1190 \n",
            "| Epoch [ 27/ 60] Iter[101/251]\t\tLoss: 0.5099 \n",
            "| Epoch [ 27/ 60] Iter[106/251]\t\tLoss: 0.3567 \n",
            "| Epoch [ 27/ 60] Iter[111/251]\t\tLoss: 0.2715 \n",
            "| Epoch [ 27/ 60] Iter[116/251]\t\tLoss: 0.0593 \n",
            "| Epoch [ 27/ 60] Iter[121/251]\t\tLoss: 7.1659 \n",
            "| Epoch [ 27/ 60] Iter[126/251]\t\tLoss: 0.1932 \n",
            "| Epoch [ 27/ 60] Iter[131/251]\t\tLoss: 0.8113 \n",
            "| Epoch [ 27/ 60] Iter[136/251]\t\tLoss: 0.5488 \n",
            "| Epoch [ 27/ 60] Iter[141/251]\t\tLoss: 2.5307 \n",
            "| Epoch [ 27/ 60] Iter[146/251]\t\tLoss: 0.4315 \n",
            "| Epoch [ 27/ 60] Iter[151/251]\t\tLoss: 0.7315 \n",
            "| Epoch [ 27/ 60] Iter[156/251]\t\tLoss: 0.6032 \n",
            "| Epoch [ 27/ 60] Iter[161/251]\t\tLoss: 0.3526 \n",
            "| Epoch [ 27/ 60] Iter[166/251]\t\tLoss: 6.3904 \n",
            "| Epoch [ 27/ 60] Iter[171/251]\t\tLoss: 0.9700 \n",
            "| Epoch [ 27/ 60] Iter[176/251]\t\tLoss: 0.0180 \n",
            "| Epoch [ 27/ 60] Iter[181/251]\t\tLoss: 0.3064 \n",
            "| Epoch [ 27/ 60] Iter[186/251]\t\tLoss: 0.6957 \n",
            "| Epoch [ 27/ 60] Iter[191/251]\t\tLoss: 1.2085 \n",
            "| Epoch [ 27/ 60] Iter[196/251]\t\tLoss: 0.7092 \n",
            "| Epoch [ 27/ 60] Iter[201/251]\t\tLoss: 0.2874 \n",
            "| Epoch [ 27/ 60] Iter[206/251]\t\tLoss: 0.2157 \n",
            "| Epoch [ 27/ 60] Iter[211/251]\t\tLoss: 0.2878 \n",
            "| Epoch [ 27/ 60] Iter[216/251]\t\tLoss: 0.5405 \n",
            "| Epoch [ 27/ 60] Iter[221/251]\t\tLoss: 0.8475 \n",
            "| Epoch [ 27/ 60] Iter[226/251]\t\tLoss: 0.2823 \n",
            "| Epoch [ 27/ 60] Iter[231/251]\t\tLoss: 0.4468 \n",
            "| Epoch [ 27/ 60] Iter[236/251]\t\tLoss: 0.2098 \n",
            "| Epoch [ 27/ 60] Iter[241/251]\t\tLoss: 0.2085 \n",
            "| Epoch [ 27/ 60] Iter[246/251]\t\tLoss: 0.2800 \n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 0.2180 \n",
            "\n",
            "=> Training Epoch #28, LR=0.0000\n",
            "| Epoch [ 28/ 60] Iter[  1/251]\t\tLoss: 0.2311 \n",
            "| Epoch [ 28/ 60] Iter[  6/251]\t\tLoss: 0.0340 \n",
            "| Epoch [ 28/ 60] Iter[ 11/251]\t\tLoss: 0.8931 \n",
            "| Epoch [ 28/ 60] Iter[ 16/251]\t\tLoss: 0.3178 \n",
            "| Epoch [ 28/ 60] Iter[ 21/251]\t\tLoss: 0.4867 \n",
            "| Epoch [ 28/ 60] Iter[ 26/251]\t\tLoss: 0.2880 \n",
            "| Epoch [ 28/ 60] Iter[ 31/251]\t\tLoss: 0.0378 \n",
            "| Epoch [ 28/ 60] Iter[ 36/251]\t\tLoss: 0.2376 \n",
            "| Epoch [ 28/ 60] Iter[ 41/251]\t\tLoss: 0.4589 \n",
            "| Epoch [ 28/ 60] Iter[ 46/251]\t\tLoss: 0.6764 \n",
            "| Epoch [ 28/ 60] Iter[ 51/251]\t\tLoss: 6.3526 \n",
            "| Epoch [ 28/ 60] Iter[ 56/251]\t\tLoss: 0.6126 \n",
            "| Epoch [ 28/ 60] Iter[ 61/251]\t\tLoss: 0.2461 \n",
            "| Epoch [ 28/ 60] Iter[ 66/251]\t\tLoss: 0.5297 \n",
            "| Epoch [ 28/ 60] Iter[ 71/251]\t\tLoss: 0.6217 \n",
            "| Epoch [ 28/ 60] Iter[ 76/251]\t\tLoss: 1.3793 \n",
            "| Epoch [ 28/ 60] Iter[ 81/251]\t\tLoss: 0.5838 \n",
            "| Epoch [ 28/ 60] Iter[ 86/251]\t\tLoss: 0.4346 \n",
            "| Epoch [ 28/ 60] Iter[ 91/251]\t\tLoss: 0.1692 \n",
            "| Epoch [ 28/ 60] Iter[ 96/251]\t\tLoss: 0.9075 \n",
            "| Epoch [ 28/ 60] Iter[101/251]\t\tLoss: 0.2843 \n",
            "| Epoch [ 28/ 60] Iter[106/251]\t\tLoss: 0.2958 \n",
            "| Epoch [ 28/ 60] Iter[111/251]\t\tLoss: 0.1693 \n",
            "| Epoch [ 28/ 60] Iter[116/251]\t\tLoss: 0.3774 \n",
            "| Epoch [ 28/ 60] Iter[121/251]\t\tLoss: 6.4094 \n",
            "| Epoch [ 28/ 60] Iter[126/251]\t\tLoss: 6.7343 \n",
            "| Epoch [ 28/ 60] Iter[131/251]\t\tLoss: 0.2445 \n",
            "| Epoch [ 28/ 60] Iter[136/251]\t\tLoss: 0.2147 \n",
            "| Epoch [ 28/ 60] Iter[141/251]\t\tLoss: 0.7744 \n",
            "| Epoch [ 28/ 60] Iter[146/251]\t\tLoss: 0.7770 \n",
            "| Epoch [ 28/ 60] Iter[151/251]\t\tLoss: 0.0426 \n",
            "| Epoch [ 28/ 60] Iter[156/251]\t\tLoss: 0.8741 \n",
            "| Epoch [ 28/ 60] Iter[161/251]\t\tLoss: 1.9799 \n",
            "| Epoch [ 28/ 60] Iter[166/251]\t\tLoss: 0.3607 \n",
            "| Epoch [ 28/ 60] Iter[171/251]\t\tLoss: 0.3298 \n",
            "| Epoch [ 28/ 60] Iter[176/251]\t\tLoss: 0.5311 \n",
            "| Epoch [ 28/ 60] Iter[181/251]\t\tLoss: 1.0705 \n",
            "| Epoch [ 28/ 60] Iter[186/251]\t\tLoss: 0.2598 \n",
            "| Epoch [ 28/ 60] Iter[191/251]\t\tLoss: 0.0906 \n",
            "| Epoch [ 28/ 60] Iter[196/251]\t\tLoss: 0.2476 \n",
            "| Epoch [ 28/ 60] Iter[201/251]\t\tLoss: 0.2340 \n",
            "| Epoch [ 28/ 60] Iter[206/251]\t\tLoss: 0.5126 \n",
            "| Epoch [ 28/ 60] Iter[211/251]\t\tLoss: 0.5598 \n",
            "| Epoch [ 28/ 60] Iter[216/251]\t\tLoss: 5.9854 \n",
            "| Epoch [ 28/ 60] Iter[221/251]\t\tLoss: 0.3528 \n",
            "| Epoch [ 28/ 60] Iter[226/251]\t\tLoss: 0.1673 \n",
            "| Epoch [ 28/ 60] Iter[231/251]\t\tLoss: 5.6465 \n",
            "| Epoch [ 28/ 60] Iter[236/251]\t\tLoss: 0.4609 \n",
            "| Epoch [ 28/ 60] Iter[241/251]\t\tLoss: 0.3514 \n",
            "| Epoch [ 28/ 60] Iter[246/251]\t\tLoss: 0.2970 \n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 0.2188 \n",
            "\n",
            "=> Training Epoch #29, LR=0.0000\n",
            "| Epoch [ 29/ 60] Iter[  1/251]\t\tLoss: 0.1621 \n",
            "| Epoch [ 29/ 60] Iter[  6/251]\t\tLoss: 0.3898 \n",
            "| Epoch [ 29/ 60] Iter[ 11/251]\t\tLoss: 0.4365 \n",
            "| Epoch [ 29/ 60] Iter[ 16/251]\t\tLoss: 0.6284 \n",
            "| Epoch [ 29/ 60] Iter[ 21/251]\t\tLoss: 0.4464 \n",
            "| Epoch [ 29/ 60] Iter[ 26/251]\t\tLoss: 0.5827 \n",
            "| Epoch [ 29/ 60] Iter[ 31/251]\t\tLoss: 0.1738 \n",
            "| Epoch [ 29/ 60] Iter[ 36/251]\t\tLoss: 5.9667 \n",
            "| Epoch [ 29/ 60] Iter[ 41/251]\t\tLoss: 0.3141 \n",
            "| Epoch [ 29/ 60] Iter[ 46/251]\t\tLoss: 0.9374 \n",
            "| Epoch [ 29/ 60] Iter[ 51/251]\t\tLoss: 0.3155 \n",
            "| Epoch [ 29/ 60] Iter[ 56/251]\t\tLoss: 0.1760 \n",
            "| Epoch [ 29/ 60] Iter[ 61/251]\t\tLoss: 0.0970 \n",
            "| Epoch [ 29/ 60] Iter[ 66/251]\t\tLoss: 0.6469 \n",
            "| Epoch [ 29/ 60] Iter[ 71/251]\t\tLoss: 0.3051 \n",
            "| Epoch [ 29/ 60] Iter[ 76/251]\t\tLoss: 1.2470 \n",
            "| Epoch [ 29/ 60] Iter[ 81/251]\t\tLoss: 0.4890 \n",
            "| Epoch [ 29/ 60] Iter[ 86/251]\t\tLoss: 0.3059 \n",
            "| Epoch [ 29/ 60] Iter[ 91/251]\t\tLoss: 0.7649 \n",
            "| Epoch [ 29/ 60] Iter[ 96/251]\t\tLoss: 0.7059 \n",
            "| Epoch [ 29/ 60] Iter[101/251]\t\tLoss: 0.7022 \n",
            "| Epoch [ 29/ 60] Iter[106/251]\t\tLoss: 0.4224 \n",
            "| Epoch [ 29/ 60] Iter[111/251]\t\tLoss: 0.5130 \n",
            "| Epoch [ 29/ 60] Iter[116/251]\t\tLoss: 1.0204 \n",
            "| Epoch [ 29/ 60] Iter[121/251]\t\tLoss: 0.3076 \n",
            "| Epoch [ 29/ 60] Iter[126/251]\t\tLoss: 0.7783 \n",
            "| Epoch [ 29/ 60] Iter[131/251]\t\tLoss: 0.4162 \n",
            "| Epoch [ 29/ 60] Iter[136/251]\t\tLoss: 0.9055 \n",
            "| Epoch [ 29/ 60] Iter[141/251]\t\tLoss: 0.2884 \n",
            "| Epoch [ 29/ 60] Iter[146/251]\t\tLoss: 6.0528 \n",
            "| Epoch [ 29/ 60] Iter[151/251]\t\tLoss: 1.5445 \n",
            "| Epoch [ 29/ 60] Iter[156/251]\t\tLoss: 0.1483 \n",
            "| Epoch [ 29/ 60] Iter[161/251]\t\tLoss: 0.3668 \n",
            "| Epoch [ 29/ 60] Iter[166/251]\t\tLoss: 1.1577 \n",
            "| Epoch [ 29/ 60] Iter[171/251]\t\tLoss: 0.2419 \n",
            "| Epoch [ 29/ 60] Iter[176/251]\t\tLoss: 0.3973 \n",
            "| Epoch [ 29/ 60] Iter[181/251]\t\tLoss: 0.4629 \n",
            "| Epoch [ 29/ 60] Iter[186/251]\t\tLoss: 0.0658 \n",
            "| Epoch [ 29/ 60] Iter[191/251]\t\tLoss: 0.5180 \n",
            "| Epoch [ 29/ 60] Iter[196/251]\t\tLoss: 0.1139 \n",
            "| Epoch [ 29/ 60] Iter[201/251]\t\tLoss: 0.4500 \n",
            "| Epoch [ 29/ 60] Iter[206/251]\t\tLoss: 6.8750 \n",
            "| Epoch [ 29/ 60] Iter[211/251]\t\tLoss: 0.3841 \n",
            "| Epoch [ 29/ 60] Iter[216/251]\t\tLoss: 0.4134 \n",
            "| Epoch [ 29/ 60] Iter[221/251]\t\tLoss: 0.3350 \n",
            "| Epoch [ 29/ 60] Iter[226/251]\t\tLoss: 0.8700 \n",
            "| Epoch [ 29/ 60] Iter[231/251]\t\tLoss: 0.2731 \n",
            "| Epoch [ 29/ 60] Iter[236/251]\t\tLoss: 0.9227 \n",
            "| Epoch [ 29/ 60] Iter[241/251]\t\tLoss: 0.3762 \n",
            "| Epoch [ 29/ 60] Iter[246/251]\t\tLoss: 0.5240 \n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 0.2188 \n",
            "\n",
            "=> Training Epoch #30, LR=0.0000\n",
            "| Epoch [ 30/ 60] Iter[  1/251]\t\tLoss: 0.0802 \n",
            "| Epoch [ 30/ 60] Iter[  6/251]\t\tLoss: 0.1833 \n",
            "| Epoch [ 30/ 60] Iter[ 11/251]\t\tLoss: 0.3619 \n",
            "| Epoch [ 30/ 60] Iter[ 16/251]\t\tLoss: 0.3742 \n",
            "| Epoch [ 30/ 60] Iter[ 21/251]\t\tLoss: 6.2265 \n",
            "| Epoch [ 30/ 60] Iter[ 26/251]\t\tLoss: 0.2608 \n",
            "| Epoch [ 30/ 60] Iter[ 31/251]\t\tLoss: 0.3500 \n",
            "| Epoch [ 30/ 60] Iter[ 36/251]\t\tLoss: 0.3939 \n",
            "| Epoch [ 30/ 60] Iter[ 41/251]\t\tLoss: 0.2336 \n",
            "| Epoch [ 30/ 60] Iter[ 46/251]\t\tLoss: 0.3375 \n",
            "| Epoch [ 30/ 60] Iter[ 51/251]\t\tLoss: 0.4819 \n",
            "| Epoch [ 30/ 60] Iter[ 56/251]\t\tLoss: 6.4026 \n",
            "| Epoch [ 30/ 60] Iter[ 61/251]\t\tLoss: 0.2566 \n",
            "| Epoch [ 30/ 60] Iter[ 66/251]\t\tLoss: 0.9684 \n",
            "| Epoch [ 30/ 60] Iter[ 71/251]\t\tLoss: 6.4665 \n",
            "| Epoch [ 30/ 60] Iter[ 76/251]\t\tLoss: 1.1422 \n",
            "| Epoch [ 30/ 60] Iter[ 81/251]\t\tLoss: 6.1434 \n",
            "| Epoch [ 30/ 60] Iter[ 86/251]\t\tLoss: 1.0282 \n",
            "| Epoch [ 30/ 60] Iter[ 91/251]\t\tLoss: 0.0746 \n",
            "| Epoch [ 30/ 60] Iter[ 96/251]\t\tLoss: 0.6233 \n",
            "| Epoch [ 30/ 60] Iter[101/251]\t\tLoss: 1.1660 \n",
            "| Epoch [ 30/ 60] Iter[106/251]\t\tLoss: 0.2831 \n",
            "| Epoch [ 30/ 60] Iter[111/251]\t\tLoss: 0.7635 \n",
            "| Epoch [ 30/ 60] Iter[116/251]\t\tLoss: 0.1759 \n",
            "| Epoch [ 30/ 60] Iter[121/251]\t\tLoss: 0.7071 \n",
            "| Epoch [ 30/ 60] Iter[126/251]\t\tLoss: 0.0410 \n",
            "| Epoch [ 30/ 60] Iter[131/251]\t\tLoss: 0.3057 \n",
            "| Epoch [ 30/ 60] Iter[136/251]\t\tLoss: 6.1057 \n",
            "| Epoch [ 30/ 60] Iter[141/251]\t\tLoss: 0.5786 \n",
            "| Epoch [ 30/ 60] Iter[146/251]\t\tLoss: 0.4794 \n",
            "| Epoch [ 30/ 60] Iter[151/251]\t\tLoss: 1.3340 \n",
            "| Epoch [ 30/ 60] Iter[156/251]\t\tLoss: 0.5637 \n",
            "| Epoch [ 30/ 60] Iter[161/251]\t\tLoss: 0.0356 \n",
            "| Epoch [ 30/ 60] Iter[166/251]\t\tLoss: 0.0443 \n",
            "| Epoch [ 30/ 60] Iter[171/251]\t\tLoss: 0.1890 \n",
            "| Epoch [ 30/ 60] Iter[176/251]\t\tLoss: 0.1248 \n",
            "| Epoch [ 30/ 60] Iter[181/251]\t\tLoss: 0.7095 \n",
            "| Epoch [ 30/ 60] Iter[186/251]\t\tLoss: 0.7713 \n",
            "| Epoch [ 30/ 60] Iter[191/251]\t\tLoss: 0.2962 \n",
            "| Epoch [ 30/ 60] Iter[196/251]\t\tLoss: 1.4622 \n",
            "| Epoch [ 30/ 60] Iter[201/251]\t\tLoss: 0.7689 \n",
            "| Epoch [ 30/ 60] Iter[206/251]\t\tLoss: 0.1960 \n",
            "| Epoch [ 30/ 60] Iter[211/251]\t\tLoss: 0.6857 \n",
            "| Epoch [ 30/ 60] Iter[216/251]\t\tLoss: 0.3778 \n",
            "| Epoch [ 30/ 60] Iter[221/251]\t\tLoss: 0.2961 \n",
            "| Epoch [ 30/ 60] Iter[226/251]\t\tLoss: 0.4430 \n",
            "| Epoch [ 30/ 60] Iter[231/251]\t\tLoss: 0.1689 \n",
            "| Epoch [ 30/ 60] Iter[236/251]\t\tLoss: 0.7078 \n",
            "| Epoch [ 30/ 60] Iter[241/251]\t\tLoss: 0.6957 \n",
            "| Epoch [ 30/ 60] Iter[246/251]\t\tLoss: 0.2911 \n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 0.2193 \n",
            "\n",
            "=> Training Epoch #31, LR=0.0000\n",
            "| Epoch [ 31/ 60] Iter[  1/251]\t\tLoss: 0.4288 \n",
            "| Epoch [ 31/ 60] Iter[  6/251]\t\tLoss: 0.6571 \n",
            "| Epoch [ 31/ 60] Iter[ 11/251]\t\tLoss: 0.5665 \n",
            "| Epoch [ 31/ 60] Iter[ 16/251]\t\tLoss: 0.6646 \n",
            "| Epoch [ 31/ 60] Iter[ 21/251]\t\tLoss: 0.3652 \n",
            "| Epoch [ 31/ 60] Iter[ 26/251]\t\tLoss: 0.4971 \n",
            "| Epoch [ 31/ 60] Iter[ 31/251]\t\tLoss: 0.2685 \n",
            "| Epoch [ 31/ 60] Iter[ 36/251]\t\tLoss: 0.2750 \n",
            "| Epoch [ 31/ 60] Iter[ 41/251]\t\tLoss: 0.9532 \n",
            "| Epoch [ 31/ 60] Iter[ 46/251]\t\tLoss: 0.4094 \n",
            "| Epoch [ 31/ 60] Iter[ 51/251]\t\tLoss: 1.8901 \n",
            "| Epoch [ 31/ 60] Iter[ 56/251]\t\tLoss: 0.7019 \n",
            "| Epoch [ 31/ 60] Iter[ 61/251]\t\tLoss: 0.4273 \n",
            "| Epoch [ 31/ 60] Iter[ 66/251]\t\tLoss: 0.6268 \n",
            "| Epoch [ 31/ 60] Iter[ 71/251]\t\tLoss: 0.3631 \n",
            "| Epoch [ 31/ 60] Iter[ 76/251]\t\tLoss: 2.2006 \n",
            "| Epoch [ 31/ 60] Iter[ 81/251]\t\tLoss: 0.2445 \n",
            "| Epoch [ 31/ 60] Iter[ 86/251]\t\tLoss: 0.2215 \n",
            "| Epoch [ 31/ 60] Iter[ 91/251]\t\tLoss: 0.4638 \n",
            "| Epoch [ 31/ 60] Iter[ 96/251]\t\tLoss: 0.4082 \n",
            "| Epoch [ 31/ 60] Iter[101/251]\t\tLoss: 0.4434 \n",
            "| Epoch [ 31/ 60] Iter[106/251]\t\tLoss: 0.0894 \n",
            "| Epoch [ 31/ 60] Iter[111/251]\t\tLoss: 0.1934 \n",
            "| Epoch [ 31/ 60] Iter[116/251]\t\tLoss: 0.1683 \n",
            "| Epoch [ 31/ 60] Iter[121/251]\t\tLoss: 0.5488 \n",
            "| Epoch [ 31/ 60] Iter[126/251]\t\tLoss: 0.9343 \n",
            "| Epoch [ 31/ 60] Iter[131/251]\t\tLoss: 1.6469 \n",
            "| Epoch [ 31/ 60] Iter[136/251]\t\tLoss: 0.1895 \n",
            "| Epoch [ 31/ 60] Iter[141/251]\t\tLoss: 0.2531 \n",
            "| Epoch [ 31/ 60] Iter[146/251]\t\tLoss: 0.1019 \n",
            "| Epoch [ 31/ 60] Iter[151/251]\t\tLoss: 0.6155 \n",
            "| Epoch [ 31/ 60] Iter[156/251]\t\tLoss: 0.0884 \n",
            "| Epoch [ 31/ 60] Iter[161/251]\t\tLoss: 0.6035 \n",
            "| Epoch [ 31/ 60] Iter[166/251]\t\tLoss: 0.7208 \n",
            "| Epoch [ 31/ 60] Iter[171/251]\t\tLoss: 5.9747 \n",
            "| Epoch [ 31/ 60] Iter[176/251]\t\tLoss: 0.4792 \n",
            "| Epoch [ 31/ 60] Iter[181/251]\t\tLoss: 0.1620 \n",
            "| Epoch [ 31/ 60] Iter[186/251]\t\tLoss: 0.8596 \n",
            "| Epoch [ 31/ 60] Iter[191/251]\t\tLoss: 1.0699 \n",
            "| Epoch [ 31/ 60] Iter[196/251]\t\tLoss: 0.9982 \n",
            "| Epoch [ 31/ 60] Iter[201/251]\t\tLoss: 5.9243 \n",
            "| Epoch [ 31/ 60] Iter[206/251]\t\tLoss: 0.6051 \n",
            "| Epoch [ 31/ 60] Iter[211/251]\t\tLoss: 0.3547 \n",
            "| Epoch [ 31/ 60] Iter[216/251]\t\tLoss: 0.5685 \n",
            "| Epoch [ 31/ 60] Iter[221/251]\t\tLoss: 0.1485 \n",
            "| Epoch [ 31/ 60] Iter[226/251]\t\tLoss: 1.0798 \n",
            "| Epoch [ 31/ 60] Iter[231/251]\t\tLoss: 3.0532 \n",
            "| Epoch [ 31/ 60] Iter[236/251]\t\tLoss: 0.2394 \n",
            "| Epoch [ 31/ 60] Iter[241/251]\t\tLoss: 0.2801 \n",
            "| Epoch [ 31/ 60] Iter[246/251]\t\tLoss: 0.4678 \n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 0.2197 \n",
            "\n",
            "=> Training Epoch #32, LR=0.0000\n",
            "| Epoch [ 32/ 60] Iter[  1/251]\t\tLoss: 1.2813 \n",
            "| Epoch [ 32/ 60] Iter[  6/251]\t\tLoss: 0.6627 \n",
            "| Epoch [ 32/ 60] Iter[ 11/251]\t\tLoss: 0.0419 \n",
            "| Epoch [ 32/ 60] Iter[ 16/251]\t\tLoss: 0.5699 \n",
            "| Epoch [ 32/ 60] Iter[ 21/251]\t\tLoss: 0.2155 \n",
            "| Epoch [ 32/ 60] Iter[ 26/251]\t\tLoss: 0.2816 \n",
            "| Epoch [ 32/ 60] Iter[ 31/251]\t\tLoss: 0.4712 \n",
            "| Epoch [ 32/ 60] Iter[ 36/251]\t\tLoss: 0.5906 \n",
            "| Epoch [ 32/ 60] Iter[ 41/251]\t\tLoss: 0.2790 \n",
            "| Epoch [ 32/ 60] Iter[ 46/251]\t\tLoss: 0.1133 \n",
            "| Epoch [ 32/ 60] Iter[ 51/251]\t\tLoss: 6.1675 \n",
            "| Epoch [ 32/ 60] Iter[ 56/251]\t\tLoss: 0.5897 \n",
            "| Epoch [ 32/ 60] Iter[ 61/251]\t\tLoss: 6.0523 \n",
            "| Epoch [ 32/ 60] Iter[ 66/251]\t\tLoss: 1.0740 \n",
            "| Epoch [ 32/ 60] Iter[ 71/251]\t\tLoss: 0.7214 \n",
            "| Epoch [ 32/ 60] Iter[ 76/251]\t\tLoss: 0.1931 \n",
            "| Epoch [ 32/ 60] Iter[ 81/251]\t\tLoss: 0.4399 \n",
            "| Epoch [ 32/ 60] Iter[ 86/251]\t\tLoss: 0.2943 \n",
            "| Epoch [ 32/ 60] Iter[ 91/251]\t\tLoss: 0.2833 \n",
            "| Epoch [ 32/ 60] Iter[ 96/251]\t\tLoss: 0.6890 \n",
            "| Epoch [ 32/ 60] Iter[101/251]\t\tLoss: 6.4508 \n",
            "| Epoch [ 32/ 60] Iter[106/251]\t\tLoss: 0.2961 \n",
            "| Epoch [ 32/ 60] Iter[111/251]\t\tLoss: 0.4282 \n",
            "| Epoch [ 32/ 60] Iter[116/251]\t\tLoss: 0.6761 \n",
            "| Epoch [ 32/ 60] Iter[121/251]\t\tLoss: 0.3348 \n",
            "| Epoch [ 32/ 60] Iter[126/251]\t\tLoss: 5.6802 \n",
            "| Epoch [ 32/ 60] Iter[131/251]\t\tLoss: 0.0089 \n",
            "| Epoch [ 32/ 60] Iter[136/251]\t\tLoss: 0.4544 \n",
            "| Epoch [ 32/ 60] Iter[141/251]\t\tLoss: 0.3202 \n",
            "| Epoch [ 32/ 60] Iter[146/251]\t\tLoss: 0.4851 \n",
            "| Epoch [ 32/ 60] Iter[151/251]\t\tLoss: 0.8201 \n",
            "| Epoch [ 32/ 60] Iter[156/251]\t\tLoss: 0.3951 \n",
            "| Epoch [ 32/ 60] Iter[161/251]\t\tLoss: 0.2415 \n",
            "| Epoch [ 32/ 60] Iter[166/251]\t\tLoss: 0.6730 \n",
            "| Epoch [ 32/ 60] Iter[171/251]\t\tLoss: 0.3107 \n",
            "| Epoch [ 32/ 60] Iter[176/251]\t\tLoss: 0.1623 \n",
            "| Epoch [ 32/ 60] Iter[181/251]\t\tLoss: 0.4016 \n",
            "| Epoch [ 32/ 60] Iter[186/251]\t\tLoss: 0.3622 \n",
            "| Epoch [ 32/ 60] Iter[191/251]\t\tLoss: 0.3168 \n",
            "| Epoch [ 32/ 60] Iter[196/251]\t\tLoss: 0.5016 \n",
            "| Epoch [ 32/ 60] Iter[201/251]\t\tLoss: 1.0676 \n",
            "| Epoch [ 32/ 60] Iter[206/251]\t\tLoss: 0.1833 \n",
            "| Epoch [ 32/ 60] Iter[211/251]\t\tLoss: 2.4589 \n",
            "| Epoch [ 32/ 60] Iter[216/251]\t\tLoss: 0.4860 \n",
            "| Epoch [ 32/ 60] Iter[221/251]\t\tLoss: 0.1576 \n",
            "| Epoch [ 32/ 60] Iter[226/251]\t\tLoss: 0.5007 \n",
            "| Epoch [ 32/ 60] Iter[231/251]\t\tLoss: 0.6169 \n",
            "| Epoch [ 32/ 60] Iter[236/251]\t\tLoss: 0.6745 \n",
            "| Epoch [ 32/ 60] Iter[241/251]\t\tLoss: 0.5861 \n",
            "| Epoch [ 32/ 60] Iter[246/251]\t\tLoss: 0.3096 \n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 0.2196 \n",
            "\n",
            "=> Training Epoch #33, LR=0.0000\n",
            "| Epoch [ 33/ 60] Iter[  1/251]\t\tLoss: 0.7007 \n",
            "| Epoch [ 33/ 60] Iter[  6/251]\t\tLoss: 0.7291 \n",
            "| Epoch [ 33/ 60] Iter[ 11/251]\t\tLoss: 2.5404 \n",
            "| Epoch [ 33/ 60] Iter[ 16/251]\t\tLoss: 0.3249 \n",
            "| Epoch [ 33/ 60] Iter[ 21/251]\t\tLoss: 0.0879 \n",
            "| Epoch [ 33/ 60] Iter[ 26/251]\t\tLoss: 5.7831 \n",
            "| Epoch [ 33/ 60] Iter[ 31/251]\t\tLoss: 0.4553 \n",
            "| Epoch [ 33/ 60] Iter[ 36/251]\t\tLoss: 0.8215 \n",
            "| Epoch [ 33/ 60] Iter[ 41/251]\t\tLoss: 0.5206 \n",
            "| Epoch [ 33/ 60] Iter[ 46/251]\t\tLoss: 0.6607 \n",
            "| Epoch [ 33/ 60] Iter[ 51/251]\t\tLoss: 0.6301 \n",
            "| Epoch [ 33/ 60] Iter[ 56/251]\t\tLoss: 0.2744 \n",
            "| Epoch [ 33/ 60] Iter[ 61/251]\t\tLoss: 0.3318 \n",
            "| Epoch [ 33/ 60] Iter[ 66/251]\t\tLoss: 0.2866 \n",
            "| Epoch [ 33/ 60] Iter[ 71/251]\t\tLoss: 0.2151 \n",
            "| Epoch [ 33/ 60] Iter[ 76/251]\t\tLoss: 0.6004 \n",
            "| Epoch [ 33/ 60] Iter[ 81/251]\t\tLoss: 0.4692 \n",
            "| Epoch [ 33/ 60] Iter[ 86/251]\t\tLoss: 6.2628 \n",
            "| Epoch [ 33/ 60] Iter[ 91/251]\t\tLoss: 0.4460 \n",
            "| Epoch [ 33/ 60] Iter[ 96/251]\t\tLoss: 0.6985 \n",
            "| Epoch [ 33/ 60] Iter[101/251]\t\tLoss: 0.1998 \n",
            "| Epoch [ 33/ 60] Iter[106/251]\t\tLoss: 0.5146 \n",
            "| Epoch [ 33/ 60] Iter[111/251]\t\tLoss: 0.4790 \n",
            "| Epoch [ 33/ 60] Iter[116/251]\t\tLoss: 2.1603 \n",
            "| Epoch [ 33/ 60] Iter[121/251]\t\tLoss: 1.1179 \n",
            "| Epoch [ 33/ 60] Iter[126/251]\t\tLoss: 0.5903 \n",
            "| Epoch [ 33/ 60] Iter[131/251]\t\tLoss: 0.3617 \n",
            "| Epoch [ 33/ 60] Iter[136/251]\t\tLoss: 0.7580 \n",
            "| Epoch [ 33/ 60] Iter[141/251]\t\tLoss: 0.6483 \n",
            "| Epoch [ 33/ 60] Iter[146/251]\t\tLoss: 0.1697 \n",
            "| Epoch [ 33/ 60] Iter[151/251]\t\tLoss: 0.4246 \n",
            "| Epoch [ 33/ 60] Iter[156/251]\t\tLoss: 0.0581 \n",
            "| Epoch [ 33/ 60] Iter[161/251]\t\tLoss: 0.3570 \n",
            "| Epoch [ 33/ 60] Iter[166/251]\t\tLoss: 0.7637 \n",
            "| Epoch [ 33/ 60] Iter[171/251]\t\tLoss: 0.4606 \n",
            "| Epoch [ 33/ 60] Iter[176/251]\t\tLoss: 0.0840 \n",
            "| Epoch [ 33/ 60] Iter[181/251]\t\tLoss: 0.6761 \n",
            "| Epoch [ 33/ 60] Iter[186/251]\t\tLoss: 1.1606 \n",
            "| Epoch [ 33/ 60] Iter[191/251]\t\tLoss: 0.4020 \n",
            "| Epoch [ 33/ 60] Iter[196/251]\t\tLoss: 0.3860 \n",
            "| Epoch [ 33/ 60] Iter[201/251]\t\tLoss: 6.0172 \n",
            "| Epoch [ 33/ 60] Iter[206/251]\t\tLoss: 0.4502 \n",
            "| Epoch [ 33/ 60] Iter[211/251]\t\tLoss: 0.4436 \n",
            "| Epoch [ 33/ 60] Iter[216/251]\t\tLoss: 0.2751 \n",
            "| Epoch [ 33/ 60] Iter[221/251]\t\tLoss: 0.4330 \n",
            "| Epoch [ 33/ 60] Iter[226/251]\t\tLoss: 0.1125 \n",
            "| Epoch [ 33/ 60] Iter[231/251]\t\tLoss: 0.1552 \n",
            "| Epoch [ 33/ 60] Iter[236/251]\t\tLoss: 0.3392 \n",
            "| Epoch [ 33/ 60] Iter[241/251]\t\tLoss: 0.1463 \n",
            "| Epoch [ 33/ 60] Iter[246/251]\t\tLoss: 0.9117 \n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 0.2195 \n",
            "\n",
            "=> Training Epoch #34, LR=0.0000\n",
            "| Epoch [ 34/ 60] Iter[  1/251]\t\tLoss: 1.2354 \n",
            "| Epoch [ 34/ 60] Iter[  6/251]\t\tLoss: 0.3656 \n",
            "| Epoch [ 34/ 60] Iter[ 11/251]\t\tLoss: 0.2377 \n",
            "| Epoch [ 34/ 60] Iter[ 16/251]\t\tLoss: 0.6866 \n",
            "| Epoch [ 34/ 60] Iter[ 21/251]\t\tLoss: 0.6988 \n",
            "| Epoch [ 34/ 60] Iter[ 26/251]\t\tLoss: 0.1963 \n",
            "| Epoch [ 34/ 60] Iter[ 31/251]\t\tLoss: 0.3296 \n",
            "| Epoch [ 34/ 60] Iter[ 36/251]\t\tLoss: 0.1398 \n",
            "| Epoch [ 34/ 60] Iter[ 41/251]\t\tLoss: 0.4513 \n",
            "| Epoch [ 34/ 60] Iter[ 46/251]\t\tLoss: 5.8056 \n",
            "| Epoch [ 34/ 60] Iter[ 51/251]\t\tLoss: 6.2910 \n",
            "| Epoch [ 34/ 60] Iter[ 56/251]\t\tLoss: 0.5012 \n",
            "| Epoch [ 34/ 60] Iter[ 61/251]\t\tLoss: 0.5473 \n",
            "| Epoch [ 34/ 60] Iter[ 66/251]\t\tLoss: 0.3832 \n",
            "| Epoch [ 34/ 60] Iter[ 71/251]\t\tLoss: 0.3006 \n",
            "| Epoch [ 34/ 60] Iter[ 76/251]\t\tLoss: 0.4279 \n",
            "| Epoch [ 34/ 60] Iter[ 81/251]\t\tLoss: 0.3163 \n",
            "| Epoch [ 34/ 60] Iter[ 86/251]\t\tLoss: 1.0571 \n",
            "| Epoch [ 34/ 60] Iter[ 91/251]\t\tLoss: 0.2443 \n",
            "| Epoch [ 34/ 60] Iter[ 96/251]\t\tLoss: 1.1284 \n",
            "| Epoch [ 34/ 60] Iter[101/251]\t\tLoss: 0.3986 \n",
            "| Epoch [ 34/ 60] Iter[106/251]\t\tLoss: 0.2088 \n",
            "| Epoch [ 34/ 60] Iter[111/251]\t\tLoss: 2.7235 \n",
            "| Epoch [ 34/ 60] Iter[116/251]\t\tLoss: 0.2022 \n",
            "| Epoch [ 34/ 60] Iter[121/251]\t\tLoss: 0.5864 \n",
            "| Epoch [ 34/ 60] Iter[126/251]\t\tLoss: 0.2177 \n",
            "| Epoch [ 34/ 60] Iter[131/251]\t\tLoss: 0.1326 \n",
            "| Epoch [ 34/ 60] Iter[136/251]\t\tLoss: 0.2372 \n",
            "| Epoch [ 34/ 60] Iter[141/251]\t\tLoss: 0.9291 \n",
            "| Epoch [ 34/ 60] Iter[146/251]\t\tLoss: 0.1315 \n",
            "| Epoch [ 34/ 60] Iter[151/251]\t\tLoss: 0.2998 \n",
            "| Epoch [ 34/ 60] Iter[156/251]\t\tLoss: 6.2432 \n",
            "| Epoch [ 34/ 60] Iter[161/251]\t\tLoss: 0.7423 \n",
            "| Epoch [ 34/ 60] Iter[166/251]\t\tLoss: 0.1701 \n",
            "| Epoch [ 34/ 60] Iter[171/251]\t\tLoss: 0.4697 \n",
            "| Epoch [ 34/ 60] Iter[176/251]\t\tLoss: 0.1840 \n",
            "| Epoch [ 34/ 60] Iter[181/251]\t\tLoss: 0.3936 \n",
            "| Epoch [ 34/ 60] Iter[186/251]\t\tLoss: 6.3540 \n",
            "| Epoch [ 34/ 60] Iter[191/251]\t\tLoss: 0.3854 \n",
            "| Epoch [ 34/ 60] Iter[196/251]\t\tLoss: 5.9452 \n",
            "| Epoch [ 34/ 60] Iter[201/251]\t\tLoss: 0.2348 \n",
            "| Epoch [ 34/ 60] Iter[206/251]\t\tLoss: 0.1509 \n",
            "| Epoch [ 34/ 60] Iter[211/251]\t\tLoss: 6.3157 \n",
            "| Epoch [ 34/ 60] Iter[216/251]\t\tLoss: 6.2871 \n",
            "| Epoch [ 34/ 60] Iter[221/251]\t\tLoss: 0.0941 \n",
            "| Epoch [ 34/ 60] Iter[226/251]\t\tLoss: 0.1292 \n",
            "| Epoch [ 34/ 60] Iter[231/251]\t\tLoss: 6.2428 \n",
            "| Epoch [ 34/ 60] Iter[236/251]\t\tLoss: 0.1301 \n",
            "| Epoch [ 34/ 60] Iter[241/251]\t\tLoss: 0.1301 \n",
            "| Epoch [ 34/ 60] Iter[246/251]\t\tLoss: 0.1216 \n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 0.2197 \n",
            "\n",
            "=> Training Epoch #35, LR=0.0000\n",
            "| Epoch [ 35/ 60] Iter[  1/251]\t\tLoss: 0.1348 \n",
            "| Epoch [ 35/ 60] Iter[  6/251]\t\tLoss: 0.1282 \n",
            "| Epoch [ 35/ 60] Iter[ 11/251]\t\tLoss: 0.2864 \n",
            "| Epoch [ 35/ 60] Iter[ 16/251]\t\tLoss: 1.2128 \n",
            "| Epoch [ 35/ 60] Iter[ 21/251]\t\tLoss: 0.7882 \n",
            "| Epoch [ 35/ 60] Iter[ 26/251]\t\tLoss: 0.3935 \n",
            "| Epoch [ 35/ 60] Iter[ 31/251]\t\tLoss: 0.8533 \n",
            "| Epoch [ 35/ 60] Iter[ 36/251]\t\tLoss: 0.0422 \n",
            "| Epoch [ 35/ 60] Iter[ 41/251]\t\tLoss: 0.1481 \n",
            "| Epoch [ 35/ 60] Iter[ 46/251]\t\tLoss: 0.2834 \n",
            "| Epoch [ 35/ 60] Iter[ 51/251]\t\tLoss: 0.3198 \n",
            "| Epoch [ 35/ 60] Iter[ 56/251]\t\tLoss: 0.7996 \n",
            "| Epoch [ 35/ 60] Iter[ 61/251]\t\tLoss: 0.4045 \n",
            "| Epoch [ 35/ 60] Iter[ 66/251]\t\tLoss: 1.3717 \n",
            "| Epoch [ 35/ 60] Iter[ 71/251]\t\tLoss: 0.6509 \n",
            "| Epoch [ 35/ 60] Iter[ 76/251]\t\tLoss: 1.1468 \n",
            "| Epoch [ 35/ 60] Iter[ 81/251]\t\tLoss: 0.3196 \n",
            "| Epoch [ 35/ 60] Iter[ 86/251]\t\tLoss: 0.5131 \n",
            "| Epoch [ 35/ 60] Iter[ 91/251]\t\tLoss: 0.1004 \n",
            "| Epoch [ 35/ 60] Iter[ 96/251]\t\tLoss: 0.5071 \n",
            "| Epoch [ 35/ 60] Iter[101/251]\t\tLoss: 0.1586 \n",
            "| Epoch [ 35/ 60] Iter[106/251]\t\tLoss: 0.5016 \n",
            "| Epoch [ 35/ 60] Iter[111/251]\t\tLoss: 0.6260 \n",
            "| Epoch [ 35/ 60] Iter[116/251]\t\tLoss: 0.1384 \n",
            "| Epoch [ 35/ 60] Iter[121/251]\t\tLoss: 0.4992 \n",
            "| Epoch [ 35/ 60] Iter[126/251]\t\tLoss: 0.4830 \n",
            "| Epoch [ 35/ 60] Iter[131/251]\t\tLoss: 0.1624 \n",
            "| Epoch [ 35/ 60] Iter[136/251]\t\tLoss: 0.5441 \n",
            "| Epoch [ 35/ 60] Iter[141/251]\t\tLoss: 0.6503 \n",
            "| Epoch [ 35/ 60] Iter[146/251]\t\tLoss: 1.1831 \n",
            "| Epoch [ 35/ 60] Iter[151/251]\t\tLoss: 0.4057 \n",
            "| Epoch [ 35/ 60] Iter[156/251]\t\tLoss: 0.1215 \n",
            "| Epoch [ 35/ 60] Iter[161/251]\t\tLoss: 0.5056 \n",
            "| Epoch [ 35/ 60] Iter[166/251]\t\tLoss: 0.4230 \n",
            "| Epoch [ 35/ 60] Iter[171/251]\t\tLoss: 0.4632 \n",
            "| Epoch [ 35/ 60] Iter[176/251]\t\tLoss: 0.4502 \n",
            "| Epoch [ 35/ 60] Iter[181/251]\t\tLoss: 0.4909 \n",
            "| Epoch [ 35/ 60] Iter[186/251]\t\tLoss: 0.3040 \n",
            "| Epoch [ 35/ 60] Iter[191/251]\t\tLoss: 0.1949 \n",
            "| Epoch [ 35/ 60] Iter[196/251]\t\tLoss: 0.8306 \n",
            "| Epoch [ 35/ 60] Iter[201/251]\t\tLoss: 0.3706 \n",
            "| Epoch [ 35/ 60] Iter[206/251]\t\tLoss: 0.6215 \n",
            "| Epoch [ 35/ 60] Iter[211/251]\t\tLoss: 0.5089 \n",
            "| Epoch [ 35/ 60] Iter[216/251]\t\tLoss: 0.4770 \n",
            "| Epoch [ 35/ 60] Iter[221/251]\t\tLoss: 1.5620 \n",
            "| Epoch [ 35/ 60] Iter[226/251]\t\tLoss: 0.0328 \n",
            "| Epoch [ 35/ 60] Iter[231/251]\t\tLoss: 0.1976 \n",
            "| Epoch [ 35/ 60] Iter[236/251]\t\tLoss: 0.1062 \n",
            "| Epoch [ 35/ 60] Iter[241/251]\t\tLoss: 0.9592 \n",
            "| Epoch [ 35/ 60] Iter[246/251]\t\tLoss: 0.4636 \n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 0.2198 \n",
            "\n",
            "=> Training Epoch #36, LR=0.0000\n",
            "| Epoch [ 36/ 60] Iter[  1/251]\t\tLoss: 0.4399 \n",
            "| Epoch [ 36/ 60] Iter[  6/251]\t\tLoss: 0.8288 \n",
            "| Epoch [ 36/ 60] Iter[ 11/251]\t\tLoss: 0.5760 \n",
            "| Epoch [ 36/ 60] Iter[ 16/251]\t\tLoss: 0.5153 \n",
            "| Epoch [ 36/ 60] Iter[ 21/251]\t\tLoss: 0.2130 \n",
            "| Epoch [ 36/ 60] Iter[ 26/251]\t\tLoss: 0.1195 \n",
            "| Epoch [ 36/ 60] Iter[ 31/251]\t\tLoss: 1.5471 \n",
            "| Epoch [ 36/ 60] Iter[ 36/251]\t\tLoss: 1.6141 \n",
            "| Epoch [ 36/ 60] Iter[ 41/251]\t\tLoss: 0.6279 \n",
            "| Epoch [ 36/ 60] Iter[ 46/251]\t\tLoss: 0.2740 \n",
            "| Epoch [ 36/ 60] Iter[ 51/251]\t\tLoss: 0.4232 \n",
            "| Epoch [ 36/ 60] Iter[ 56/251]\t\tLoss: 0.1962 \n",
            "| Epoch [ 36/ 60] Iter[ 61/251]\t\tLoss: 0.1900 \n",
            "| Epoch [ 36/ 60] Iter[ 66/251]\t\tLoss: 0.5570 \n",
            "| Epoch [ 36/ 60] Iter[ 71/251]\t\tLoss: 0.6054 \n",
            "| Epoch [ 36/ 60] Iter[ 76/251]\t\tLoss: 0.0289 \n",
            "| Epoch [ 36/ 60] Iter[ 81/251]\t\tLoss: 0.5962 \n",
            "| Epoch [ 36/ 60] Iter[ 86/251]\t\tLoss: 0.5403 \n",
            "| Epoch [ 36/ 60] Iter[ 91/251]\t\tLoss: 0.8252 \n",
            "| Epoch [ 36/ 60] Iter[ 96/251]\t\tLoss: 0.2865 \n",
            "| Epoch [ 36/ 60] Iter[101/251]\t\tLoss: 0.2592 \n",
            "| Epoch [ 36/ 60] Iter[106/251]\t\tLoss: 6.3505 \n",
            "| Epoch [ 36/ 60] Iter[111/251]\t\tLoss: 0.1549 \n",
            "| Epoch [ 36/ 60] Iter[116/251]\t\tLoss: 6.7514 \n",
            "| Epoch [ 36/ 60] Iter[121/251]\t\tLoss: 0.3418 \n",
            "| Epoch [ 36/ 60] Iter[126/251]\t\tLoss: 3.1678 \n",
            "| Epoch [ 36/ 60] Iter[131/251]\t\tLoss: 0.0938 \n",
            "| Epoch [ 36/ 60] Iter[136/251]\t\tLoss: 0.5665 \n",
            "| Epoch [ 36/ 60] Iter[141/251]\t\tLoss: 1.0796 \n",
            "| Epoch [ 36/ 60] Iter[146/251]\t\tLoss: 0.5856 \n",
            "| Epoch [ 36/ 60] Iter[151/251]\t\tLoss: 0.2844 \n",
            "| Epoch [ 36/ 60] Iter[156/251]\t\tLoss: 0.7173 \n",
            "| Epoch [ 36/ 60] Iter[161/251]\t\tLoss: 0.9955 \n",
            "| Epoch [ 36/ 60] Iter[166/251]\t\tLoss: 0.7016 \n",
            "| Epoch [ 36/ 60] Iter[171/251]\t\tLoss: 0.3836 \n",
            "| Epoch [ 36/ 60] Iter[176/251]\t\tLoss: 0.1652 \n",
            "| Epoch [ 36/ 60] Iter[181/251]\t\tLoss: 2.5895 \n",
            "| Epoch [ 36/ 60] Iter[186/251]\t\tLoss: 0.4086 \n",
            "| Epoch [ 36/ 60] Iter[191/251]\t\tLoss: 0.2854 \n",
            "| Epoch [ 36/ 60] Iter[196/251]\t\tLoss: 0.2286 \n",
            "| Epoch [ 36/ 60] Iter[201/251]\t\tLoss: 0.3839 \n",
            "| Epoch [ 36/ 60] Iter[206/251]\t\tLoss: 0.0848 \n",
            "| Epoch [ 36/ 60] Iter[211/251]\t\tLoss: 0.1009 \n",
            "| Epoch [ 36/ 60] Iter[216/251]\t\tLoss: 0.0972 \n",
            "| Epoch [ 36/ 60] Iter[221/251]\t\tLoss: 0.5687 \n",
            "| Epoch [ 36/ 60] Iter[226/251]\t\tLoss: 0.0545 \n",
            "| Epoch [ 36/ 60] Iter[231/251]\t\tLoss: 0.4126 \n",
            "| Epoch [ 36/ 60] Iter[236/251]\t\tLoss: 0.3639 \n",
            "| Epoch [ 36/ 60] Iter[241/251]\t\tLoss: 0.5882 \n",
            "| Epoch [ 36/ 60] Iter[246/251]\t\tLoss: 0.2399 \n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.2193 \n",
            "\n",
            "=> Training Epoch #37, LR=0.0000\n",
            "| Epoch [ 37/ 60] Iter[  1/251]\t\tLoss: 0.5416 \n",
            "| Epoch [ 37/ 60] Iter[  6/251]\t\tLoss: 0.9058 \n",
            "| Epoch [ 37/ 60] Iter[ 11/251]\t\tLoss: 0.1006 \n",
            "| Epoch [ 37/ 60] Iter[ 16/251]\t\tLoss: 0.4971 \n",
            "| Epoch [ 37/ 60] Iter[ 21/251]\t\tLoss: 0.3920 \n",
            "| Epoch [ 37/ 60] Iter[ 26/251]\t\tLoss: 1.3165 \n",
            "| Epoch [ 37/ 60] Iter[ 31/251]\t\tLoss: 0.1087 \n",
            "| Epoch [ 37/ 60] Iter[ 36/251]\t\tLoss: 0.1767 \n",
            "| Epoch [ 37/ 60] Iter[ 41/251]\t\tLoss: 0.3137 \n",
            "| Epoch [ 37/ 60] Iter[ 46/251]\t\tLoss: 0.1143 \n",
            "| Epoch [ 37/ 60] Iter[ 51/251]\t\tLoss: 0.7745 \n",
            "| Epoch [ 37/ 60] Iter[ 56/251]\t\tLoss: 0.5486 \n",
            "| Epoch [ 37/ 60] Iter[ 61/251]\t\tLoss: 0.1507 \n",
            "| Epoch [ 37/ 60] Iter[ 66/251]\t\tLoss: 0.4242 \n",
            "| Epoch [ 37/ 60] Iter[ 71/251]\t\tLoss: 0.0446 \n",
            "| Epoch [ 37/ 60] Iter[ 76/251]\t\tLoss: 0.4342 \n",
            "| Epoch [ 37/ 60] Iter[ 81/251]\t\tLoss: 0.4179 \n",
            "| Epoch [ 37/ 60] Iter[ 86/251]\t\tLoss: 0.7717 \n",
            "| Epoch [ 37/ 60] Iter[ 91/251]\t\tLoss: 0.2221 \n",
            "| Epoch [ 37/ 60] Iter[ 96/251]\t\tLoss: 0.0831 \n",
            "| Epoch [ 37/ 60] Iter[101/251]\t\tLoss: 0.4209 \n",
            "| Epoch [ 37/ 60] Iter[106/251]\t\tLoss: 0.5427 \n",
            "| Epoch [ 37/ 60] Iter[111/251]\t\tLoss: 0.6489 \n",
            "| Epoch [ 37/ 60] Iter[116/251]\t\tLoss: 0.4975 \n",
            "| Epoch [ 37/ 60] Iter[121/251]\t\tLoss: 1.1305 \n",
            "| Epoch [ 37/ 60] Iter[126/251]\t\tLoss: 0.2608 \n",
            "| Epoch [ 37/ 60] Iter[131/251]\t\tLoss: 0.4251 \n",
            "| Epoch [ 37/ 60] Iter[136/251]\t\tLoss: 0.4516 \n",
            "| Epoch [ 37/ 60] Iter[141/251]\t\tLoss: 0.4933 \n",
            "| Epoch [ 37/ 60] Iter[146/251]\t\tLoss: 0.1966 \n",
            "| Epoch [ 37/ 60] Iter[151/251]\t\tLoss: 0.4804 \n",
            "| Epoch [ 37/ 60] Iter[156/251]\t\tLoss: 0.0631 \n",
            "| Epoch [ 37/ 60] Iter[161/251]\t\tLoss: 0.0559 \n",
            "| Epoch [ 37/ 60] Iter[166/251]\t\tLoss: 0.6476 \n",
            "| Epoch [ 37/ 60] Iter[171/251]\t\tLoss: 0.9527 \n",
            "| Epoch [ 37/ 60] Iter[176/251]\t\tLoss: 0.6946 \n",
            "| Epoch [ 37/ 60] Iter[181/251]\t\tLoss: 0.1149 \n",
            "| Epoch [ 37/ 60] Iter[186/251]\t\tLoss: 0.7647 \n",
            "| Epoch [ 37/ 60] Iter[191/251]\t\tLoss: 0.4358 \n",
            "| Epoch [ 37/ 60] Iter[196/251]\t\tLoss: 5.7832 \n",
            "| Epoch [ 37/ 60] Iter[201/251]\t\tLoss: 0.4641 \n",
            "| Epoch [ 37/ 60] Iter[206/251]\t\tLoss: 2.0650 \n",
            "| Epoch [ 37/ 60] Iter[211/251]\t\tLoss: 0.1701 \n",
            "| Epoch [ 37/ 60] Iter[216/251]\t\tLoss: 0.8550 \n",
            "| Epoch [ 37/ 60] Iter[221/251]\t\tLoss: 0.4772 \n",
            "| Epoch [ 37/ 60] Iter[226/251]\t\tLoss: 0.9279 \n",
            "| Epoch [ 37/ 60] Iter[231/251]\t\tLoss: 0.4779 \n",
            "| Epoch [ 37/ 60] Iter[236/251]\t\tLoss: 0.3106 \n",
            "| Epoch [ 37/ 60] Iter[241/251]\t\tLoss: 0.1631 \n",
            "| Epoch [ 37/ 60] Iter[246/251]\t\tLoss: 0.0666 \n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 0.2189 \n",
            "\n",
            "=> Training Epoch #38, LR=0.0000\n",
            "| Epoch [ 38/ 60] Iter[  1/251]\t\tLoss: 6.0847 \n",
            "| Epoch [ 38/ 60] Iter[  6/251]\t\tLoss: 0.2932 \n",
            "| Epoch [ 38/ 60] Iter[ 11/251]\t\tLoss: 2.3603 \n",
            "| Epoch [ 38/ 60] Iter[ 16/251]\t\tLoss: 0.0752 \n",
            "| Epoch [ 38/ 60] Iter[ 21/251]\t\tLoss: 0.2714 \n",
            "| Epoch [ 38/ 60] Iter[ 26/251]\t\tLoss: 0.1740 \n",
            "| Epoch [ 38/ 60] Iter[ 31/251]\t\tLoss: 6.6284 \n",
            "| Epoch [ 38/ 60] Iter[ 36/251]\t\tLoss: 0.3246 \n",
            "| Epoch [ 38/ 60] Iter[ 41/251]\t\tLoss: 0.4554 \n",
            "| Epoch [ 38/ 60] Iter[ 46/251]\t\tLoss: 0.3571 \n",
            "| Epoch [ 38/ 60] Iter[ 51/251]\t\tLoss: 0.1328 \n",
            "| Epoch [ 38/ 60] Iter[ 56/251]\t\tLoss: 0.3426 \n",
            "| Epoch [ 38/ 60] Iter[ 61/251]\t\tLoss: 0.3682 \n",
            "| Epoch [ 38/ 60] Iter[ 66/251]\t\tLoss: 5.8690 \n",
            "| Epoch [ 38/ 60] Iter[ 71/251]\t\tLoss: 0.1504 \n",
            "| Epoch [ 38/ 60] Iter[ 76/251]\t\tLoss: 0.3251 \n",
            "| Epoch [ 38/ 60] Iter[ 81/251]\t\tLoss: 0.3758 \n",
            "| Epoch [ 38/ 60] Iter[ 86/251]\t\tLoss: 0.2753 \n",
            "| Epoch [ 38/ 60] Iter[ 91/251]\t\tLoss: 0.4203 \n",
            "| Epoch [ 38/ 60] Iter[ 96/251]\t\tLoss: 0.3955 \n",
            "| Epoch [ 38/ 60] Iter[101/251]\t\tLoss: 0.4724 \n",
            "| Epoch [ 38/ 60] Iter[106/251]\t\tLoss: 0.1463 \n",
            "| Epoch [ 38/ 60] Iter[111/251]\t\tLoss: 0.3856 \n",
            "| Epoch [ 38/ 60] Iter[116/251]\t\tLoss: 0.0312 \n",
            "| Epoch [ 38/ 60] Iter[121/251]\t\tLoss: 0.5237 \n",
            "| Epoch [ 38/ 60] Iter[126/251]\t\tLoss: 0.8243 \n",
            "| Epoch [ 38/ 60] Iter[131/251]\t\tLoss: 0.1980 \n",
            "| Epoch [ 38/ 60] Iter[136/251]\t\tLoss: 0.7110 \n",
            "| Epoch [ 38/ 60] Iter[141/251]\t\tLoss: 0.3012 \n",
            "| Epoch [ 38/ 60] Iter[146/251]\t\tLoss: 0.3019 \n",
            "| Epoch [ 38/ 60] Iter[151/251]\t\tLoss: 1.6775 \n",
            "| Epoch [ 38/ 60] Iter[156/251]\t\tLoss: 0.5622 \n",
            "| Epoch [ 38/ 60] Iter[161/251]\t\tLoss: 0.6524 \n",
            "| Epoch [ 38/ 60] Iter[166/251]\t\tLoss: 1.0640 \n",
            "| Epoch [ 38/ 60] Iter[171/251]\t\tLoss: 6.4880 \n",
            "| Epoch [ 38/ 60] Iter[176/251]\t\tLoss: 0.3372 \n",
            "| Epoch [ 38/ 60] Iter[181/251]\t\tLoss: 0.7204 \n",
            "| Epoch [ 38/ 60] Iter[186/251]\t\tLoss: 1.9079 \n",
            "| Epoch [ 38/ 60] Iter[191/251]\t\tLoss: 0.2266 \n",
            "| Epoch [ 38/ 60] Iter[196/251]\t\tLoss: 0.2596 \n",
            "| Epoch [ 38/ 60] Iter[201/251]\t\tLoss: 1.2086 \n",
            "| Epoch [ 38/ 60] Iter[206/251]\t\tLoss: 0.3161 \n",
            "| Epoch [ 38/ 60] Iter[211/251]\t\tLoss: 3.6142 \n",
            "| Epoch [ 38/ 60] Iter[216/251]\t\tLoss: 0.3403 \n",
            "| Epoch [ 38/ 60] Iter[221/251]\t\tLoss: 0.2199 \n",
            "| Epoch [ 38/ 60] Iter[226/251]\t\tLoss: 0.1037 \n",
            "| Epoch [ 38/ 60] Iter[231/251]\t\tLoss: 0.4662 \n",
            "| Epoch [ 38/ 60] Iter[236/251]\t\tLoss: 0.3671 \n",
            "| Epoch [ 38/ 60] Iter[241/251]\t\tLoss: 0.7522 \n",
            "| Epoch [ 38/ 60] Iter[246/251]\t\tLoss: 6.3395 \n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 0.2198 \n",
            "\n",
            "=> Training Epoch #39, LR=0.0000\n",
            "| Epoch [ 39/ 60] Iter[  1/251]\t\tLoss: 0.1326 \n",
            "| Epoch [ 39/ 60] Iter[  6/251]\t\tLoss: 0.7198 \n",
            "| Epoch [ 39/ 60] Iter[ 11/251]\t\tLoss: 1.2850 \n",
            "| Epoch [ 39/ 60] Iter[ 16/251]\t\tLoss: 0.6819 \n",
            "| Epoch [ 39/ 60] Iter[ 21/251]\t\tLoss: 0.3713 \n",
            "| Epoch [ 39/ 60] Iter[ 26/251]\t\tLoss: 0.3968 \n",
            "| Epoch [ 39/ 60] Iter[ 31/251]\t\tLoss: 0.0588 \n",
            "| Epoch [ 39/ 60] Iter[ 36/251]\t\tLoss: 0.2890 \n",
            "| Epoch [ 39/ 60] Iter[ 41/251]\t\tLoss: 0.7291 \n",
            "| Epoch [ 39/ 60] Iter[ 46/251]\t\tLoss: 6.0878 \n",
            "| Epoch [ 39/ 60] Iter[ 51/251]\t\tLoss: 0.1614 \n",
            "| Epoch [ 39/ 60] Iter[ 56/251]\t\tLoss: 1.6194 \n",
            "| Epoch [ 39/ 60] Iter[ 61/251]\t\tLoss: 0.5535 \n",
            "| Epoch [ 39/ 60] Iter[ 66/251]\t\tLoss: 0.6288 \n",
            "| Epoch [ 39/ 60] Iter[ 71/251]\t\tLoss: 0.6858 \n",
            "| Epoch [ 39/ 60] Iter[ 76/251]\t\tLoss: 0.2486 \n",
            "| Epoch [ 39/ 60] Iter[ 81/251]\t\tLoss: 0.0209 \n",
            "| Epoch [ 39/ 60] Iter[ 86/251]\t\tLoss: 0.2807 \n",
            "| Epoch [ 39/ 60] Iter[ 91/251]\t\tLoss: 1.7960 \n",
            "| Epoch [ 39/ 60] Iter[ 96/251]\t\tLoss: 0.5644 \n",
            "| Epoch [ 39/ 60] Iter[101/251]\t\tLoss: 0.3380 \n",
            "| Epoch [ 39/ 60] Iter[106/251]\t\tLoss: 0.3416 \n",
            "| Epoch [ 39/ 60] Iter[111/251]\t\tLoss: 0.0653 \n",
            "| Epoch [ 39/ 60] Iter[116/251]\t\tLoss: 0.1324 \n",
            "| Epoch [ 39/ 60] Iter[121/251]\t\tLoss: 0.1441 \n",
            "| Epoch [ 39/ 60] Iter[126/251]\t\tLoss: 0.1783 \n",
            "| Epoch [ 39/ 60] Iter[131/251]\t\tLoss: 0.4406 \n",
            "| Epoch [ 39/ 60] Iter[136/251]\t\tLoss: 0.4223 \n",
            "| Epoch [ 39/ 60] Iter[141/251]\t\tLoss: 0.2420 \n",
            "| Epoch [ 39/ 60] Iter[146/251]\t\tLoss: 0.3011 \n",
            "| Epoch [ 39/ 60] Iter[151/251]\t\tLoss: 1.4298 \n",
            "| Epoch [ 39/ 60] Iter[156/251]\t\tLoss: 0.1645 \n",
            "| Epoch [ 39/ 60] Iter[161/251]\t\tLoss: 0.3728 \n",
            "| Epoch [ 39/ 60] Iter[166/251]\t\tLoss: 0.2383 \n",
            "| Epoch [ 39/ 60] Iter[171/251]\t\tLoss: 0.0338 \n",
            "| Epoch [ 39/ 60] Iter[176/251]\t\tLoss: 0.1752 \n",
            "| Epoch [ 39/ 60] Iter[181/251]\t\tLoss: 0.5846 \n",
            "| Epoch [ 39/ 60] Iter[186/251]\t\tLoss: 0.5081 \n",
            "| Epoch [ 39/ 60] Iter[191/251]\t\tLoss: 0.5132 \n",
            "| Epoch [ 39/ 60] Iter[196/251]\t\tLoss: 0.3190 \n",
            "| Epoch [ 39/ 60] Iter[201/251]\t\tLoss: 0.4593 \n",
            "| Epoch [ 39/ 60] Iter[206/251]\t\tLoss: 0.1384 \n",
            "| Epoch [ 39/ 60] Iter[211/251]\t\tLoss: 6.3970 \n",
            "| Epoch [ 39/ 60] Iter[216/251]\t\tLoss: 1.6630 \n",
            "| Epoch [ 39/ 60] Iter[221/251]\t\tLoss: 0.7158 \n",
            "| Epoch [ 39/ 60] Iter[226/251]\t\tLoss: 0.2155 \n",
            "| Epoch [ 39/ 60] Iter[231/251]\t\tLoss: 6.0998 \n",
            "| Epoch [ 39/ 60] Iter[236/251]\t\tLoss: 0.5015 \n",
            "| Epoch [ 39/ 60] Iter[241/251]\t\tLoss: 0.5222 \n",
            "| Epoch [ 39/ 60] Iter[246/251]\t\tLoss: 0.0886 \n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 0.2196 \n",
            "\n",
            "=> Training Epoch #40, LR=0.0000\n",
            "| Epoch [ 40/ 60] Iter[  1/251]\t\tLoss: 3.5773 \n",
            "| Epoch [ 40/ 60] Iter[  6/251]\t\tLoss: 0.3920 \n",
            "| Epoch [ 40/ 60] Iter[ 11/251]\t\tLoss: 6.9373 \n",
            "| Epoch [ 40/ 60] Iter[ 16/251]\t\tLoss: 0.8281 \n",
            "| Epoch [ 40/ 60] Iter[ 21/251]\t\tLoss: 0.0528 \n",
            "| Epoch [ 40/ 60] Iter[ 26/251]\t\tLoss: 0.9101 \n",
            "| Epoch [ 40/ 60] Iter[ 31/251]\t\tLoss: 0.2970 \n",
            "| Epoch [ 40/ 60] Iter[ 36/251]\t\tLoss: 0.3213 \n",
            "| Epoch [ 40/ 60] Iter[ 41/251]\t\tLoss: 0.5617 \n",
            "| Epoch [ 40/ 60] Iter[ 46/251]\t\tLoss: 0.4911 \n",
            "| Epoch [ 40/ 60] Iter[ 51/251]\t\tLoss: 0.5078 \n",
            "| Epoch [ 40/ 60] Iter[ 56/251]\t\tLoss: 0.4752 \n",
            "| Epoch [ 40/ 60] Iter[ 61/251]\t\tLoss: 0.4410 \n",
            "| Epoch [ 40/ 60] Iter[ 66/251]\t\tLoss: 0.2266 \n",
            "| Epoch [ 40/ 60] Iter[ 71/251]\t\tLoss: 0.1128 \n",
            "| Epoch [ 40/ 60] Iter[ 76/251]\t\tLoss: 6.1693 \n",
            "| Epoch [ 40/ 60] Iter[ 81/251]\t\tLoss: 5.9428 \n",
            "| Epoch [ 40/ 60] Iter[ 86/251]\t\tLoss: 0.0968 \n",
            "| Epoch [ 40/ 60] Iter[ 91/251]\t\tLoss: 0.3036 \n",
            "| Epoch [ 40/ 60] Iter[ 96/251]\t\tLoss: 0.3903 \n",
            "| Epoch [ 40/ 60] Iter[101/251]\t\tLoss: 0.7159 \n",
            "| Epoch [ 40/ 60] Iter[106/251]\t\tLoss: 0.3483 \n",
            "| Epoch [ 40/ 60] Iter[111/251]\t\tLoss: 0.7502 \n",
            "| Epoch [ 40/ 60] Iter[116/251]\t\tLoss: 0.4016 \n",
            "| Epoch [ 40/ 60] Iter[121/251]\t\tLoss: 0.5572 \n",
            "| Epoch [ 40/ 60] Iter[126/251]\t\tLoss: 1.1874 \n",
            "| Epoch [ 40/ 60] Iter[131/251]\t\tLoss: 0.0814 \n",
            "| Epoch [ 40/ 60] Iter[136/251]\t\tLoss: 0.3663 \n",
            "| Epoch [ 40/ 60] Iter[141/251]\t\tLoss: 6.9137 \n",
            "| Epoch [ 40/ 60] Iter[146/251]\t\tLoss: 0.1897 \n",
            "| Epoch [ 40/ 60] Iter[151/251]\t\tLoss: 6.2876 \n",
            "| Epoch [ 40/ 60] Iter[156/251]\t\tLoss: 0.6384 \n",
            "| Epoch [ 40/ 60] Iter[161/251]\t\tLoss: 0.1414 \n",
            "| Epoch [ 40/ 60] Iter[166/251]\t\tLoss: 0.4966 \n",
            "| Epoch [ 40/ 60] Iter[171/251]\t\tLoss: 0.2950 \n",
            "| Epoch [ 40/ 60] Iter[176/251]\t\tLoss: 0.1311 \n",
            "| Epoch [ 40/ 60] Iter[181/251]\t\tLoss: 1.2005 \n",
            "| Epoch [ 40/ 60] Iter[186/251]\t\tLoss: 0.5834 \n",
            "| Epoch [ 40/ 60] Iter[191/251]\t\tLoss: 0.5292 \n",
            "| Epoch [ 40/ 60] Iter[196/251]\t\tLoss: 0.3934 \n",
            "| Epoch [ 40/ 60] Iter[201/251]\t\tLoss: 0.4134 \n",
            "| Epoch [ 40/ 60] Iter[206/251]\t\tLoss: 0.4323 \n",
            "| Epoch [ 40/ 60] Iter[211/251]\t\tLoss: 0.0356 \n",
            "| Epoch [ 40/ 60] Iter[216/251]\t\tLoss: 0.4783 \n",
            "| Epoch [ 40/ 60] Iter[221/251]\t\tLoss: 1.4487 \n",
            "| Epoch [ 40/ 60] Iter[226/251]\t\tLoss: 1.5444 \n",
            "| Epoch [ 40/ 60] Iter[231/251]\t\tLoss: 0.0730 \n",
            "| Epoch [ 40/ 60] Iter[236/251]\t\tLoss: 0.3229 \n",
            "| Epoch [ 40/ 60] Iter[241/251]\t\tLoss: 0.8416 \n",
            "| Epoch [ 40/ 60] Iter[246/251]\t\tLoss: 0.1142 \n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 0.2195 \n",
            "\n",
            "=> Training Epoch #41, LR=0.0000\n",
            "| Epoch [ 41/ 60] Iter[  1/251]\t\tLoss: 0.9649 \n",
            "| Epoch [ 41/ 60] Iter[  6/251]\t\tLoss: 0.2205 \n",
            "| Epoch [ 41/ 60] Iter[ 11/251]\t\tLoss: 0.4671 \n",
            "| Epoch [ 41/ 60] Iter[ 16/251]\t\tLoss: 0.2272 \n",
            "| Epoch [ 41/ 60] Iter[ 21/251]\t\tLoss: 0.1369 \n",
            "| Epoch [ 41/ 60] Iter[ 26/251]\t\tLoss: 0.5049 \n",
            "| Epoch [ 41/ 60] Iter[ 31/251]\t\tLoss: 6.7223 \n",
            "| Epoch [ 41/ 60] Iter[ 36/251]\t\tLoss: 0.5566 \n",
            "| Epoch [ 41/ 60] Iter[ 41/251]\t\tLoss: 0.5531 \n",
            "| Epoch [ 41/ 60] Iter[ 46/251]\t\tLoss: 0.5200 \n",
            "| Epoch [ 41/ 60] Iter[ 51/251]\t\tLoss: 0.2037 \n",
            "| Epoch [ 41/ 60] Iter[ 56/251]\t\tLoss: 0.5365 \n",
            "| Epoch [ 41/ 60] Iter[ 61/251]\t\tLoss: 0.0187 \n",
            "| Epoch [ 41/ 60] Iter[ 66/251]\t\tLoss: 0.3294 \n",
            "| Epoch [ 41/ 60] Iter[ 71/251]\t\tLoss: 0.2639 \n",
            "| Epoch [ 41/ 60] Iter[ 76/251]\t\tLoss: 0.1232 \n",
            "| Epoch [ 41/ 60] Iter[ 81/251]\t\tLoss: 0.5951 \n",
            "| Epoch [ 41/ 60] Iter[ 86/251]\t\tLoss: 0.2825 \n",
            "| Epoch [ 41/ 60] Iter[ 91/251]\t\tLoss: 0.6828 \n",
            "| Epoch [ 41/ 60] Iter[ 96/251]\t\tLoss: 0.4436 \n",
            "| Epoch [ 41/ 60] Iter[101/251]\t\tLoss: 0.4923 \n",
            "| Epoch [ 41/ 60] Iter[106/251]\t\tLoss: 6.1826 \n",
            "| Epoch [ 41/ 60] Iter[111/251]\t\tLoss: 6.3030 \n",
            "| Epoch [ 41/ 60] Iter[116/251]\t\tLoss: 0.0594 \n",
            "| Epoch [ 41/ 60] Iter[121/251]\t\tLoss: 0.2009 \n",
            "| Epoch [ 41/ 60] Iter[126/251]\t\tLoss: 6.1993 \n",
            "| Epoch [ 41/ 60] Iter[131/251]\t\tLoss: 2.0741 \n",
            "| Epoch [ 41/ 60] Iter[136/251]\t\tLoss: 0.3291 \n",
            "| Epoch [ 41/ 60] Iter[141/251]\t\tLoss: 2.5586 \n",
            "| Epoch [ 41/ 60] Iter[146/251]\t\tLoss: 0.0471 \n",
            "| Epoch [ 41/ 60] Iter[151/251]\t\tLoss: 0.2382 \n",
            "| Epoch [ 41/ 60] Iter[156/251]\t\tLoss: 0.6476 \n",
            "| Epoch [ 41/ 60] Iter[161/251]\t\tLoss: 0.0815 \n",
            "| Epoch [ 41/ 60] Iter[166/251]\t\tLoss: 0.1819 \n",
            "| Epoch [ 41/ 60] Iter[171/251]\t\tLoss: 0.4615 \n",
            "| Epoch [ 41/ 60] Iter[176/251]\t\tLoss: 6.2128 \n",
            "| Epoch [ 41/ 60] Iter[181/251]\t\tLoss: 0.6946 \n",
            "| Epoch [ 41/ 60] Iter[186/251]\t\tLoss: 1.3337 \n",
            "| Epoch [ 41/ 60] Iter[191/251]\t\tLoss: 0.2810 \n",
            "| Epoch [ 41/ 60] Iter[196/251]\t\tLoss: 3.9099 \n",
            "| Epoch [ 41/ 60] Iter[201/251]\t\tLoss: 0.5970 \n",
            "| Epoch [ 41/ 60] Iter[206/251]\t\tLoss: 3.0845 \n",
            "| Epoch [ 41/ 60] Iter[211/251]\t\tLoss: 0.1197 \n",
            "| Epoch [ 41/ 60] Iter[216/251]\t\tLoss: 0.6365 \n",
            "| Epoch [ 41/ 60] Iter[221/251]\t\tLoss: 0.8424 \n",
            "| Epoch [ 41/ 60] Iter[226/251]\t\tLoss: 0.7180 \n",
            "| Epoch [ 41/ 60] Iter[231/251]\t\tLoss: 0.0698 \n",
            "| Epoch [ 41/ 60] Iter[236/251]\t\tLoss: 0.1270 \n",
            "| Epoch [ 41/ 60] Iter[241/251]\t\tLoss: 0.0742 \n",
            "| Epoch [ 41/ 60] Iter[246/251]\t\tLoss: 0.5803 \n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 0.2199 \n",
            "\n",
            "=> Training Epoch #42, LR=0.0000\n",
            "| Epoch [ 42/ 60] Iter[  1/251]\t\tLoss: 6.2713 \n",
            "| Epoch [ 42/ 60] Iter[  6/251]\t\tLoss: 0.7537 \n",
            "| Epoch [ 42/ 60] Iter[ 11/251]\t\tLoss: 0.0742 \n",
            "| Epoch [ 42/ 60] Iter[ 16/251]\t\tLoss: 6.4830 \n",
            "| Epoch [ 42/ 60] Iter[ 21/251]\t\tLoss: 0.1512 \n",
            "| Epoch [ 42/ 60] Iter[ 26/251]\t\tLoss: 0.3498 \n",
            "| Epoch [ 42/ 60] Iter[ 31/251]\t\tLoss: 1.0497 \n",
            "| Epoch [ 42/ 60] Iter[ 36/251]\t\tLoss: 0.0346 \n",
            "| Epoch [ 42/ 60] Iter[ 41/251]\t\tLoss: 0.3994 \n",
            "| Epoch [ 42/ 60] Iter[ 46/251]\t\tLoss: 5.8162 \n",
            "| Epoch [ 42/ 60] Iter[ 51/251]\t\tLoss: 0.8078 \n",
            "| Epoch [ 42/ 60] Iter[ 56/251]\t\tLoss: 0.7576 \n",
            "| Epoch [ 42/ 60] Iter[ 61/251]\t\tLoss: 0.5935 \n",
            "| Epoch [ 42/ 60] Iter[ 66/251]\t\tLoss: 0.4312 \n",
            "| Epoch [ 42/ 60] Iter[ 71/251]\t\tLoss: 0.4484 \n",
            "| Epoch [ 42/ 60] Iter[ 76/251]\t\tLoss: 2.4643 \n",
            "| Epoch [ 42/ 60] Iter[ 81/251]\t\tLoss: 0.5021 \n",
            "| Epoch [ 42/ 60] Iter[ 86/251]\t\tLoss: 0.2539 \n",
            "| Epoch [ 42/ 60] Iter[ 91/251]\t\tLoss: 6.7325 \n",
            "| Epoch [ 42/ 60] Iter[ 96/251]\t\tLoss: 0.0178 \n",
            "| Epoch [ 42/ 60] Iter[101/251]\t\tLoss: 0.3687 \n",
            "| Epoch [ 42/ 60] Iter[106/251]\t\tLoss: 0.5895 \n",
            "| Epoch [ 42/ 60] Iter[111/251]\t\tLoss: 0.1307 \n",
            "| Epoch [ 42/ 60] Iter[116/251]\t\tLoss: 0.6124 \n",
            "| Epoch [ 42/ 60] Iter[121/251]\t\tLoss: 0.2311 \n",
            "| Epoch [ 42/ 60] Iter[126/251]\t\tLoss: 0.7505 \n",
            "| Epoch [ 42/ 60] Iter[131/251]\t\tLoss: 1.4332 \n",
            "| Epoch [ 42/ 60] Iter[136/251]\t\tLoss: 0.2611 \n",
            "| Epoch [ 42/ 60] Iter[141/251]\t\tLoss: 2.5529 \n",
            "| Epoch [ 42/ 60] Iter[146/251]\t\tLoss: 0.7287 \n",
            "| Epoch [ 42/ 60] Iter[151/251]\t\tLoss: 0.5919 \n",
            "| Epoch [ 42/ 60] Iter[156/251]\t\tLoss: 0.2459 \n",
            "| Epoch [ 42/ 60] Iter[161/251]\t\tLoss: 1.0878 \n",
            "| Epoch [ 42/ 60] Iter[166/251]\t\tLoss: 0.3967 \n",
            "| Epoch [ 42/ 60] Iter[171/251]\t\tLoss: 0.5803 \n",
            "| Epoch [ 42/ 60] Iter[176/251]\t\tLoss: 1.1122 \n",
            "| Epoch [ 42/ 60] Iter[181/251]\t\tLoss: 0.4257 \n",
            "| Epoch [ 42/ 60] Iter[186/251]\t\tLoss: 0.3960 \n",
            "| Epoch [ 42/ 60] Iter[191/251]\t\tLoss: 0.6229 \n",
            "| Epoch [ 42/ 60] Iter[196/251]\t\tLoss: 2.6556 \n",
            "| Epoch [ 42/ 60] Iter[201/251]\t\tLoss: 0.1360 \n",
            "| Epoch [ 42/ 60] Iter[206/251]\t\tLoss: 0.5315 \n",
            "| Epoch [ 42/ 60] Iter[211/251]\t\tLoss: 0.6820 \n",
            "| Epoch [ 42/ 60] Iter[216/251]\t\tLoss: 0.9186 \n",
            "| Epoch [ 42/ 60] Iter[221/251]\t\tLoss: 0.3066 \n",
            "| Epoch [ 42/ 60] Iter[226/251]\t\tLoss: 2.3089 \n",
            "| Epoch [ 42/ 60] Iter[231/251]\t\tLoss: 0.1799 \n",
            "| Epoch [ 42/ 60] Iter[236/251]\t\tLoss: 0.5512 \n",
            "| Epoch [ 42/ 60] Iter[241/251]\t\tLoss: 0.5343 \n",
            "| Epoch [ 42/ 60] Iter[246/251]\t\tLoss: 6.1051 \n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 0.2194 \n",
            "\n",
            "=> Training Epoch #43, LR=0.0000\n",
            "| Epoch [ 43/ 60] Iter[  1/251]\t\tLoss: 1.1907 \n",
            "| Epoch [ 43/ 60] Iter[  6/251]\t\tLoss: 0.1396 \n",
            "| Epoch [ 43/ 60] Iter[ 11/251]\t\tLoss: 0.7919 \n",
            "| Epoch [ 43/ 60] Iter[ 16/251]\t\tLoss: 0.1152 \n",
            "| Epoch [ 43/ 60] Iter[ 21/251]\t\tLoss: 0.5011 \n",
            "| Epoch [ 43/ 60] Iter[ 26/251]\t\tLoss: 6.5969 \n",
            "| Epoch [ 43/ 60] Iter[ 31/251]\t\tLoss: 0.1831 \n",
            "| Epoch [ 43/ 60] Iter[ 36/251]\t\tLoss: 0.9892 \n",
            "| Epoch [ 43/ 60] Iter[ 41/251]\t\tLoss: 0.3211 \n",
            "| Epoch [ 43/ 60] Iter[ 46/251]\t\tLoss: 6.1849 \n",
            "| Epoch [ 43/ 60] Iter[ 51/251]\t\tLoss: 0.0318 \n",
            "| Epoch [ 43/ 60] Iter[ 56/251]\t\tLoss: 0.2212 \n",
            "| Epoch [ 43/ 60] Iter[ 61/251]\t\tLoss: 0.4952 \n",
            "| Epoch [ 43/ 60] Iter[ 66/251]\t\tLoss: 0.7757 \n",
            "| Epoch [ 43/ 60] Iter[ 71/251]\t\tLoss: 0.5987 \n",
            "| Epoch [ 43/ 60] Iter[ 76/251]\t\tLoss: 0.1187 \n",
            "| Epoch [ 43/ 60] Iter[ 81/251]\t\tLoss: 0.0554 \n",
            "| Epoch [ 43/ 60] Iter[ 86/251]\t\tLoss: 2.5406 \n",
            "| Epoch [ 43/ 60] Iter[ 91/251]\t\tLoss: 0.3089 \n",
            "| Epoch [ 43/ 60] Iter[ 96/251]\t\tLoss: 0.6598 \n",
            "| Epoch [ 43/ 60] Iter[101/251]\t\tLoss: 0.9434 \n",
            "| Epoch [ 43/ 60] Iter[106/251]\t\tLoss: 1.1959 \n",
            "| Epoch [ 43/ 60] Iter[111/251]\t\tLoss: 0.4903 \n",
            "| Epoch [ 43/ 60] Iter[116/251]\t\tLoss: 1.0226 \n",
            "| Epoch [ 43/ 60] Iter[121/251]\t\tLoss: 0.0502 \n",
            "| Epoch [ 43/ 60] Iter[126/251]\t\tLoss: 0.0532 \n",
            "| Epoch [ 43/ 60] Iter[131/251]\t\tLoss: 0.1346 \n",
            "| Epoch [ 43/ 60] Iter[136/251]\t\tLoss: 0.5949 \n",
            "| Epoch [ 43/ 60] Iter[141/251]\t\tLoss: 0.1703 \n",
            "| Epoch [ 43/ 60] Iter[146/251]\t\tLoss: 0.2427 \n",
            "| Epoch [ 43/ 60] Iter[151/251]\t\tLoss: 6.0078 \n",
            "| Epoch [ 43/ 60] Iter[156/251]\t\tLoss: 0.7636 \n",
            "| Epoch [ 43/ 60] Iter[161/251]\t\tLoss: 0.3879 \n",
            "| Epoch [ 43/ 60] Iter[166/251]\t\tLoss: 0.1290 \n",
            "| Epoch [ 43/ 60] Iter[171/251]\t\tLoss: 0.5637 \n",
            "| Epoch [ 43/ 60] Iter[176/251]\t\tLoss: 0.2361 \n",
            "| Epoch [ 43/ 60] Iter[181/251]\t\tLoss: 6.1245 \n",
            "| Epoch [ 43/ 60] Iter[186/251]\t\tLoss: 0.2999 \n",
            "| Epoch [ 43/ 60] Iter[191/251]\t\tLoss: 0.1141 \n",
            "| Epoch [ 43/ 60] Iter[196/251]\t\tLoss: 0.2871 \n",
            "| Epoch [ 43/ 60] Iter[201/251]\t\tLoss: 0.6053 \n",
            "| Epoch [ 43/ 60] Iter[206/251]\t\tLoss: 0.1373 \n",
            "| Epoch [ 43/ 60] Iter[211/251]\t\tLoss: 0.3073 \n",
            "| Epoch [ 43/ 60] Iter[216/251]\t\tLoss: 0.7388 \n",
            "| Epoch [ 43/ 60] Iter[221/251]\t\tLoss: 0.3034 \n",
            "| Epoch [ 43/ 60] Iter[226/251]\t\tLoss: 0.6921 \n",
            "| Epoch [ 43/ 60] Iter[231/251]\t\tLoss: 1.5977 \n",
            "| Epoch [ 43/ 60] Iter[236/251]\t\tLoss: 0.0856 \n",
            "| Epoch [ 43/ 60] Iter[241/251]\t\tLoss: 0.2322 \n",
            "| Epoch [ 43/ 60] Iter[246/251]\t\tLoss: 0.2745 \n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 0.2199 \n",
            "\n",
            "=> Training Epoch #44, LR=0.0000\n",
            "| Epoch [ 44/ 60] Iter[  1/251]\t\tLoss: 0.8529 \n",
            "| Epoch [ 44/ 60] Iter[  6/251]\t\tLoss: 0.4627 \n",
            "| Epoch [ 44/ 60] Iter[ 11/251]\t\tLoss: 0.3350 \n",
            "| Epoch [ 44/ 60] Iter[ 16/251]\t\tLoss: 0.3488 \n",
            "| Epoch [ 44/ 60] Iter[ 21/251]\t\tLoss: 6.1414 \n",
            "| Epoch [ 44/ 60] Iter[ 26/251]\t\tLoss: 0.6798 \n",
            "| Epoch [ 44/ 60] Iter[ 31/251]\t\tLoss: 0.4399 \n",
            "| Epoch [ 44/ 60] Iter[ 36/251]\t\tLoss: 0.7611 \n",
            "| Epoch [ 44/ 60] Iter[ 41/251]\t\tLoss: 0.2801 \n",
            "| Epoch [ 44/ 60] Iter[ 46/251]\t\tLoss: 1.3032 \n",
            "| Epoch [ 44/ 60] Iter[ 51/251]\t\tLoss: 0.0918 \n",
            "| Epoch [ 44/ 60] Iter[ 56/251]\t\tLoss: 0.0716 \n",
            "| Epoch [ 44/ 60] Iter[ 61/251]\t\tLoss: 0.3130 \n",
            "| Epoch [ 44/ 60] Iter[ 66/251]\t\tLoss: 0.1367 \n",
            "| Epoch [ 44/ 60] Iter[ 71/251]\t\tLoss: 0.2712 \n",
            "| Epoch [ 44/ 60] Iter[ 76/251]\t\tLoss: 0.3417 \n",
            "| Epoch [ 44/ 60] Iter[ 81/251]\t\tLoss: 0.5548 \n",
            "| Epoch [ 44/ 60] Iter[ 86/251]\t\tLoss: 0.4240 \n",
            "| Epoch [ 44/ 60] Iter[ 91/251]\t\tLoss: 0.3877 \n",
            "| Epoch [ 44/ 60] Iter[ 96/251]\t\tLoss: 0.1465 \n",
            "| Epoch [ 44/ 60] Iter[101/251]\t\tLoss: 0.5510 \n",
            "| Epoch [ 44/ 60] Iter[106/251]\t\tLoss: 5.9965 \n",
            "| Epoch [ 44/ 60] Iter[111/251]\t\tLoss: 0.3044 \n",
            "| Epoch [ 44/ 60] Iter[116/251]\t\tLoss: 0.0810 \n",
            "| Epoch [ 44/ 60] Iter[121/251]\t\tLoss: 0.3001 \n",
            "| Epoch [ 44/ 60] Iter[126/251]\t\tLoss: 0.4218 \n",
            "| Epoch [ 44/ 60] Iter[131/251]\t\tLoss: 0.4830 \n",
            "| Epoch [ 44/ 60] Iter[136/251]\t\tLoss: 0.4693 \n",
            "| Epoch [ 44/ 60] Iter[141/251]\t\tLoss: 0.4491 \n",
            "| Epoch [ 44/ 60] Iter[146/251]\t\tLoss: 6.2762 \n",
            "| Epoch [ 44/ 60] Iter[151/251]\t\tLoss: 0.2978 \n",
            "| Epoch [ 44/ 60] Iter[156/251]\t\tLoss: 0.1440 \n",
            "| Epoch [ 44/ 60] Iter[161/251]\t\tLoss: 0.5463 \n",
            "| Epoch [ 44/ 60] Iter[166/251]\t\tLoss: 1.3400 \n",
            "| Epoch [ 44/ 60] Iter[171/251]\t\tLoss: 5.5945 \n",
            "| Epoch [ 44/ 60] Iter[176/251]\t\tLoss: 0.5590 \n",
            "| Epoch [ 44/ 60] Iter[181/251]\t\tLoss: 0.1049 \n",
            "| Epoch [ 44/ 60] Iter[186/251]\t\tLoss: 0.4360 \n",
            "| Epoch [ 44/ 60] Iter[191/251]\t\tLoss: 0.2527 \n",
            "| Epoch [ 44/ 60] Iter[196/251]\t\tLoss: 0.8807 \n",
            "| Epoch [ 44/ 60] Iter[201/251]\t\tLoss: 0.5502 \n",
            "| Epoch [ 44/ 60] Iter[206/251]\t\tLoss: 0.4201 \n",
            "| Epoch [ 44/ 60] Iter[211/251]\t\tLoss: 0.9618 \n",
            "| Epoch [ 44/ 60] Iter[216/251]\t\tLoss: 0.7413 \n",
            "| Epoch [ 44/ 60] Iter[221/251]\t\tLoss: 0.8357 \n",
            "| Epoch [ 44/ 60] Iter[226/251]\t\tLoss: 0.1164 \n",
            "| Epoch [ 44/ 60] Iter[231/251]\t\tLoss: 6.0151 \n",
            "| Epoch [ 44/ 60] Iter[236/251]\t\tLoss: 0.6451 \n",
            "| Epoch [ 44/ 60] Iter[241/251]\t\tLoss: 0.2105 \n",
            "| Epoch [ 44/ 60] Iter[246/251]\t\tLoss: 1.0058 \n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 0.2199 \n",
            "\n",
            "=> Training Epoch #45, LR=0.0000\n",
            "| Epoch [ 45/ 60] Iter[  1/251]\t\tLoss: 0.5680 \n",
            "| Epoch [ 45/ 60] Iter[  6/251]\t\tLoss: 1.0202 \n",
            "| Epoch [ 45/ 60] Iter[ 11/251]\t\tLoss: 5.9131 \n",
            "| Epoch [ 45/ 60] Iter[ 16/251]\t\tLoss: 0.1236 \n",
            "| Epoch [ 45/ 60] Iter[ 21/251]\t\tLoss: 0.7330 \n",
            "| Epoch [ 45/ 60] Iter[ 26/251]\t\tLoss: 0.7158 \n",
            "| Epoch [ 45/ 60] Iter[ 31/251]\t\tLoss: 0.3546 \n",
            "| Epoch [ 45/ 60] Iter[ 36/251]\t\tLoss: 0.0207 \n",
            "| Epoch [ 45/ 60] Iter[ 41/251]\t\tLoss: 0.2447 \n",
            "| Epoch [ 45/ 60] Iter[ 46/251]\t\tLoss: 0.5221 \n",
            "| Epoch [ 45/ 60] Iter[ 51/251]\t\tLoss: 0.8062 \n",
            "| Epoch [ 45/ 60] Iter[ 56/251]\t\tLoss: 0.3908 \n",
            "| Epoch [ 45/ 60] Iter[ 61/251]\t\tLoss: 1.1762 \n",
            "| Epoch [ 45/ 60] Iter[ 66/251]\t\tLoss: 0.6856 \n",
            "| Epoch [ 45/ 60] Iter[ 71/251]\t\tLoss: 5.8615 \n",
            "| Epoch [ 45/ 60] Iter[ 76/251]\t\tLoss: 0.5368 \n",
            "| Epoch [ 45/ 60] Iter[ 81/251]\t\tLoss: 0.0999 \n",
            "| Epoch [ 45/ 60] Iter[ 86/251]\t\tLoss: 0.7620 \n",
            "| Epoch [ 45/ 60] Iter[ 91/251]\t\tLoss: 2.2090 \n",
            "| Epoch [ 45/ 60] Iter[ 96/251]\t\tLoss: 0.5060 \n",
            "| Epoch [ 45/ 60] Iter[101/251]\t\tLoss: 0.8554 \n",
            "| Epoch [ 45/ 60] Iter[106/251]\t\tLoss: 0.1347 \n",
            "| Epoch [ 45/ 60] Iter[111/251]\t\tLoss: 0.6480 \n",
            "| Epoch [ 45/ 60] Iter[116/251]\t\tLoss: 1.0340 \n",
            "| Epoch [ 45/ 60] Iter[121/251]\t\tLoss: 0.0717 \n",
            "| Epoch [ 45/ 60] Iter[126/251]\t\tLoss: 0.3495 \n",
            "| Epoch [ 45/ 60] Iter[131/251]\t\tLoss: 0.2034 \n",
            "| Epoch [ 45/ 60] Iter[136/251]\t\tLoss: 0.8239 \n",
            "| Epoch [ 45/ 60] Iter[141/251]\t\tLoss: 0.1369 \n",
            "| Epoch [ 45/ 60] Iter[146/251]\t\tLoss: 0.5104 \n",
            "| Epoch [ 45/ 60] Iter[151/251]\t\tLoss: 3.8958 \n",
            "| Epoch [ 45/ 60] Iter[156/251]\t\tLoss: 0.5440 \n",
            "| Epoch [ 45/ 60] Iter[161/251]\t\tLoss: 6.4901 \n",
            "| Epoch [ 45/ 60] Iter[166/251]\t\tLoss: 6.1755 \n",
            "| Epoch [ 45/ 60] Iter[171/251]\t\tLoss: 0.4585 \n",
            "| Epoch [ 45/ 60] Iter[176/251]\t\tLoss: 0.5524 \n",
            "| Epoch [ 45/ 60] Iter[181/251]\t\tLoss: 0.3747 \n",
            "| Epoch [ 45/ 60] Iter[186/251]\t\tLoss: 2.5941 \n",
            "| Epoch [ 45/ 60] Iter[191/251]\t\tLoss: 0.2743 \n",
            "| Epoch [ 45/ 60] Iter[196/251]\t\tLoss: 0.4271 \n",
            "| Epoch [ 45/ 60] Iter[201/251]\t\tLoss: 0.1450 \n",
            "| Epoch [ 45/ 60] Iter[206/251]\t\tLoss: 5.8206 \n",
            "| Epoch [ 45/ 60] Iter[211/251]\t\tLoss: 1.0667 \n",
            "| Epoch [ 45/ 60] Iter[216/251]\t\tLoss: 0.5988 \n",
            "| Epoch [ 45/ 60] Iter[221/251]\t\tLoss: 0.4841 \n",
            "| Epoch [ 45/ 60] Iter[226/251]\t\tLoss: 0.4785 \n",
            "| Epoch [ 45/ 60] Iter[231/251]\t\tLoss: 0.9094 \n",
            "| Epoch [ 45/ 60] Iter[236/251]\t\tLoss: 0.7229 \n",
            "| Epoch [ 45/ 60] Iter[241/251]\t\tLoss: 0.7212 \n",
            "| Epoch [ 45/ 60] Iter[246/251]\t\tLoss: 0.6611 \n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 0.2195 \n",
            "\n",
            "=> Training Epoch #46, LR=0.0000\n",
            "| Epoch [ 46/ 60] Iter[  1/251]\t\tLoss: 1.4178 \n",
            "| Epoch [ 46/ 60] Iter[  6/251]\t\tLoss: 0.7100 \n",
            "| Epoch [ 46/ 60] Iter[ 11/251]\t\tLoss: 0.0519 \n",
            "| Epoch [ 46/ 60] Iter[ 16/251]\t\tLoss: 0.8826 \n",
            "| Epoch [ 46/ 60] Iter[ 21/251]\t\tLoss: 0.0416 \n",
            "| Epoch [ 46/ 60] Iter[ 26/251]\t\tLoss: 0.1339 \n",
            "| Epoch [ 46/ 60] Iter[ 31/251]\t\tLoss: 0.6054 \n",
            "| Epoch [ 46/ 60] Iter[ 36/251]\t\tLoss: 0.2253 \n",
            "| Epoch [ 46/ 60] Iter[ 41/251]\t\tLoss: 0.8301 \n",
            "| Epoch [ 46/ 60] Iter[ 46/251]\t\tLoss: 0.8345 \n",
            "| Epoch [ 46/ 60] Iter[ 51/251]\t\tLoss: 0.4562 \n",
            "| Epoch [ 46/ 60] Iter[ 56/251]\t\tLoss: 0.1767 \n",
            "| Epoch [ 46/ 60] Iter[ 61/251]\t\tLoss: 0.4436 \n",
            "| Epoch [ 46/ 60] Iter[ 66/251]\t\tLoss: 6.4055 \n",
            "| Epoch [ 46/ 60] Iter[ 71/251]\t\tLoss: 0.4441 \n",
            "| Epoch [ 46/ 60] Iter[ 76/251]\t\tLoss: 0.4274 \n",
            "| Epoch [ 46/ 60] Iter[ 81/251]\t\tLoss: 5.9079 \n",
            "| Epoch [ 46/ 60] Iter[ 86/251]\t\tLoss: 0.7305 \n",
            "| Epoch [ 46/ 60] Iter[ 91/251]\t\tLoss: 0.4227 \n",
            "| Epoch [ 46/ 60] Iter[ 96/251]\t\tLoss: 0.3647 \n",
            "| Epoch [ 46/ 60] Iter[101/251]\t\tLoss: 0.3032 \n",
            "| Epoch [ 46/ 60] Iter[106/251]\t\tLoss: 6.4763 \n",
            "| Epoch [ 46/ 60] Iter[111/251]\t\tLoss: 0.8911 \n",
            "| Epoch [ 46/ 60] Iter[116/251]\t\tLoss: 0.1088 \n",
            "| Epoch [ 46/ 60] Iter[121/251]\t\tLoss: 0.9808 \n",
            "| Epoch [ 46/ 60] Iter[126/251]\t\tLoss: 0.2014 \n",
            "| Epoch [ 46/ 60] Iter[131/251]\t\tLoss: 0.4898 \n",
            "| Epoch [ 46/ 60] Iter[136/251]\t\tLoss: 0.5037 \n",
            "| Epoch [ 46/ 60] Iter[141/251]\t\tLoss: 0.0093 \n",
            "| Epoch [ 46/ 60] Iter[146/251]\t\tLoss: 0.6866 \n",
            "| Epoch [ 46/ 60] Iter[151/251]\t\tLoss: 6.3597 \n",
            "| Epoch [ 46/ 60] Iter[156/251]\t\tLoss: 0.5285 \n",
            "| Epoch [ 46/ 60] Iter[161/251]\t\tLoss: 1.1461 \n",
            "| Epoch [ 46/ 60] Iter[166/251]\t\tLoss: 0.6176 \n",
            "| Epoch [ 46/ 60] Iter[171/251]\t\tLoss: 0.1586 \n",
            "| Epoch [ 46/ 60] Iter[176/251]\t\tLoss: 6.7538 \n",
            "| Epoch [ 46/ 60] Iter[181/251]\t\tLoss: 0.4531 \n",
            "| Epoch [ 46/ 60] Iter[186/251]\t\tLoss: 0.7373 \n",
            "| Epoch [ 46/ 60] Iter[191/251]\t\tLoss: 0.3878 \n",
            "| Epoch [ 46/ 60] Iter[196/251]\t\tLoss: 0.1805 \n",
            "| Epoch [ 46/ 60] Iter[201/251]\t\tLoss: 0.2657 \n",
            "| Epoch [ 46/ 60] Iter[206/251]\t\tLoss: 0.4148 \n",
            "| Epoch [ 46/ 60] Iter[211/251]\t\tLoss: 0.5771 \n",
            "| Epoch [ 46/ 60] Iter[216/251]\t\tLoss: 0.9895 \n",
            "| Epoch [ 46/ 60] Iter[221/251]\t\tLoss: 0.4628 \n",
            "| Epoch [ 46/ 60] Iter[226/251]\t\tLoss: 0.2653 \n",
            "| Epoch [ 46/ 60] Iter[231/251]\t\tLoss: 0.9281 \n",
            "| Epoch [ 46/ 60] Iter[236/251]\t\tLoss: 0.0930 \n",
            "| Epoch [ 46/ 60] Iter[241/251]\t\tLoss: 1.2399 \n",
            "| Epoch [ 46/ 60] Iter[246/251]\t\tLoss: 0.2892 \n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 0.2197 \n",
            "\n",
            "=> Training Epoch #47, LR=0.0000\n",
            "| Epoch [ 47/ 60] Iter[  1/251]\t\tLoss: 0.6167 \n",
            "| Epoch [ 47/ 60] Iter[  6/251]\t\tLoss: 0.0880 \n",
            "| Epoch [ 47/ 60] Iter[ 11/251]\t\tLoss: 0.5438 \n",
            "| Epoch [ 47/ 60] Iter[ 16/251]\t\tLoss: 0.1951 \n",
            "| Epoch [ 47/ 60] Iter[ 21/251]\t\tLoss: 0.0874 \n",
            "| Epoch [ 47/ 60] Iter[ 26/251]\t\tLoss: 0.5205 \n",
            "| Epoch [ 47/ 60] Iter[ 31/251]\t\tLoss: 6.0353 \n",
            "| Epoch [ 47/ 60] Iter[ 36/251]\t\tLoss: 1.1938 \n",
            "| Epoch [ 47/ 60] Iter[ 41/251]\t\tLoss: 0.1388 \n",
            "| Epoch [ 47/ 60] Iter[ 46/251]\t\tLoss: 0.2409 \n",
            "| Epoch [ 47/ 60] Iter[ 51/251]\t\tLoss: 0.1678 \n",
            "| Epoch [ 47/ 60] Iter[ 56/251]\t\tLoss: 0.1523 \n",
            "| Epoch [ 47/ 60] Iter[ 61/251]\t\tLoss: 0.8549 \n",
            "| Epoch [ 47/ 60] Iter[ 66/251]\t\tLoss: 0.0623 \n",
            "| Epoch [ 47/ 60] Iter[ 71/251]\t\tLoss: 0.2190 \n",
            "| Epoch [ 47/ 60] Iter[ 76/251]\t\tLoss: 6.4581 \n",
            "| Epoch [ 47/ 60] Iter[ 81/251]\t\tLoss: 1.1685 \n",
            "| Epoch [ 47/ 60] Iter[ 86/251]\t\tLoss: 0.1115 \n",
            "| Epoch [ 47/ 60] Iter[ 91/251]\t\tLoss: 0.1858 \n",
            "| Epoch [ 47/ 60] Iter[ 96/251]\t\tLoss: 0.5997 \n",
            "| Epoch [ 47/ 60] Iter[101/251]\t\tLoss: 6.3749 \n",
            "| Epoch [ 47/ 60] Iter[106/251]\t\tLoss: 0.3365 \n",
            "| Epoch [ 47/ 60] Iter[111/251]\t\tLoss: 0.6518 \n",
            "| Epoch [ 47/ 60] Iter[116/251]\t\tLoss: 0.3996 \n",
            "| Epoch [ 47/ 60] Iter[121/251]\t\tLoss: 6.1564 \n",
            "| Epoch [ 47/ 60] Iter[126/251]\t\tLoss: 0.4897 \n",
            "| Epoch [ 47/ 60] Iter[131/251]\t\tLoss: 0.5520 \n",
            "| Epoch [ 47/ 60] Iter[136/251]\t\tLoss: 0.3357 \n",
            "| Epoch [ 47/ 60] Iter[141/251]\t\tLoss: 0.0104 \n",
            "| Epoch [ 47/ 60] Iter[146/251]\t\tLoss: 0.1395 \n",
            "| Epoch [ 47/ 60] Iter[151/251]\t\tLoss: 0.2332 \n",
            "| Epoch [ 47/ 60] Iter[156/251]\t\tLoss: 0.1901 \n",
            "| Epoch [ 47/ 60] Iter[161/251]\t\tLoss: 0.7357 \n",
            "| Epoch [ 47/ 60] Iter[166/251]\t\tLoss: 0.7142 \n",
            "| Epoch [ 47/ 60] Iter[171/251]\t\tLoss: 0.3416 \n",
            "| Epoch [ 47/ 60] Iter[176/251]\t\tLoss: 5.6027 \n",
            "| Epoch [ 47/ 60] Iter[181/251]\t\tLoss: 0.4943 \n",
            "| Epoch [ 47/ 60] Iter[186/251]\t\tLoss: 0.5210 \n",
            "| Epoch [ 47/ 60] Iter[191/251]\t\tLoss: 0.4546 \n",
            "| Epoch [ 47/ 60] Iter[196/251]\t\tLoss: 0.6430 \n",
            "| Epoch [ 47/ 60] Iter[201/251]\t\tLoss: 0.6423 \n",
            "| Epoch [ 47/ 60] Iter[206/251]\t\tLoss: 0.3014 \n",
            "| Epoch [ 47/ 60] Iter[211/251]\t\tLoss: 0.1123 \n",
            "| Epoch [ 47/ 60] Iter[216/251]\t\tLoss: 5.8693 \n",
            "| Epoch [ 47/ 60] Iter[221/251]\t\tLoss: 0.3572 \n",
            "| Epoch [ 47/ 60] Iter[226/251]\t\tLoss: 0.3169 \n",
            "| Epoch [ 47/ 60] Iter[231/251]\t\tLoss: 1.8883 \n",
            "| Epoch [ 47/ 60] Iter[236/251]\t\tLoss: 0.5150 \n",
            "| Epoch [ 47/ 60] Iter[241/251]\t\tLoss: 0.3423 \n",
            "| Epoch [ 47/ 60] Iter[246/251]\t\tLoss: 0.4879 \n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 0.2201 \n",
            "\n",
            "=> Training Epoch #48, LR=0.0000\n",
            "| Epoch [ 48/ 60] Iter[  1/251]\t\tLoss: 0.7852 \n",
            "| Epoch [ 48/ 60] Iter[  6/251]\t\tLoss: 0.3843 \n",
            "| Epoch [ 48/ 60] Iter[ 11/251]\t\tLoss: 0.4058 \n",
            "| Epoch [ 48/ 60] Iter[ 16/251]\t\tLoss: 0.1204 \n",
            "| Epoch [ 48/ 60] Iter[ 21/251]\t\tLoss: 6.2733 \n",
            "| Epoch [ 48/ 60] Iter[ 26/251]\t\tLoss: 0.3866 \n",
            "| Epoch [ 48/ 60] Iter[ 31/251]\t\tLoss: 5.9962 \n",
            "| Epoch [ 48/ 60] Iter[ 36/251]\t\tLoss: 0.0980 \n",
            "| Epoch [ 48/ 60] Iter[ 41/251]\t\tLoss: 6.1240 \n",
            "| Epoch [ 48/ 60] Iter[ 46/251]\t\tLoss: 0.6180 \n",
            "| Epoch [ 48/ 60] Iter[ 51/251]\t\tLoss: 0.9582 \n",
            "| Epoch [ 48/ 60] Iter[ 56/251]\t\tLoss: 0.6275 \n",
            "| Epoch [ 48/ 60] Iter[ 61/251]\t\tLoss: 0.1904 \n",
            "| Epoch [ 48/ 60] Iter[ 66/251]\t\tLoss: 0.1954 \n",
            "| Epoch [ 48/ 60] Iter[ 71/251]\t\tLoss: 0.4034 \n",
            "| Epoch [ 48/ 60] Iter[ 76/251]\t\tLoss: 0.1675 \n",
            "| Epoch [ 48/ 60] Iter[ 81/251]\t\tLoss: 0.3389 \n",
            "| Epoch [ 48/ 60] Iter[ 86/251]\t\tLoss: 0.3797 \n",
            "| Epoch [ 48/ 60] Iter[ 91/251]\t\tLoss: 0.8383 \n",
            "| Epoch [ 48/ 60] Iter[ 96/251]\t\tLoss: 0.0581 \n",
            "| Epoch [ 48/ 60] Iter[101/251]\t\tLoss: 0.6616 \n",
            "| Epoch [ 48/ 60] Iter[106/251]\t\tLoss: 0.1552 \n",
            "| Epoch [ 48/ 60] Iter[111/251]\t\tLoss: 1.9034 \n",
            "| Epoch [ 48/ 60] Iter[116/251]\t\tLoss: 6.2740 \n",
            "| Epoch [ 48/ 60] Iter[121/251]\t\tLoss: 0.4509 \n",
            "| Epoch [ 48/ 60] Iter[126/251]\t\tLoss: 0.3886 \n",
            "| Epoch [ 48/ 60] Iter[131/251]\t\tLoss: 0.3416 \n",
            "| Epoch [ 48/ 60] Iter[136/251]\t\tLoss: 1.2769 \n",
            "| Epoch [ 48/ 60] Iter[141/251]\t\tLoss: 0.2753 \n",
            "| Epoch [ 48/ 60] Iter[146/251]\t\tLoss: 0.0815 \n",
            "| Epoch [ 48/ 60] Iter[151/251]\t\tLoss: 0.1169 \n",
            "| Epoch [ 48/ 60] Iter[156/251]\t\tLoss: 2.2453 \n",
            "| Epoch [ 48/ 60] Iter[161/251]\t\tLoss: 0.7353 \n",
            "| Epoch [ 48/ 60] Iter[166/251]\t\tLoss: 0.8662 \n",
            "| Epoch [ 48/ 60] Iter[171/251]\t\tLoss: 0.2473 \n",
            "| Epoch [ 48/ 60] Iter[176/251]\t\tLoss: 0.1752 \n",
            "| Epoch [ 48/ 60] Iter[181/251]\t\tLoss: 6.7401 \n",
            "| Epoch [ 48/ 60] Iter[186/251]\t\tLoss: 0.5052 \n",
            "| Epoch [ 48/ 60] Iter[191/251]\t\tLoss: 0.2861 \n",
            "| Epoch [ 48/ 60] Iter[196/251]\t\tLoss: 2.5550 \n",
            "| Epoch [ 48/ 60] Iter[201/251]\t\tLoss: 6.2751 \n",
            "| Epoch [ 48/ 60] Iter[206/251]\t\tLoss: 0.3218 \n",
            "| Epoch [ 48/ 60] Iter[211/251]\t\tLoss: 5.8814 \n",
            "| Epoch [ 48/ 60] Iter[216/251]\t\tLoss: 0.4042 \n",
            "| Epoch [ 48/ 60] Iter[221/251]\t\tLoss: 0.7553 \n",
            "| Epoch [ 48/ 60] Iter[226/251]\t\tLoss: 0.2248 \n",
            "| Epoch [ 48/ 60] Iter[231/251]\t\tLoss: 0.4632 \n",
            "| Epoch [ 48/ 60] Iter[236/251]\t\tLoss: 0.7496 \n",
            "| Epoch [ 48/ 60] Iter[241/251]\t\tLoss: 0.5113 \n",
            "| Epoch [ 48/ 60] Iter[246/251]\t\tLoss: 1.8207 \n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 0.2201 \n",
            "\n",
            "=> Training Epoch #49, LR=0.0000\n",
            "| Epoch [ 49/ 60] Iter[  1/251]\t\tLoss: 0.0897 \n",
            "| Epoch [ 49/ 60] Iter[  6/251]\t\tLoss: 0.1463 \n",
            "| Epoch [ 49/ 60] Iter[ 11/251]\t\tLoss: 0.4659 \n",
            "| Epoch [ 49/ 60] Iter[ 16/251]\t\tLoss: 0.6450 \n",
            "| Epoch [ 49/ 60] Iter[ 21/251]\t\tLoss: 0.7309 \n",
            "| Epoch [ 49/ 60] Iter[ 26/251]\t\tLoss: 0.4163 \n",
            "| Epoch [ 49/ 60] Iter[ 31/251]\t\tLoss: 0.7390 \n",
            "| Epoch [ 49/ 60] Iter[ 36/251]\t\tLoss: 0.2044 \n",
            "| Epoch [ 49/ 60] Iter[ 41/251]\t\tLoss: 0.4772 \n",
            "| Epoch [ 49/ 60] Iter[ 46/251]\t\tLoss: 0.3560 \n",
            "| Epoch [ 49/ 60] Iter[ 51/251]\t\tLoss: 0.3661 \n",
            "| Epoch [ 49/ 60] Iter[ 56/251]\t\tLoss: 0.4402 \n",
            "| Epoch [ 49/ 60] Iter[ 61/251]\t\tLoss: 0.7337 \n",
            "| Epoch [ 49/ 60] Iter[ 66/251]\t\tLoss: 0.7390 \n",
            "| Epoch [ 49/ 60] Iter[ 71/251]\t\tLoss: 0.5675 \n",
            "| Epoch [ 49/ 60] Iter[ 76/251]\t\tLoss: 0.6972 \n",
            "| Epoch [ 49/ 60] Iter[ 81/251]\t\tLoss: 0.8345 \n",
            "| Epoch [ 49/ 60] Iter[ 86/251]\t\tLoss: 0.2912 \n",
            "| Epoch [ 49/ 60] Iter[ 91/251]\t\tLoss: 1.0995 \n",
            "| Epoch [ 49/ 60] Iter[ 96/251]\t\tLoss: 0.2141 \n",
            "| Epoch [ 49/ 60] Iter[101/251]\t\tLoss: 0.5662 \n",
            "| Epoch [ 49/ 60] Iter[106/251]\t\tLoss: 0.4064 \n",
            "| Epoch [ 49/ 60] Iter[111/251]\t\tLoss: 0.1838 \n",
            "| Epoch [ 49/ 60] Iter[116/251]\t\tLoss: 0.2630 \n",
            "| Epoch [ 49/ 60] Iter[121/251]\t\tLoss: 6.3256 \n",
            "| Epoch [ 49/ 60] Iter[126/251]\t\tLoss: 0.8576 \n",
            "| Epoch [ 49/ 60] Iter[131/251]\t\tLoss: 0.2751 \n",
            "| Epoch [ 49/ 60] Iter[136/251]\t\tLoss: 0.1456 \n",
            "| Epoch [ 49/ 60] Iter[141/251]\t\tLoss: 0.5195 \n",
            "| Epoch [ 49/ 60] Iter[146/251]\t\tLoss: 0.3463 \n",
            "| Epoch [ 49/ 60] Iter[151/251]\t\tLoss: 0.5668 \n",
            "| Epoch [ 49/ 60] Iter[156/251]\t\tLoss: 0.4439 \n",
            "| Epoch [ 49/ 60] Iter[161/251]\t\tLoss: 0.5437 \n",
            "| Epoch [ 49/ 60] Iter[166/251]\t\tLoss: 0.3154 \n",
            "| Epoch [ 49/ 60] Iter[171/251]\t\tLoss: 0.5952 \n",
            "| Epoch [ 49/ 60] Iter[176/251]\t\tLoss: 0.6298 \n",
            "| Epoch [ 49/ 60] Iter[181/251]\t\tLoss: 0.4086 \n",
            "| Epoch [ 49/ 60] Iter[186/251]\t\tLoss: 0.1690 \n",
            "| Epoch [ 49/ 60] Iter[191/251]\t\tLoss: 0.2957 \n",
            "| Epoch [ 49/ 60] Iter[196/251]\t\tLoss: 0.1431 \n",
            "| Epoch [ 49/ 60] Iter[201/251]\t\tLoss: 0.9167 \n",
            "| Epoch [ 49/ 60] Iter[206/251]\t\tLoss: 0.6655 \n",
            "| Epoch [ 49/ 60] Iter[211/251]\t\tLoss: 0.6494 \n",
            "| Epoch [ 49/ 60] Iter[216/251]\t\tLoss: 6.8029 \n",
            "| Epoch [ 49/ 60] Iter[221/251]\t\tLoss: 0.9796 \n",
            "| Epoch [ 49/ 60] Iter[226/251]\t\tLoss: 0.1433 \n",
            "| Epoch [ 49/ 60] Iter[231/251]\t\tLoss: 0.4012 \n",
            "| Epoch [ 49/ 60] Iter[236/251]\t\tLoss: 0.7867 \n",
            "| Epoch [ 49/ 60] Iter[241/251]\t\tLoss: 0.7313 \n",
            "| Epoch [ 49/ 60] Iter[246/251]\t\tLoss: 0.3699 \n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 0.2203 \n",
            "\n",
            "=> Training Epoch #50, LR=0.0000\n",
            "| Epoch [ 50/ 60] Iter[  1/251]\t\tLoss: 0.7085 \n",
            "| Epoch [ 50/ 60] Iter[  6/251]\t\tLoss: 0.2681 \n",
            "| Epoch [ 50/ 60] Iter[ 11/251]\t\tLoss: 6.6836 \n",
            "| Epoch [ 50/ 60] Iter[ 16/251]\t\tLoss: 0.4126 \n",
            "| Epoch [ 50/ 60] Iter[ 21/251]\t\tLoss: 0.1961 \n",
            "| Epoch [ 50/ 60] Iter[ 26/251]\t\tLoss: 0.8521 \n",
            "| Epoch [ 50/ 60] Iter[ 31/251]\t\tLoss: 6.5230 \n",
            "| Epoch [ 50/ 60] Iter[ 36/251]\t\tLoss: 6.1508 \n",
            "| Epoch [ 50/ 60] Iter[ 41/251]\t\tLoss: 0.3079 \n",
            "| Epoch [ 50/ 60] Iter[ 46/251]\t\tLoss: 0.4384 \n",
            "| Epoch [ 50/ 60] Iter[ 51/251]\t\tLoss: 0.7145 \n",
            "| Epoch [ 50/ 60] Iter[ 56/251]\t\tLoss: 0.1879 \n",
            "| Epoch [ 50/ 60] Iter[ 61/251]\t\tLoss: 0.5296 \n",
            "| Epoch [ 50/ 60] Iter[ 66/251]\t\tLoss: 0.4125 \n",
            "| Epoch [ 50/ 60] Iter[ 71/251]\t\tLoss: 0.0946 \n",
            "| Epoch [ 50/ 60] Iter[ 76/251]\t\tLoss: 1.6589 \n",
            "| Epoch [ 50/ 60] Iter[ 81/251]\t\tLoss: 0.4923 \n",
            "| Epoch [ 50/ 60] Iter[ 86/251]\t\tLoss: 0.2127 \n",
            "| Epoch [ 50/ 60] Iter[ 91/251]\t\tLoss: 0.5277 \n",
            "| Epoch [ 50/ 60] Iter[ 96/251]\t\tLoss: 0.1247 \n",
            "| Epoch [ 50/ 60] Iter[101/251]\t\tLoss: 0.3031 \n",
            "| Epoch [ 50/ 60] Iter[106/251]\t\tLoss: 0.5638 \n",
            "| Epoch [ 50/ 60] Iter[111/251]\t\tLoss: 0.4817 \n",
            "| Epoch [ 50/ 60] Iter[116/251]\t\tLoss: 0.4875 \n",
            "| Epoch [ 50/ 60] Iter[121/251]\t\tLoss: 0.2153 \n",
            "| Epoch [ 50/ 60] Iter[126/251]\t\tLoss: 0.2540 \n",
            "| Epoch [ 50/ 60] Iter[131/251]\t\tLoss: 1.2312 \n",
            "| Epoch [ 50/ 60] Iter[136/251]\t\tLoss: 0.3099 \n",
            "| Epoch [ 50/ 60] Iter[141/251]\t\tLoss: 0.2420 \n",
            "| Epoch [ 50/ 60] Iter[146/251]\t\tLoss: 1.0609 \n",
            "| Epoch [ 50/ 60] Iter[151/251]\t\tLoss: 0.2071 \n",
            "| Epoch [ 50/ 60] Iter[156/251]\t\tLoss: 0.4977 \n",
            "| Epoch [ 50/ 60] Iter[161/251]\t\tLoss: 1.3151 \n",
            "| Epoch [ 50/ 60] Iter[166/251]\t\tLoss: 0.2858 \n",
            "| Epoch [ 50/ 60] Iter[171/251]\t\tLoss: 0.4255 \n",
            "| Epoch [ 50/ 60] Iter[176/251]\t\tLoss: 0.4864 \n",
            "| Epoch [ 50/ 60] Iter[181/251]\t\tLoss: 0.7725 \n",
            "| Epoch [ 50/ 60] Iter[186/251]\t\tLoss: 0.3833 \n",
            "| Epoch [ 50/ 60] Iter[191/251]\t\tLoss: 0.1334 \n",
            "| Epoch [ 50/ 60] Iter[196/251]\t\tLoss: 0.6579 \n",
            "| Epoch [ 50/ 60] Iter[201/251]\t\tLoss: 0.1060 \n",
            "| Epoch [ 50/ 60] Iter[206/251]\t\tLoss: 0.6053 \n",
            "| Epoch [ 50/ 60] Iter[211/251]\t\tLoss: 0.7402 \n",
            "| Epoch [ 50/ 60] Iter[216/251]\t\tLoss: 0.1373 \n",
            "| Epoch [ 50/ 60] Iter[221/251]\t\tLoss: 0.6896 \n",
            "| Epoch [ 50/ 60] Iter[226/251]\t\tLoss: 0.1958 \n",
            "| Epoch [ 50/ 60] Iter[231/251]\t\tLoss: 0.0802 \n",
            "| Epoch [ 50/ 60] Iter[236/251]\t\tLoss: 0.2820 \n",
            "| Epoch [ 50/ 60] Iter[241/251]\t\tLoss: 0.3765 \n",
            "| Epoch [ 50/ 60] Iter[246/251]\t\tLoss: 1.0013 \n",
            "\n",
            "| Validation Epoch #50\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #51, LR=0.0000\n",
            "| Epoch [ 51/ 60] Iter[  1/251]\t\tLoss: 0.3562 \n",
            "| Epoch [ 51/ 60] Iter[  6/251]\t\tLoss: 6.0721 \n",
            "| Epoch [ 51/ 60] Iter[ 11/251]\t\tLoss: 0.7859 \n",
            "| Epoch [ 51/ 60] Iter[ 16/251]\t\tLoss: 0.3528 \n",
            "| Epoch [ 51/ 60] Iter[ 21/251]\t\tLoss: 0.1210 \n",
            "| Epoch [ 51/ 60] Iter[ 26/251]\t\tLoss: 1.1447 \n",
            "| Epoch [ 51/ 60] Iter[ 31/251]\t\tLoss: 0.0767 \n",
            "| Epoch [ 51/ 60] Iter[ 36/251]\t\tLoss: 0.3375 \n",
            "| Epoch [ 51/ 60] Iter[ 41/251]\t\tLoss: 0.4281 \n",
            "| Epoch [ 51/ 60] Iter[ 46/251]\t\tLoss: 0.4802 \n",
            "| Epoch [ 51/ 60] Iter[ 51/251]\t\tLoss: 0.2933 \n",
            "| Epoch [ 51/ 60] Iter[ 56/251]\t\tLoss: 0.1892 \n",
            "| Epoch [ 51/ 60] Iter[ 61/251]\t\tLoss: 0.8595 \n",
            "| Epoch [ 51/ 60] Iter[ 66/251]\t\tLoss: 0.3800 \n",
            "| Epoch [ 51/ 60] Iter[ 71/251]\t\tLoss: 0.3541 \n",
            "| Epoch [ 51/ 60] Iter[ 76/251]\t\tLoss: 0.7964 \n",
            "| Epoch [ 51/ 60] Iter[ 81/251]\t\tLoss: 0.5114 \n",
            "| Epoch [ 51/ 60] Iter[ 86/251]\t\tLoss: 0.3950 \n",
            "| Epoch [ 51/ 60] Iter[ 91/251]\t\tLoss: 0.5090 \n",
            "| Epoch [ 51/ 60] Iter[ 96/251]\t\tLoss: 0.7156 \n",
            "| Epoch [ 51/ 60] Iter[101/251]\t\tLoss: 0.8567 \n",
            "| Epoch [ 51/ 60] Iter[106/251]\t\tLoss: 0.8612 \n",
            "| Epoch [ 51/ 60] Iter[111/251]\t\tLoss: 0.3299 \n",
            "| Epoch [ 51/ 60] Iter[116/251]\t\tLoss: 1.6310 \n",
            "| Epoch [ 51/ 60] Iter[121/251]\t\tLoss: 0.7685 \n",
            "| Epoch [ 51/ 60] Iter[126/251]\t\tLoss: 1.1697 \n",
            "| Epoch [ 51/ 60] Iter[131/251]\t\tLoss: 0.0903 \n",
            "| Epoch [ 51/ 60] Iter[136/251]\t\tLoss: 0.0239 \n",
            "| Epoch [ 51/ 60] Iter[141/251]\t\tLoss: 0.3703 \n",
            "| Epoch [ 51/ 60] Iter[146/251]\t\tLoss: 0.4857 \n",
            "| Epoch [ 51/ 60] Iter[151/251]\t\tLoss: 6.0266 \n",
            "| Epoch [ 51/ 60] Iter[156/251]\t\tLoss: 0.1771 \n",
            "| Epoch [ 51/ 60] Iter[161/251]\t\tLoss: 0.9048 \n",
            "| Epoch [ 51/ 60] Iter[166/251]\t\tLoss: 0.6066 \n",
            "| Epoch [ 51/ 60] Iter[171/251]\t\tLoss: 0.7198 \n",
            "| Epoch [ 51/ 60] Iter[176/251]\t\tLoss: 0.4598 \n",
            "| Epoch [ 51/ 60] Iter[181/251]\t\tLoss: 3.8681 \n",
            "| Epoch [ 51/ 60] Iter[186/251]\t\tLoss: 0.5034 \n",
            "| Epoch [ 51/ 60] Iter[191/251]\t\tLoss: 0.2533 \n",
            "| Epoch [ 51/ 60] Iter[196/251]\t\tLoss: 0.3161 \n",
            "| Epoch [ 51/ 60] Iter[201/251]\t\tLoss: 0.1613 \n",
            "| Epoch [ 51/ 60] Iter[206/251]\t\tLoss: 0.0588 \n",
            "| Epoch [ 51/ 60] Iter[211/251]\t\tLoss: 0.4231 \n",
            "| Epoch [ 51/ 60] Iter[216/251]\t\tLoss: 0.3143 \n",
            "| Epoch [ 51/ 60] Iter[221/251]\t\tLoss: 0.9421 \n",
            "| Epoch [ 51/ 60] Iter[226/251]\t\tLoss: 6.5352 \n",
            "| Epoch [ 51/ 60] Iter[231/251]\t\tLoss: 0.7336 \n",
            "| Epoch [ 51/ 60] Iter[236/251]\t\tLoss: 0.7114 \n",
            "| Epoch [ 51/ 60] Iter[241/251]\t\tLoss: 0.6413 \n",
            "| Epoch [ 51/ 60] Iter[246/251]\t\tLoss: 0.4175 \n",
            "\n",
            "| Validation Epoch #51\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #52, LR=0.0000\n",
            "| Epoch [ 52/ 60] Iter[  1/251]\t\tLoss: 0.2201 \n",
            "| Epoch [ 52/ 60] Iter[  6/251]\t\tLoss: 0.1210 \n",
            "| Epoch [ 52/ 60] Iter[ 11/251]\t\tLoss: 0.4961 \n",
            "| Epoch [ 52/ 60] Iter[ 16/251]\t\tLoss: 0.5627 \n",
            "| Epoch [ 52/ 60] Iter[ 21/251]\t\tLoss: 0.4911 \n",
            "| Epoch [ 52/ 60] Iter[ 26/251]\t\tLoss: 0.2389 \n",
            "| Epoch [ 52/ 60] Iter[ 31/251]\t\tLoss: 0.2831 \n",
            "| Epoch [ 52/ 60] Iter[ 36/251]\t\tLoss: 0.0244 \n",
            "| Epoch [ 52/ 60] Iter[ 41/251]\t\tLoss: 0.2064 \n",
            "| Epoch [ 52/ 60] Iter[ 46/251]\t\tLoss: 0.3773 \n",
            "| Epoch [ 52/ 60] Iter[ 51/251]\t\tLoss: 0.0779 \n",
            "| Epoch [ 52/ 60] Iter[ 56/251]\t\tLoss: 0.7144 \n",
            "| Epoch [ 52/ 60] Iter[ 61/251]\t\tLoss: 0.5010 \n",
            "| Epoch [ 52/ 60] Iter[ 66/251]\t\tLoss: 0.0382 \n",
            "| Epoch [ 52/ 60] Iter[ 71/251]\t\tLoss: 0.2157 \n",
            "| Epoch [ 52/ 60] Iter[ 76/251]\t\tLoss: 0.3088 \n",
            "| Epoch [ 52/ 60] Iter[ 81/251]\t\tLoss: 0.6588 \n",
            "| Epoch [ 52/ 60] Iter[ 86/251]\t\tLoss: 0.0699 \n",
            "| Epoch [ 52/ 60] Iter[ 91/251]\t\tLoss: 0.6304 \n",
            "| Epoch [ 52/ 60] Iter[ 96/251]\t\tLoss: 0.2259 \n",
            "| Epoch [ 52/ 60] Iter[101/251]\t\tLoss: 0.2913 \n",
            "| Epoch [ 52/ 60] Iter[106/251]\t\tLoss: 1.0339 \n",
            "| Epoch [ 52/ 60] Iter[111/251]\t\tLoss: 1.0506 \n",
            "| Epoch [ 52/ 60] Iter[116/251]\t\tLoss: 0.3790 \n",
            "| Epoch [ 52/ 60] Iter[121/251]\t\tLoss: 0.4102 \n",
            "| Epoch [ 52/ 60] Iter[126/251]\t\tLoss: 0.2546 \n",
            "| Epoch [ 52/ 60] Iter[131/251]\t\tLoss: 0.3562 \n",
            "| Epoch [ 52/ 60] Iter[136/251]\t\tLoss: 0.6651 \n",
            "| Epoch [ 52/ 60] Iter[141/251]\t\tLoss: 0.4481 \n",
            "| Epoch [ 52/ 60] Iter[146/251]\t\tLoss: 5.8322 \n",
            "| Epoch [ 52/ 60] Iter[151/251]\t\tLoss: 0.4448 \n",
            "| Epoch [ 52/ 60] Iter[156/251]\t\tLoss: 6.1298 \n",
            "| Epoch [ 52/ 60] Iter[161/251]\t\tLoss: 0.1286 \n",
            "| Epoch [ 52/ 60] Iter[166/251]\t\tLoss: 1.1910 \n",
            "| Epoch [ 52/ 60] Iter[171/251]\t\tLoss: 0.4411 \n",
            "| Epoch [ 52/ 60] Iter[176/251]\t\tLoss: 6.2144 \n",
            "| Epoch [ 52/ 60] Iter[181/251]\t\tLoss: 0.4400 \n",
            "| Epoch [ 52/ 60] Iter[186/251]\t\tLoss: 7.1074 \n",
            "| Epoch [ 52/ 60] Iter[191/251]\t\tLoss: 0.4048 \n",
            "| Epoch [ 52/ 60] Iter[196/251]\t\tLoss: 0.4181 \n",
            "| Epoch [ 52/ 60] Iter[201/251]\t\tLoss: 1.3545 \n",
            "| Epoch [ 52/ 60] Iter[206/251]\t\tLoss: 0.6567 \n",
            "| Epoch [ 52/ 60] Iter[211/251]\t\tLoss: 0.3513 \n",
            "| Epoch [ 52/ 60] Iter[216/251]\t\tLoss: 0.1425 \n",
            "| Epoch [ 52/ 60] Iter[221/251]\t\tLoss: 0.1907 \n",
            "| Epoch [ 52/ 60] Iter[226/251]\t\tLoss: 0.5401 \n",
            "| Epoch [ 52/ 60] Iter[231/251]\t\tLoss: 0.0218 \n",
            "| Epoch [ 52/ 60] Iter[236/251]\t\tLoss: 0.4811 \n",
            "| Epoch [ 52/ 60] Iter[241/251]\t\tLoss: 0.6566 \n",
            "| Epoch [ 52/ 60] Iter[246/251]\t\tLoss: 0.2236 \n",
            "\n",
            "| Validation Epoch #52\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #53, LR=0.0000\n",
            "| Epoch [ 53/ 60] Iter[  1/251]\t\tLoss: 1.6151 \n",
            "| Epoch [ 53/ 60] Iter[  6/251]\t\tLoss: 0.7895 \n",
            "| Epoch [ 53/ 60] Iter[ 11/251]\t\tLoss: 0.4265 \n",
            "| Epoch [ 53/ 60] Iter[ 16/251]\t\tLoss: 0.4756 \n",
            "| Epoch [ 53/ 60] Iter[ 21/251]\t\tLoss: 0.4180 \n",
            "| Epoch [ 53/ 60] Iter[ 26/251]\t\tLoss: 0.2626 \n",
            "| Epoch [ 53/ 60] Iter[ 31/251]\t\tLoss: 0.6666 \n",
            "| Epoch [ 53/ 60] Iter[ 36/251]\t\tLoss: 5.7958 \n",
            "| Epoch [ 53/ 60] Iter[ 41/251]\t\tLoss: 6.7788 \n",
            "| Epoch [ 53/ 60] Iter[ 46/251]\t\tLoss: 0.8522 \n",
            "| Epoch [ 53/ 60] Iter[ 51/251]\t\tLoss: 0.2765 \n",
            "| Epoch [ 53/ 60] Iter[ 56/251]\t\tLoss: 0.7277 \n",
            "| Epoch [ 53/ 60] Iter[ 61/251]\t\tLoss: 0.1854 \n",
            "| Epoch [ 53/ 60] Iter[ 66/251]\t\tLoss: 0.1252 \n",
            "| Epoch [ 53/ 60] Iter[ 71/251]\t\tLoss: 0.7702 \n",
            "| Epoch [ 53/ 60] Iter[ 76/251]\t\tLoss: 0.4257 \n",
            "| Epoch [ 53/ 60] Iter[ 81/251]\t\tLoss: 0.1761 \n",
            "| Epoch [ 53/ 60] Iter[ 86/251]\t\tLoss: 0.4366 \n",
            "| Epoch [ 53/ 60] Iter[ 91/251]\t\tLoss: 0.5698 \n",
            "| Epoch [ 53/ 60] Iter[ 96/251]\t\tLoss: 1.2232 \n",
            "| Epoch [ 53/ 60] Iter[101/251]\t\tLoss: 0.2353 \n",
            "| Epoch [ 53/ 60] Iter[106/251]\t\tLoss: 0.4731 \n",
            "| Epoch [ 53/ 60] Iter[111/251]\t\tLoss: 0.4075 \n",
            "| Epoch [ 53/ 60] Iter[116/251]\t\tLoss: 0.2888 \n",
            "| Epoch [ 53/ 60] Iter[121/251]\t\tLoss: 0.3918 \n",
            "| Epoch [ 53/ 60] Iter[126/251]\t\tLoss: 0.5074 \n",
            "| Epoch [ 53/ 60] Iter[131/251]\t\tLoss: 0.2976 \n",
            "| Epoch [ 53/ 60] Iter[136/251]\t\tLoss: 0.2364 \n",
            "| Epoch [ 53/ 60] Iter[141/251]\t\tLoss: 0.9831 \n",
            "| Epoch [ 53/ 60] Iter[146/251]\t\tLoss: 0.5531 \n",
            "| Epoch [ 53/ 60] Iter[151/251]\t\tLoss: 1.5171 \n",
            "| Epoch [ 53/ 60] Iter[156/251]\t\tLoss: 0.0474 \n",
            "| Epoch [ 53/ 60] Iter[161/251]\t\tLoss: 0.3225 \n",
            "| Epoch [ 53/ 60] Iter[166/251]\t\tLoss: 0.5720 \n",
            "| Epoch [ 53/ 60] Iter[171/251]\t\tLoss: 0.1205 \n",
            "| Epoch [ 53/ 60] Iter[176/251]\t\tLoss: 0.5341 \n",
            "| Epoch [ 53/ 60] Iter[181/251]\t\tLoss: 0.3207 \n",
            "| Epoch [ 53/ 60] Iter[186/251]\t\tLoss: 1.3555 \n",
            "| Epoch [ 53/ 60] Iter[191/251]\t\tLoss: 6.2681 \n",
            "| Epoch [ 53/ 60] Iter[196/251]\t\tLoss: 0.4436 \n",
            "| Epoch [ 53/ 60] Iter[201/251]\t\tLoss: 0.3016 \n",
            "| Epoch [ 53/ 60] Iter[206/251]\t\tLoss: 0.2786 \n",
            "| Epoch [ 53/ 60] Iter[211/251]\t\tLoss: 0.3460 \n",
            "| Epoch [ 53/ 60] Iter[216/251]\t\tLoss: 1.1283 \n",
            "| Epoch [ 53/ 60] Iter[221/251]\t\tLoss: 6.1008 \n",
            "| Epoch [ 53/ 60] Iter[226/251]\t\tLoss: 0.3783 \n",
            "| Epoch [ 53/ 60] Iter[231/251]\t\tLoss: 0.5993 \n",
            "| Epoch [ 53/ 60] Iter[236/251]\t\tLoss: 0.3644 \n",
            "| Epoch [ 53/ 60] Iter[241/251]\t\tLoss: 0.8367 \n",
            "| Epoch [ 53/ 60] Iter[246/251]\t\tLoss: 0.4480 \n",
            "\n",
            "| Validation Epoch #53\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #54, LR=0.0000\n",
            "| Epoch [ 54/ 60] Iter[  1/251]\t\tLoss: 0.4453 \n",
            "| Epoch [ 54/ 60] Iter[  6/251]\t\tLoss: 0.4789 \n",
            "| Epoch [ 54/ 60] Iter[ 11/251]\t\tLoss: 0.4055 \n",
            "| Epoch [ 54/ 60] Iter[ 16/251]\t\tLoss: 6.5485 \n",
            "| Epoch [ 54/ 60] Iter[ 21/251]\t\tLoss: 0.1844 \n",
            "| Epoch [ 54/ 60] Iter[ 26/251]\t\tLoss: 1.1354 \n",
            "| Epoch [ 54/ 60] Iter[ 31/251]\t\tLoss: 0.0206 \n",
            "| Epoch [ 54/ 60] Iter[ 36/251]\t\tLoss: 0.3568 \n",
            "| Epoch [ 54/ 60] Iter[ 41/251]\t\tLoss: 5.7968 \n",
            "| Epoch [ 54/ 60] Iter[ 46/251]\t\tLoss: 0.4268 \n",
            "| Epoch [ 54/ 60] Iter[ 51/251]\t\tLoss: 0.5408 \n",
            "| Epoch [ 54/ 60] Iter[ 56/251]\t\tLoss: 0.0872 \n",
            "| Epoch [ 54/ 60] Iter[ 61/251]\t\tLoss: 0.5533 \n",
            "| Epoch [ 54/ 60] Iter[ 66/251]\t\tLoss: 0.2936 \n",
            "| Epoch [ 54/ 60] Iter[ 71/251]\t\tLoss: 0.3300 \n",
            "| Epoch [ 54/ 60] Iter[ 76/251]\t\tLoss: 0.0990 \n",
            "| Epoch [ 54/ 60] Iter[ 81/251]\t\tLoss: 0.3465 \n",
            "| Epoch [ 54/ 60] Iter[ 86/251]\t\tLoss: 0.4451 \n",
            "| Epoch [ 54/ 60] Iter[ 91/251]\t\tLoss: 0.1089 \n",
            "| Epoch [ 54/ 60] Iter[ 96/251]\t\tLoss: 0.4670 \n",
            "| Epoch [ 54/ 60] Iter[101/251]\t\tLoss: 0.2355 \n",
            "| Epoch [ 54/ 60] Iter[106/251]\t\tLoss: 6.5802 \n",
            "| Epoch [ 54/ 60] Iter[111/251]\t\tLoss: 0.4536 \n",
            "| Epoch [ 54/ 60] Iter[116/251]\t\tLoss: 0.2952 \n",
            "| Epoch [ 54/ 60] Iter[121/251]\t\tLoss: 0.8805 \n",
            "| Epoch [ 54/ 60] Iter[126/251]\t\tLoss: 5.6237 \n",
            "| Epoch [ 54/ 60] Iter[131/251]\t\tLoss: 0.4548 \n",
            "| Epoch [ 54/ 60] Iter[136/251]\t\tLoss: 0.3147 \n",
            "| Epoch [ 54/ 60] Iter[141/251]\t\tLoss: 0.2997 \n",
            "| Epoch [ 54/ 60] Iter[146/251]\t\tLoss: 0.3773 \n",
            "| Epoch [ 54/ 60] Iter[151/251]\t\tLoss: 0.5210 \n",
            "| Epoch [ 54/ 60] Iter[156/251]\t\tLoss: 5.7948 \n",
            "| Epoch [ 54/ 60] Iter[161/251]\t\tLoss: 0.3529 \n",
            "| Epoch [ 54/ 60] Iter[166/251]\t\tLoss: 0.7500 \n",
            "| Epoch [ 54/ 60] Iter[171/251]\t\tLoss: 0.2091 \n",
            "| Epoch [ 54/ 60] Iter[176/251]\t\tLoss: 0.7932 \n",
            "| Epoch [ 54/ 60] Iter[181/251]\t\tLoss: 0.7293 \n",
            "| Epoch [ 54/ 60] Iter[186/251]\t\tLoss: 0.5198 \n",
            "| Epoch [ 54/ 60] Iter[191/251]\t\tLoss: 6.5111 \n",
            "| Epoch [ 54/ 60] Iter[196/251]\t\tLoss: 0.3901 \n",
            "| Epoch [ 54/ 60] Iter[201/251]\t\tLoss: 0.1902 \n",
            "| Epoch [ 54/ 60] Iter[206/251]\t\tLoss: 0.7032 \n",
            "| Epoch [ 54/ 60] Iter[211/251]\t\tLoss: 0.6889 \n",
            "| Epoch [ 54/ 60] Iter[216/251]\t\tLoss: 0.3863 \n",
            "| Epoch [ 54/ 60] Iter[221/251]\t\tLoss: 0.6837 \n",
            "| Epoch [ 54/ 60] Iter[226/251]\t\tLoss: 0.2938 \n",
            "| Epoch [ 54/ 60] Iter[231/251]\t\tLoss: 0.4825 \n",
            "| Epoch [ 54/ 60] Iter[236/251]\t\tLoss: 0.3549 \n",
            "| Epoch [ 54/ 60] Iter[241/251]\t\tLoss: 0.3707 \n",
            "| Epoch [ 54/ 60] Iter[246/251]\t\tLoss: 0.0891 \n",
            "\n",
            "| Validation Epoch #54\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #55, LR=0.0000\n",
            "| Epoch [ 55/ 60] Iter[  1/251]\t\tLoss: 0.4383 \n",
            "| Epoch [ 55/ 60] Iter[  6/251]\t\tLoss: 0.8519 \n",
            "| Epoch [ 55/ 60] Iter[ 11/251]\t\tLoss: 0.3949 \n",
            "| Epoch [ 55/ 60] Iter[ 16/251]\t\tLoss: 0.1470 \n",
            "| Epoch [ 55/ 60] Iter[ 21/251]\t\tLoss: 0.5512 \n",
            "| Epoch [ 55/ 60] Iter[ 26/251]\t\tLoss: 0.4522 \n",
            "| Epoch [ 55/ 60] Iter[ 31/251]\t\tLoss: 0.9071 \n",
            "| Epoch [ 55/ 60] Iter[ 36/251]\t\tLoss: 0.2888 \n",
            "| Epoch [ 55/ 60] Iter[ 41/251]\t\tLoss: 0.5114 \n",
            "| Epoch [ 55/ 60] Iter[ 46/251]\t\tLoss: 0.4329 \n",
            "| Epoch [ 55/ 60] Iter[ 51/251]\t\tLoss: 0.6000 \n",
            "| Epoch [ 55/ 60] Iter[ 56/251]\t\tLoss: 0.2626 \n",
            "| Epoch [ 55/ 60] Iter[ 61/251]\t\tLoss: 0.0674 \n",
            "| Epoch [ 55/ 60] Iter[ 66/251]\t\tLoss: 0.5908 \n",
            "| Epoch [ 55/ 60] Iter[ 71/251]\t\tLoss: 6.6797 \n",
            "| Epoch [ 55/ 60] Iter[ 76/251]\t\tLoss: 3.6285 \n",
            "| Epoch [ 55/ 60] Iter[ 81/251]\t\tLoss: 0.1845 \n",
            "| Epoch [ 55/ 60] Iter[ 86/251]\t\tLoss: 0.8325 \n",
            "| Epoch [ 55/ 60] Iter[ 91/251]\t\tLoss: 0.0516 \n",
            "| Epoch [ 55/ 60] Iter[ 96/251]\t\tLoss: 0.2513 \n",
            "| Epoch [ 55/ 60] Iter[101/251]\t\tLoss: 0.1917 \n",
            "| Epoch [ 55/ 60] Iter[106/251]\t\tLoss: 0.4042 \n",
            "| Epoch [ 55/ 60] Iter[111/251]\t\tLoss: 0.5618 \n",
            "| Epoch [ 55/ 60] Iter[116/251]\t\tLoss: 0.7079 \n",
            "| Epoch [ 55/ 60] Iter[121/251]\t\tLoss: 0.4250 \n",
            "| Epoch [ 55/ 60] Iter[126/251]\t\tLoss: 2.1805 \n",
            "| Epoch [ 55/ 60] Iter[131/251]\t\tLoss: 0.6620 \n",
            "| Epoch [ 55/ 60] Iter[136/251]\t\tLoss: 0.8523 \n",
            "| Epoch [ 55/ 60] Iter[141/251]\t\tLoss: 0.4202 \n",
            "| Epoch [ 55/ 60] Iter[146/251]\t\tLoss: 0.2540 \n",
            "| Epoch [ 55/ 60] Iter[151/251]\t\tLoss: 6.5146 \n",
            "| Epoch [ 55/ 60] Iter[156/251]\t\tLoss: 6.3106 \n",
            "| Epoch [ 55/ 60] Iter[161/251]\t\tLoss: 0.2313 \n",
            "| Epoch [ 55/ 60] Iter[166/251]\t\tLoss: 0.7651 \n",
            "| Epoch [ 55/ 60] Iter[171/251]\t\tLoss: 1.2693 \n",
            "| Epoch [ 55/ 60] Iter[176/251]\t\tLoss: 1.1683 \n",
            "| Epoch [ 55/ 60] Iter[181/251]\t\tLoss: 0.8834 \n",
            "| Epoch [ 55/ 60] Iter[186/251]\t\tLoss: 1.4750 \n",
            "| Epoch [ 55/ 60] Iter[191/251]\t\tLoss: 0.0858 \n",
            "| Epoch [ 55/ 60] Iter[196/251]\t\tLoss: 0.6570 \n",
            "| Epoch [ 55/ 60] Iter[201/251]\t\tLoss: 0.3719 \n",
            "| Epoch [ 55/ 60] Iter[206/251]\t\tLoss: 0.1330 \n",
            "| Epoch [ 55/ 60] Iter[211/251]\t\tLoss: 0.3754 \n",
            "| Epoch [ 55/ 60] Iter[216/251]\t\tLoss: 0.1814 \n",
            "| Epoch [ 55/ 60] Iter[221/251]\t\tLoss: 1.2886 \n",
            "| Epoch [ 55/ 60] Iter[226/251]\t\tLoss: 0.4381 \n",
            "| Epoch [ 55/ 60] Iter[231/251]\t\tLoss: 1.1705 \n",
            "| Epoch [ 55/ 60] Iter[236/251]\t\tLoss: 0.4054 \n",
            "| Epoch [ 55/ 60] Iter[241/251]\t\tLoss: 0.5966 \n",
            "| Epoch [ 55/ 60] Iter[246/251]\t\tLoss: 0.5635 \n",
            "\n",
            "| Validation Epoch #55\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #56, LR=0.0000\n",
            "| Epoch [ 56/ 60] Iter[  1/251]\t\tLoss: 0.1534 \n",
            "| Epoch [ 56/ 60] Iter[  6/251]\t\tLoss: 0.5830 \n",
            "| Epoch [ 56/ 60] Iter[ 11/251]\t\tLoss: 0.1451 \n",
            "| Epoch [ 56/ 60] Iter[ 16/251]\t\tLoss: 1.1573 \n",
            "| Epoch [ 56/ 60] Iter[ 21/251]\t\tLoss: 0.9497 \n",
            "| Epoch [ 56/ 60] Iter[ 26/251]\t\tLoss: 0.1353 \n",
            "| Epoch [ 56/ 60] Iter[ 31/251]\t\tLoss: 0.8440 \n",
            "| Epoch [ 56/ 60] Iter[ 36/251]\t\tLoss: 0.7767 \n",
            "| Epoch [ 56/ 60] Iter[ 41/251]\t\tLoss: 7.6616 \n",
            "| Epoch [ 56/ 60] Iter[ 46/251]\t\tLoss: 0.2476 \n",
            "| Epoch [ 56/ 60] Iter[ 51/251]\t\tLoss: 0.3729 \n",
            "| Epoch [ 56/ 60] Iter[ 56/251]\t\tLoss: 1.1888 \n",
            "| Epoch [ 56/ 60] Iter[ 61/251]\t\tLoss: 1.2269 \n",
            "| Epoch [ 56/ 60] Iter[ 66/251]\t\tLoss: 6.2461 \n",
            "| Epoch [ 56/ 60] Iter[ 71/251]\t\tLoss: 0.2604 \n",
            "| Epoch [ 56/ 60] Iter[ 76/251]\t\tLoss: 0.1339 \n",
            "| Epoch [ 56/ 60] Iter[ 81/251]\t\tLoss: 0.6055 \n",
            "| Epoch [ 56/ 60] Iter[ 86/251]\t\tLoss: 0.2704 \n",
            "| Epoch [ 56/ 60] Iter[ 91/251]\t\tLoss: 0.2002 \n",
            "| Epoch [ 56/ 60] Iter[ 96/251]\t\tLoss: 0.7161 \n",
            "| Epoch [ 56/ 60] Iter[101/251]\t\tLoss: 0.1125 \n",
            "| Epoch [ 56/ 60] Iter[106/251]\t\tLoss: 0.2225 \n",
            "| Epoch [ 56/ 60] Iter[111/251]\t\tLoss: 0.4267 \n",
            "| Epoch [ 56/ 60] Iter[116/251]\t\tLoss: 0.3045 \n",
            "| Epoch [ 56/ 60] Iter[121/251]\t\tLoss: 0.4213 \n",
            "| Epoch [ 56/ 60] Iter[126/251]\t\tLoss: 1.6930 \n",
            "| Epoch [ 56/ 60] Iter[131/251]\t\tLoss: 0.0677 \n",
            "| Epoch [ 56/ 60] Iter[136/251]\t\tLoss: 6.3937 \n",
            "| Epoch [ 56/ 60] Iter[141/251]\t\tLoss: 0.7297 \n",
            "| Epoch [ 56/ 60] Iter[146/251]\t\tLoss: 0.4170 \n",
            "| Epoch [ 56/ 60] Iter[151/251]\t\tLoss: 0.6297 \n",
            "| Epoch [ 56/ 60] Iter[156/251]\t\tLoss: 0.6616 \n",
            "| Epoch [ 56/ 60] Iter[161/251]\t\tLoss: 1.0799 \n",
            "| Epoch [ 56/ 60] Iter[166/251]\t\tLoss: 0.3735 \n",
            "| Epoch [ 56/ 60] Iter[171/251]\t\tLoss: 1.3683 \n",
            "| Epoch [ 56/ 60] Iter[176/251]\t\tLoss: 0.6338 \n",
            "| Epoch [ 56/ 60] Iter[181/251]\t\tLoss: 5.7823 \n",
            "| Epoch [ 56/ 60] Iter[186/251]\t\tLoss: 5.9613 \n",
            "| Epoch [ 56/ 60] Iter[191/251]\t\tLoss: 5.6420 \n",
            "| Epoch [ 56/ 60] Iter[196/251]\t\tLoss: 0.4988 \n",
            "| Epoch [ 56/ 60] Iter[201/251]\t\tLoss: 0.5302 \n",
            "| Epoch [ 56/ 60] Iter[206/251]\t\tLoss: 1.1091 \n",
            "| Epoch [ 56/ 60] Iter[211/251]\t\tLoss: 0.8961 \n",
            "| Epoch [ 56/ 60] Iter[216/251]\t\tLoss: 0.3341 \n",
            "| Epoch [ 56/ 60] Iter[221/251]\t\tLoss: 0.1321 \n",
            "| Epoch [ 56/ 60] Iter[226/251]\t\tLoss: 0.6632 \n",
            "| Epoch [ 56/ 60] Iter[231/251]\t\tLoss: 0.4234 \n",
            "| Epoch [ 56/ 60] Iter[236/251]\t\tLoss: 0.3206 \n",
            "| Epoch [ 56/ 60] Iter[241/251]\t\tLoss: 0.1220 \n",
            "| Epoch [ 56/ 60] Iter[246/251]\t\tLoss: 0.1887 \n",
            "\n",
            "| Validation Epoch #56\t\t\tLoss: 0.2206 \n",
            "\n",
            "=> Training Epoch #57, LR=0.0000\n",
            "| Epoch [ 57/ 60] Iter[  1/251]\t\tLoss: 0.4182 \n",
            "| Epoch [ 57/ 60] Iter[  6/251]\t\tLoss: 0.4019 \n",
            "| Epoch [ 57/ 60] Iter[ 11/251]\t\tLoss: 0.6140 \n",
            "| Epoch [ 57/ 60] Iter[ 16/251]\t\tLoss: 0.5067 \n",
            "| Epoch [ 57/ 60] Iter[ 21/251]\t\tLoss: 0.7963 \n",
            "| Epoch [ 57/ 60] Iter[ 26/251]\t\tLoss: 0.5268 \n",
            "| Epoch [ 57/ 60] Iter[ 31/251]\t\tLoss: 6.3938 \n",
            "| Epoch [ 57/ 60] Iter[ 36/251]\t\tLoss: 0.1735 \n",
            "| Epoch [ 57/ 60] Iter[ 41/251]\t\tLoss: 0.4641 \n",
            "| Epoch [ 57/ 60] Iter[ 46/251]\t\tLoss: 0.4453 \n",
            "| Epoch [ 57/ 60] Iter[ 51/251]\t\tLoss: 0.2563 \n",
            "| Epoch [ 57/ 60] Iter[ 56/251]\t\tLoss: 0.8182 \n",
            "| Epoch [ 57/ 60] Iter[ 61/251]\t\tLoss: 0.3726 \n",
            "| Epoch [ 57/ 60] Iter[ 66/251]\t\tLoss: 6.3294 \n",
            "| Epoch [ 57/ 60] Iter[ 71/251]\t\tLoss: 0.3157 \n",
            "| Epoch [ 57/ 60] Iter[ 76/251]\t\tLoss: 0.1913 \n",
            "| Epoch [ 57/ 60] Iter[ 81/251]\t\tLoss: 0.9362 \n",
            "| Epoch [ 57/ 60] Iter[ 86/251]\t\tLoss: 0.2061 \n",
            "| Epoch [ 57/ 60] Iter[ 91/251]\t\tLoss: 0.4682 \n",
            "| Epoch [ 57/ 60] Iter[ 96/251]\t\tLoss: 0.1213 \n",
            "| Epoch [ 57/ 60] Iter[101/251]\t\tLoss: 0.5534 \n",
            "| Epoch [ 57/ 60] Iter[106/251]\t\tLoss: 0.4622 \n",
            "| Epoch [ 57/ 60] Iter[111/251]\t\tLoss: 0.5873 \n",
            "| Epoch [ 57/ 60] Iter[116/251]\t\tLoss: 0.0730 \n",
            "| Epoch [ 57/ 60] Iter[121/251]\t\tLoss: 7.0187 \n",
            "| Epoch [ 57/ 60] Iter[126/251]\t\tLoss: 0.4604 \n",
            "| Epoch [ 57/ 60] Iter[131/251]\t\tLoss: 0.5253 \n",
            "| Epoch [ 57/ 60] Iter[136/251]\t\tLoss: 0.8254 \n",
            "| Epoch [ 57/ 60] Iter[141/251]\t\tLoss: 0.5092 \n",
            "| Epoch [ 57/ 60] Iter[146/251]\t\tLoss: 0.4435 \n",
            "| Epoch [ 57/ 60] Iter[151/251]\t\tLoss: 0.5947 \n",
            "| Epoch [ 57/ 60] Iter[156/251]\t\tLoss: 0.0749 \n",
            "| Epoch [ 57/ 60] Iter[161/251]\t\tLoss: 0.5803 \n",
            "| Epoch [ 57/ 60] Iter[166/251]\t\tLoss: 0.1277 \n",
            "| Epoch [ 57/ 60] Iter[171/251]\t\tLoss: 0.8308 \n",
            "| Epoch [ 57/ 60] Iter[176/251]\t\tLoss: 0.4133 \n",
            "| Epoch [ 57/ 60] Iter[181/251]\t\tLoss: 0.9364 \n",
            "| Epoch [ 57/ 60] Iter[186/251]\t\tLoss: 0.6527 \n",
            "| Epoch [ 57/ 60] Iter[191/251]\t\tLoss: 1.0952 \n",
            "| Epoch [ 57/ 60] Iter[196/251]\t\tLoss: 0.2914 \n",
            "| Epoch [ 57/ 60] Iter[201/251]\t\tLoss: 0.1946 \n",
            "| Epoch [ 57/ 60] Iter[206/251]\t\tLoss: 0.1629 \n",
            "| Epoch [ 57/ 60] Iter[211/251]\t\tLoss: 0.1776 \n",
            "| Epoch [ 57/ 60] Iter[216/251]\t\tLoss: 0.4657 \n",
            "| Epoch [ 57/ 60] Iter[221/251]\t\tLoss: 0.0619 \n",
            "| Epoch [ 57/ 60] Iter[226/251]\t\tLoss: 0.6741 \n",
            "| Epoch [ 57/ 60] Iter[231/251]\t\tLoss: 0.1833 \n",
            "| Epoch [ 57/ 60] Iter[236/251]\t\tLoss: 0.3281 \n",
            "| Epoch [ 57/ 60] Iter[241/251]\t\tLoss: 0.5788 \n",
            "| Epoch [ 57/ 60] Iter[246/251]\t\tLoss: 0.5698 \n",
            "\n",
            "| Validation Epoch #57\t\t\tLoss: 0.2207 \n",
            "\n",
            "=> Training Epoch #58, LR=0.0000\n",
            "| Epoch [ 58/ 60] Iter[  1/251]\t\tLoss: 0.4659 \n",
            "| Epoch [ 58/ 60] Iter[  6/251]\t\tLoss: 0.3727 \n",
            "| Epoch [ 58/ 60] Iter[ 11/251]\t\tLoss: 0.3538 \n",
            "| Epoch [ 58/ 60] Iter[ 16/251]\t\tLoss: 0.2271 \n",
            "| Epoch [ 58/ 60] Iter[ 21/251]\t\tLoss: 0.9499 \n",
            "| Epoch [ 58/ 60] Iter[ 26/251]\t\tLoss: 0.3041 \n",
            "| Epoch [ 58/ 60] Iter[ 31/251]\t\tLoss: 0.3246 \n",
            "| Epoch [ 58/ 60] Iter[ 36/251]\t\tLoss: 0.2358 \n",
            "| Epoch [ 58/ 60] Iter[ 41/251]\t\tLoss: 0.3514 \n",
            "| Epoch [ 58/ 60] Iter[ 46/251]\t\tLoss: 0.0211 \n",
            "| Epoch [ 58/ 60] Iter[ 51/251]\t\tLoss: 0.2988 \n",
            "| Epoch [ 58/ 60] Iter[ 56/251]\t\tLoss: 0.4939 \n",
            "| Epoch [ 58/ 60] Iter[ 61/251]\t\tLoss: 1.0681 \n",
            "| Epoch [ 58/ 60] Iter[ 66/251]\t\tLoss: 0.3699 \n",
            "| Epoch [ 58/ 60] Iter[ 71/251]\t\tLoss: 0.0153 \n",
            "| Epoch [ 58/ 60] Iter[ 76/251]\t\tLoss: 0.7509 \n",
            "| Epoch [ 58/ 60] Iter[ 81/251]\t\tLoss: 0.3976 \n",
            "| Epoch [ 58/ 60] Iter[ 86/251]\t\tLoss: 0.1307 \n",
            "| Epoch [ 58/ 60] Iter[ 91/251]\t\tLoss: 0.5098 \n",
            "| Epoch [ 58/ 60] Iter[ 96/251]\t\tLoss: 0.6183 \n",
            "| Epoch [ 58/ 60] Iter[101/251]\t\tLoss: 0.3698 \n",
            "| Epoch [ 58/ 60] Iter[106/251]\t\tLoss: 0.4317 \n",
            "| Epoch [ 58/ 60] Iter[111/251]\t\tLoss: 2.8022 \n",
            "| Epoch [ 58/ 60] Iter[116/251]\t\tLoss: 0.1946 \n",
            "| Epoch [ 58/ 60] Iter[121/251]\t\tLoss: 0.2028 \n",
            "| Epoch [ 58/ 60] Iter[126/251]\t\tLoss: 0.5774 \n",
            "| Epoch [ 58/ 60] Iter[131/251]\t\tLoss: 0.4951 \n",
            "| Epoch [ 58/ 60] Iter[136/251]\t\tLoss: 0.5283 \n",
            "| Epoch [ 58/ 60] Iter[141/251]\t\tLoss: 1.2547 \n",
            "| Epoch [ 58/ 60] Iter[146/251]\t\tLoss: 0.4676 \n",
            "| Epoch [ 58/ 60] Iter[151/251]\t\tLoss: 0.2294 \n",
            "| Epoch [ 58/ 60] Iter[156/251]\t\tLoss: 0.1483 \n",
            "| Epoch [ 58/ 60] Iter[161/251]\t\tLoss: 0.3717 \n",
            "| Epoch [ 58/ 60] Iter[166/251]\t\tLoss: 0.8424 \n",
            "| Epoch [ 58/ 60] Iter[171/251]\t\tLoss: 0.5078 \n",
            "| Epoch [ 58/ 60] Iter[176/251]\t\tLoss: 0.3808 \n",
            "| Epoch [ 58/ 60] Iter[181/251]\t\tLoss: 0.2227 \n",
            "| Epoch [ 58/ 60] Iter[186/251]\t\tLoss: 0.4393 \n",
            "| Epoch [ 58/ 60] Iter[191/251]\t\tLoss: 0.1341 \n",
            "| Epoch [ 58/ 60] Iter[196/251]\t\tLoss: 1.4013 \n",
            "| Epoch [ 58/ 60] Iter[201/251]\t\tLoss: 0.1790 \n",
            "| Epoch [ 58/ 60] Iter[206/251]\t\tLoss: 1.0742 \n",
            "| Epoch [ 58/ 60] Iter[211/251]\t\tLoss: 0.4043 \n",
            "| Epoch [ 58/ 60] Iter[216/251]\t\tLoss: 0.5577 \n",
            "| Epoch [ 58/ 60] Iter[221/251]\t\tLoss: 0.7465 \n",
            "| Epoch [ 58/ 60] Iter[226/251]\t\tLoss: 0.2733 \n",
            "| Epoch [ 58/ 60] Iter[231/251]\t\tLoss: 0.4081 \n",
            "| Epoch [ 58/ 60] Iter[236/251]\t\tLoss: 6.8703 \n",
            "| Epoch [ 58/ 60] Iter[241/251]\t\tLoss: 0.4909 \n",
            "| Epoch [ 58/ 60] Iter[246/251]\t\tLoss: 0.2924 \n",
            "\n",
            "| Validation Epoch #58\t\t\tLoss: 0.2207 \n",
            "\n",
            "=> Training Epoch #59, LR=0.0000\n",
            "| Epoch [ 59/ 60] Iter[  1/251]\t\tLoss: 0.7789 \n",
            "| Epoch [ 59/ 60] Iter[  6/251]\t\tLoss: 0.8984 \n",
            "| Epoch [ 59/ 60] Iter[ 11/251]\t\tLoss: 0.3435 \n",
            "| Epoch [ 59/ 60] Iter[ 16/251]\t\tLoss: 3.1836 \n",
            "| Epoch [ 59/ 60] Iter[ 21/251]\t\tLoss: 0.2819 \n",
            "| Epoch [ 59/ 60] Iter[ 26/251]\t\tLoss: 0.0533 \n",
            "| Epoch [ 59/ 60] Iter[ 31/251]\t\tLoss: 0.5750 \n",
            "| Epoch [ 59/ 60] Iter[ 36/251]\t\tLoss: 0.6961 \n",
            "| Epoch [ 59/ 60] Iter[ 41/251]\t\tLoss: 0.9413 \n",
            "| Epoch [ 59/ 60] Iter[ 46/251]\t\tLoss: 0.1681 \n",
            "| Epoch [ 59/ 60] Iter[ 51/251]\t\tLoss: 0.1122 \n",
            "| Epoch [ 59/ 60] Iter[ 56/251]\t\tLoss: 2.3468 \n",
            "| Epoch [ 59/ 60] Iter[ 61/251]\t\tLoss: 6.4261 \n",
            "| Epoch [ 59/ 60] Iter[ 66/251]\t\tLoss: 0.4927 \n",
            "| Epoch [ 59/ 60] Iter[ 71/251]\t\tLoss: 1.0982 \n",
            "| Epoch [ 59/ 60] Iter[ 76/251]\t\tLoss: 0.7489 \n",
            "| Epoch [ 59/ 60] Iter[ 81/251]\t\tLoss: 0.4883 \n",
            "| Epoch [ 59/ 60] Iter[ 86/251]\t\tLoss: 0.4620 \n",
            "| Epoch [ 59/ 60] Iter[ 91/251]\t\tLoss: 0.4948 \n",
            "| Epoch [ 59/ 60] Iter[ 96/251]\t\tLoss: 0.8409 \n",
            "| Epoch [ 59/ 60] Iter[101/251]\t\tLoss: 0.5396 \n",
            "| Epoch [ 59/ 60] Iter[106/251]\t\tLoss: 0.8216 \n",
            "| Epoch [ 59/ 60] Iter[111/251]\t\tLoss: 6.2661 \n",
            "| Epoch [ 59/ 60] Iter[116/251]\t\tLoss: 0.6887 \n",
            "| Epoch [ 59/ 60] Iter[121/251]\t\tLoss: 0.3593 \n",
            "| Epoch [ 59/ 60] Iter[126/251]\t\tLoss: 0.8300 \n",
            "| Epoch [ 59/ 60] Iter[131/251]\t\tLoss: 0.4015 \n",
            "| Epoch [ 59/ 60] Iter[136/251]\t\tLoss: 0.4306 \n",
            "| Epoch [ 59/ 60] Iter[141/251]\t\tLoss: 0.5599 \n",
            "| Epoch [ 59/ 60] Iter[146/251]\t\tLoss: 0.1873 \n",
            "| Epoch [ 59/ 60] Iter[151/251]\t\tLoss: 0.2480 \n",
            "| Epoch [ 59/ 60] Iter[156/251]\t\tLoss: 0.1292 \n",
            "| Epoch [ 59/ 60] Iter[161/251]\t\tLoss: 0.6263 \n",
            "| Epoch [ 59/ 60] Iter[166/251]\t\tLoss: 0.7101 \n",
            "| Epoch [ 59/ 60] Iter[171/251]\t\tLoss: 0.7501 \n",
            "| Epoch [ 59/ 60] Iter[176/251]\t\tLoss: 0.6704 \n",
            "| Epoch [ 59/ 60] Iter[181/251]\t\tLoss: 0.1571 \n",
            "| Epoch [ 59/ 60] Iter[186/251]\t\tLoss: 0.3558 \n",
            "| Epoch [ 59/ 60] Iter[191/251]\t\tLoss: 0.9873 \n",
            "| Epoch [ 59/ 60] Iter[196/251]\t\tLoss: 0.8467 \n",
            "| Epoch [ 59/ 60] Iter[201/251]\t\tLoss: 0.3433 \n",
            "| Epoch [ 59/ 60] Iter[206/251]\t\tLoss: 0.3637 \n",
            "| Epoch [ 59/ 60] Iter[211/251]\t\tLoss: 6.6984 \n",
            "| Epoch [ 59/ 60] Iter[216/251]\t\tLoss: 0.2633 \n",
            "| Epoch [ 59/ 60] Iter[221/251]\t\tLoss: 0.1237 \n",
            "| Epoch [ 59/ 60] Iter[226/251]\t\tLoss: 0.3076 \n",
            "| Epoch [ 59/ 60] Iter[231/251]\t\tLoss: 0.2716 \n",
            "| Epoch [ 59/ 60] Iter[236/251]\t\tLoss: 0.4721 \n",
            "| Epoch [ 59/ 60] Iter[241/251]\t\tLoss: 5.7239 \n",
            "| Epoch [ 59/ 60] Iter[246/251]\t\tLoss: 0.2171 \n",
            "\n",
            "| Validation Epoch #59\t\t\tLoss: 0.2207 \n",
            "\n",
            "=> Training Epoch #0, LR=0.0010\n",
            "| Epoch [  0/ 60] Iter[  1/126]\t\tLoss: 5.3292 \n",
            "| Epoch [  0/ 60] Iter[  6/126]\t\tLoss: 6.5931 \n",
            "| Epoch [  0/ 60] Iter[ 11/126]\t\tLoss: 4.9242 \n",
            "| Epoch [  0/ 60] Iter[ 16/126]\t\tLoss: 1.3743 \n",
            "| Epoch [  0/ 60] Iter[ 21/126]\t\tLoss: 5.4008 \n",
            "| Epoch [  0/ 60] Iter[ 26/126]\t\tLoss: 0.3628 \n",
            "| Epoch [  0/ 60] Iter[ 31/126]\t\tLoss: 1.0295 \n",
            "| Epoch [  0/ 60] Iter[ 36/126]\t\tLoss: 0.2609 \n",
            "| Epoch [  0/ 60] Iter[ 41/126]\t\tLoss: 0.3063 \n",
            "| Epoch [  0/ 60] Iter[ 46/126]\t\tLoss: 0.7678 \n",
            "| Epoch [  0/ 60] Iter[ 51/126]\t\tLoss: 0.3120 \n",
            "| Epoch [  0/ 60] Iter[ 56/126]\t\tLoss: 0.6679 \n",
            "| Epoch [  0/ 60] Iter[ 61/126]\t\tLoss: 0.6495 \n",
            "| Epoch [  0/ 60] Iter[ 66/126]\t\tLoss: 0.4899 \n",
            "| Epoch [  0/ 60] Iter[ 71/126]\t\tLoss: 6.2612 \n",
            "| Epoch [  0/ 60] Iter[ 76/126]\t\tLoss: 0.0793 \n",
            "| Epoch [  0/ 60] Iter[ 81/126]\t\tLoss: 0.0377 \n",
            "| Epoch [  0/ 60] Iter[ 86/126]\t\tLoss: 0.4356 \n",
            "| Epoch [  0/ 60] Iter[ 91/126]\t\tLoss: 0.6455 \n",
            "| Epoch [  0/ 60] Iter[ 96/126]\t\tLoss: 0.6292 \n",
            "| Epoch [  0/ 60] Iter[101/126]\t\tLoss: 1.1783 \n",
            "| Epoch [  0/ 60] Iter[106/126]\t\tLoss: 0.9631 \n",
            "| Epoch [  0/ 60] Iter[111/126]\t\tLoss: 0.1902 \n",
            "| Epoch [  0/ 60] Iter[116/126]\t\tLoss: 1.0010 \n",
            "| Epoch [  0/ 60] Iter[121/126]\t\tLoss: 0.8175 \n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 0.2078 \n",
            "\n",
            "=> Training Epoch #1, LR=0.0010\n",
            "| Epoch [  1/ 60] Iter[  1/126]\t\tLoss: 0.5702 \n",
            "| Epoch [  1/ 60] Iter[  6/126]\t\tLoss: 0.8338 \n",
            "| Epoch [  1/ 60] Iter[ 11/126]\t\tLoss: 0.3732 \n",
            "| Epoch [  1/ 60] Iter[ 16/126]\t\tLoss: 0.7153 \n",
            "| Epoch [  1/ 60] Iter[ 21/126]\t\tLoss: 0.3596 \n",
            "| Epoch [  1/ 60] Iter[ 26/126]\t\tLoss: 6.3253 \n",
            "| Epoch [  1/ 60] Iter[ 31/126]\t\tLoss: 1.2225 \n",
            "| Epoch [  1/ 60] Iter[ 36/126]\t\tLoss: 6.1711 \n",
            "| Epoch [  1/ 60] Iter[ 41/126]\t\tLoss: 0.7018 \n",
            "| Epoch [  1/ 60] Iter[ 46/126]\t\tLoss: 0.7745 \n",
            "| Epoch [  1/ 60] Iter[ 51/126]\t\tLoss: 0.7248 \n",
            "| Epoch [  1/ 60] Iter[ 56/126]\t\tLoss: 0.4244 \n",
            "| Epoch [  1/ 60] Iter[ 61/126]\t\tLoss: 1.1459 \n",
            "| Epoch [  1/ 60] Iter[ 66/126]\t\tLoss: 0.8440 \n",
            "| Epoch [  1/ 60] Iter[ 71/126]\t\tLoss: 0.3311 \n",
            "| Epoch [  1/ 60] Iter[ 76/126]\t\tLoss: 0.9428 \n",
            "| Epoch [  1/ 60] Iter[ 81/126]\t\tLoss: 1.0747 \n",
            "| Epoch [  1/ 60] Iter[ 86/126]\t\tLoss: 6.6179 \n",
            "| Epoch [  1/ 60] Iter[ 91/126]\t\tLoss: 6.3931 \n",
            "| Epoch [  1/ 60] Iter[ 96/126]\t\tLoss: 0.3426 \n",
            "| Epoch [  1/ 60] Iter[101/126]\t\tLoss: 0.2197 \n",
            "| Epoch [  1/ 60] Iter[106/126]\t\tLoss: 0.6250 \n",
            "| Epoch [  1/ 60] Iter[111/126]\t\tLoss: 0.5957 \n",
            "| Epoch [  1/ 60] Iter[116/126]\t\tLoss: 0.9085 \n",
            "| Epoch [  1/ 60] Iter[121/126]\t\tLoss: 6.4394 \n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 0.2342 \n",
            "\n",
            "=> Training Epoch #2, LR=0.0010\n",
            "| Epoch [  2/ 60] Iter[  1/126]\t\tLoss: 0.1583 \n",
            "| Epoch [  2/ 60] Iter[  6/126]\t\tLoss: 0.6664 \n",
            "| Epoch [  2/ 60] Iter[ 11/126]\t\tLoss: 0.8079 \n",
            "| Epoch [  2/ 60] Iter[ 16/126]\t\tLoss: 0.2410 \n",
            "| Epoch [  2/ 60] Iter[ 21/126]\t\tLoss: 5.8788 \n",
            "| Epoch [  2/ 60] Iter[ 26/126]\t\tLoss: 0.6885 \n",
            "| Epoch [  2/ 60] Iter[ 31/126]\t\tLoss: 0.4531 \n",
            "| Epoch [  2/ 60] Iter[ 36/126]\t\tLoss: 0.7265 \n",
            "| Epoch [  2/ 60] Iter[ 41/126]\t\tLoss: 6.1380 \n",
            "| Epoch [  2/ 60] Iter[ 46/126]\t\tLoss: 0.7773 \n",
            "| Epoch [  2/ 60] Iter[ 51/126]\t\tLoss: 0.1136 \n",
            "| Epoch [  2/ 60] Iter[ 56/126]\t\tLoss: 0.1211 \n",
            "| Epoch [  2/ 60] Iter[ 61/126]\t\tLoss: 1.2525 \n",
            "| Epoch [  2/ 60] Iter[ 66/126]\t\tLoss: 0.2069 \n",
            "| Epoch [  2/ 60] Iter[ 71/126]\t\tLoss: 0.5149 \n",
            "| Epoch [  2/ 60] Iter[ 76/126]\t\tLoss: 0.3618 \n",
            "| Epoch [  2/ 60] Iter[ 81/126]\t\tLoss: 0.5067 \n",
            "| Epoch [  2/ 60] Iter[ 86/126]\t\tLoss: 0.1100 \n",
            "| Epoch [  2/ 60] Iter[ 91/126]\t\tLoss: 0.6647 \n",
            "| Epoch [  2/ 60] Iter[ 96/126]\t\tLoss: 5.9210 \n",
            "| Epoch [  2/ 60] Iter[101/126]\t\tLoss: 0.4611 \n",
            "| Epoch [  2/ 60] Iter[106/126]\t\tLoss: 0.7686 \n",
            "| Epoch [  2/ 60] Iter[111/126]\t\tLoss: 0.3483 \n",
            "| Epoch [  2/ 60] Iter[116/126]\t\tLoss: 0.5858 \n",
            "| Epoch [  2/ 60] Iter[121/126]\t\tLoss: 0.7762 \n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 0.1250 \n",
            "\n",
            "=> Training Epoch #3, LR=0.0010\n",
            "| Epoch [  3/ 60] Iter[  1/126]\t\tLoss: 0.1304 \n",
            "| Epoch [  3/ 60] Iter[  6/126]\t\tLoss: 3.5043 \n",
            "| Epoch [  3/ 60] Iter[ 11/126]\t\tLoss: 0.2470 \n",
            "| Epoch [  3/ 60] Iter[ 16/126]\t\tLoss: 0.3806 \n",
            "| Epoch [  3/ 60] Iter[ 21/126]\t\tLoss: 5.1453 \n",
            "| Epoch [  3/ 60] Iter[ 26/126]\t\tLoss: 0.5809 \n",
            "| Epoch [  3/ 60] Iter[ 31/126]\t\tLoss: 0.4534 \n",
            "| Epoch [  3/ 60] Iter[ 36/126]\t\tLoss: 0.8039 \n",
            "| Epoch [  3/ 60] Iter[ 41/126]\t\tLoss: 0.3888 \n",
            "| Epoch [  3/ 60] Iter[ 46/126]\t\tLoss: 0.0679 \n",
            "| Epoch [  3/ 60] Iter[ 51/126]\t\tLoss: 6.2021 \n",
            "| Epoch [  3/ 60] Iter[ 56/126]\t\tLoss: 0.6775 \n",
            "| Epoch [  3/ 60] Iter[ 61/126]\t\tLoss: 1.2571 \n",
            "| Epoch [  3/ 60] Iter[ 66/126]\t\tLoss: 0.7681 \n",
            "| Epoch [  3/ 60] Iter[ 71/126]\t\tLoss: 0.1274 \n",
            "| Epoch [  3/ 60] Iter[ 76/126]\t\tLoss: 0.5979 \n",
            "| Epoch [  3/ 60] Iter[ 81/126]\t\tLoss: 0.6213 \n",
            "| Epoch [  3/ 60] Iter[ 86/126]\t\tLoss: 0.8221 \n",
            "| Epoch [  3/ 60] Iter[ 91/126]\t\tLoss: 0.3351 \n",
            "| Epoch [  3/ 60] Iter[ 96/126]\t\tLoss: 1.3069 \n",
            "| Epoch [  3/ 60] Iter[101/126]\t\tLoss: 0.8418 \n",
            "| Epoch [  3/ 60] Iter[106/126]\t\tLoss: 0.3071 \n",
            "| Epoch [  3/ 60] Iter[111/126]\t\tLoss: 5.3167 \n",
            "| Epoch [  3/ 60] Iter[116/126]\t\tLoss: 0.3682 \n",
            "| Epoch [  3/ 60] Iter[121/126]\t\tLoss: 0.3190 \n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 0.1958 \n",
            "\n",
            "=> Training Epoch #4, LR=0.0010\n",
            "| Epoch [  4/ 60] Iter[  1/126]\t\tLoss: 0.4314 \n",
            "| Epoch [  4/ 60] Iter[  6/126]\t\tLoss: 1.7003 \n",
            "| Epoch [  4/ 60] Iter[ 11/126]\t\tLoss: 0.1219 \n",
            "| Epoch [  4/ 60] Iter[ 16/126]\t\tLoss: 0.6959 \n",
            "| Epoch [  4/ 60] Iter[ 21/126]\t\tLoss: 0.0962 \n",
            "| Epoch [  4/ 60] Iter[ 26/126]\t\tLoss: 0.3918 \n",
            "| Epoch [  4/ 60] Iter[ 31/126]\t\tLoss: 0.9554 \n",
            "| Epoch [  4/ 60] Iter[ 36/126]\t\tLoss: 1.2002 \n",
            "| Epoch [  4/ 60] Iter[ 41/126]\t\tLoss: 5.1557 \n",
            "| Epoch [  4/ 60] Iter[ 46/126]\t\tLoss: 6.8588 \n",
            "| Epoch [  4/ 60] Iter[ 51/126]\t\tLoss: 0.4354 \n",
            "| Epoch [  4/ 60] Iter[ 56/126]\t\tLoss: 0.7162 \n",
            "| Epoch [  4/ 60] Iter[ 61/126]\t\tLoss: 5.8195 \n",
            "| Epoch [  4/ 60] Iter[ 66/126]\t\tLoss: 6.2498 \n",
            "| Epoch [  4/ 60] Iter[ 71/126]\t\tLoss: 0.4911 \n",
            "| Epoch [  4/ 60] Iter[ 76/126]\t\tLoss: 0.2395 \n",
            "| Epoch [  4/ 60] Iter[ 81/126]\t\tLoss: 0.4029 \n",
            "| Epoch [  4/ 60] Iter[ 86/126]\t\tLoss: 0.8507 \n",
            "| Epoch [  4/ 60] Iter[ 91/126]\t\tLoss: 0.5837 \n",
            "| Epoch [  4/ 60] Iter[ 96/126]\t\tLoss: 0.6136 \n",
            "| Epoch [  4/ 60] Iter[101/126]\t\tLoss: 0.4059 \n",
            "| Epoch [  4/ 60] Iter[106/126]\t\tLoss: 1.3433 \n",
            "| Epoch [  4/ 60] Iter[111/126]\t\tLoss: 0.4616 \n",
            "| Epoch [  4/ 60] Iter[116/126]\t\tLoss: 0.1761 \n",
            "| Epoch [  4/ 60] Iter[121/126]\t\tLoss: 1.4493 \n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 0.2784 \n",
            "\n",
            "=> Training Epoch #5, LR=0.0010\n",
            "| Epoch [  5/ 60] Iter[  1/126]\t\tLoss: 0.4409 \n",
            "| Epoch [  5/ 60] Iter[  6/126]\t\tLoss: 0.9828 \n",
            "| Epoch [  5/ 60] Iter[ 11/126]\t\tLoss: 0.6291 \n",
            "| Epoch [  5/ 60] Iter[ 16/126]\t\tLoss: 1.0686 \n",
            "| Epoch [  5/ 60] Iter[ 21/126]\t\tLoss: 1.0065 \n",
            "| Epoch [  5/ 60] Iter[ 26/126]\t\tLoss: 0.7474 \n",
            "| Epoch [  5/ 60] Iter[ 31/126]\t\tLoss: 0.8197 \n",
            "| Epoch [  5/ 60] Iter[ 36/126]\t\tLoss: 0.6014 \n",
            "| Epoch [  5/ 60] Iter[ 41/126]\t\tLoss: 0.4351 \n",
            "| Epoch [  5/ 60] Iter[ 46/126]\t\tLoss: 0.4289 \n",
            "| Epoch [  5/ 60] Iter[ 51/126]\t\tLoss: 0.4851 \n",
            "| Epoch [  5/ 60] Iter[ 56/126]\t\tLoss: 0.6964 \n",
            "| Epoch [  5/ 60] Iter[ 61/126]\t\tLoss: 0.2009 \n",
            "| Epoch [  5/ 60] Iter[ 66/126]\t\tLoss: 0.7099 \n",
            "| Epoch [  5/ 60] Iter[ 71/126]\t\tLoss: 1.7770 \n",
            "| Epoch [  5/ 60] Iter[ 76/126]\t\tLoss: 6.1585 \n",
            "| Epoch [  5/ 60] Iter[ 81/126]\t\tLoss: 0.4115 \n",
            "| Epoch [  5/ 60] Iter[ 86/126]\t\tLoss: 5.7904 \n",
            "| Epoch [  5/ 60] Iter[ 91/126]\t\tLoss: 0.2010 \n",
            "| Epoch [  5/ 60] Iter[ 96/126]\t\tLoss: 0.6115 \n",
            "| Epoch [  5/ 60] Iter[101/126]\t\tLoss: 1.5348 \n",
            "| Epoch [  5/ 60] Iter[106/126]\t\tLoss: 0.1582 \n",
            "| Epoch [  5/ 60] Iter[111/126]\t\tLoss: 0.1243 \n",
            "| Epoch [  5/ 60] Iter[116/126]\t\tLoss: 0.1515 \n",
            "| Epoch [  5/ 60] Iter[121/126]\t\tLoss: 0.4611 \n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 0.1818 \n",
            "\n",
            "=> Training Epoch #6, LR=0.0010\n",
            "| Epoch [  6/ 60] Iter[  1/126]\t\tLoss: 5.5994 \n",
            "| Epoch [  6/ 60] Iter[  6/126]\t\tLoss: 1.1035 \n",
            "| Epoch [  6/ 60] Iter[ 11/126]\t\tLoss: 1.0239 \n",
            "| Epoch [  6/ 60] Iter[ 16/126]\t\tLoss: 0.4470 \n",
            "| Epoch [  6/ 60] Iter[ 21/126]\t\tLoss: 0.8726 \n",
            "| Epoch [  6/ 60] Iter[ 26/126]\t\tLoss: 0.2219 \n",
            "| Epoch [  6/ 60] Iter[ 31/126]\t\tLoss: 0.6285 \n",
            "| Epoch [  6/ 60] Iter[ 36/126]\t\tLoss: 1.5658 \n",
            "| Epoch [  6/ 60] Iter[ 41/126]\t\tLoss: 0.3654 \n",
            "| Epoch [  6/ 60] Iter[ 46/126]\t\tLoss: 6.2541 \n",
            "| Epoch [  6/ 60] Iter[ 51/126]\t\tLoss: 0.4364 \n",
            "| Epoch [  6/ 60] Iter[ 56/126]\t\tLoss: 0.8055 \n",
            "| Epoch [  6/ 60] Iter[ 61/126]\t\tLoss: 0.3580 \n",
            "| Epoch [  6/ 60] Iter[ 66/126]\t\tLoss: 0.0733 \n",
            "| Epoch [  6/ 60] Iter[ 71/126]\t\tLoss: 1.0380 \n",
            "| Epoch [  6/ 60] Iter[ 76/126]\t\tLoss: 0.6674 \n",
            "| Epoch [  6/ 60] Iter[ 81/126]\t\tLoss: 0.3378 \n",
            "| Epoch [  6/ 60] Iter[ 86/126]\t\tLoss: 0.4005 \n",
            "| Epoch [  6/ 60] Iter[ 91/126]\t\tLoss: 6.1294 \n",
            "| Epoch [  6/ 60] Iter[ 96/126]\t\tLoss: 0.2406 \n",
            "| Epoch [  6/ 60] Iter[101/126]\t\tLoss: 0.1048 \n",
            "| Epoch [  6/ 60] Iter[106/126]\t\tLoss: 0.5232 \n",
            "| Epoch [  6/ 60] Iter[111/126]\t\tLoss: 0.7229 \n",
            "| Epoch [  6/ 60] Iter[116/126]\t\tLoss: 0.3320 \n",
            "| Epoch [  6/ 60] Iter[121/126]\t\tLoss: 0.2783 \n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 0.1974 \n",
            "\n",
            "=> Training Epoch #7, LR=0.0010\n",
            "| Epoch [  7/ 60] Iter[  1/126]\t\tLoss: 6.3259 \n",
            "| Epoch [  7/ 60] Iter[  6/126]\t\tLoss: 0.4993 \n",
            "| Epoch [  7/ 60] Iter[ 11/126]\t\tLoss: 0.1676 \n",
            "| Epoch [  7/ 60] Iter[ 16/126]\t\tLoss: 0.8753 \n",
            "| Epoch [  7/ 60] Iter[ 21/126]\t\tLoss: 0.4435 \n",
            "| Epoch [  7/ 60] Iter[ 26/126]\t\tLoss: 0.7025 \n",
            "| Epoch [  7/ 60] Iter[ 31/126]\t\tLoss: 0.1716 \n",
            "| Epoch [  7/ 60] Iter[ 36/126]\t\tLoss: 5.5259 \n",
            "| Epoch [  7/ 60] Iter[ 41/126]\t\tLoss: 0.2921 \n",
            "| Epoch [  7/ 60] Iter[ 46/126]\t\tLoss: 0.4430 \n",
            "| Epoch [  7/ 60] Iter[ 51/126]\t\tLoss: 1.4876 \n",
            "| Epoch [  7/ 60] Iter[ 56/126]\t\tLoss: 0.5849 \n",
            "| Epoch [  7/ 60] Iter[ 61/126]\t\tLoss: 0.5171 \n",
            "| Epoch [  7/ 60] Iter[ 66/126]\t\tLoss: 0.7790 \n",
            "| Epoch [  7/ 60] Iter[ 71/126]\t\tLoss: 1.0019 \n",
            "| Epoch [  7/ 60] Iter[ 76/126]\t\tLoss: 0.4801 \n",
            "| Epoch [  7/ 60] Iter[ 81/126]\t\tLoss: 0.6858 \n",
            "| Epoch [  7/ 60] Iter[ 86/126]\t\tLoss: 0.5307 \n",
            "| Epoch [  7/ 60] Iter[ 91/126]\t\tLoss: 0.6580 \n",
            "| Epoch [  7/ 60] Iter[ 96/126]\t\tLoss: 1.0879 \n",
            "| Epoch [  7/ 60] Iter[101/126]\t\tLoss: 0.2098 \n",
            "| Epoch [  7/ 60] Iter[106/126]\t\tLoss: 0.3246 \n",
            "| Epoch [  7/ 60] Iter[111/126]\t\tLoss: 0.5355 \n",
            "| Epoch [  7/ 60] Iter[116/126]\t\tLoss: 0.7155 \n",
            "| Epoch [  7/ 60] Iter[121/126]\t\tLoss: 0.1645 \n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 0.2032 \n",
            "\n",
            "=> Training Epoch #8, LR=0.0010\n",
            "| Epoch [  8/ 60] Iter[  1/126]\t\tLoss: 0.4176 \n",
            "| Epoch [  8/ 60] Iter[  6/126]\t\tLoss: 0.4672 \n",
            "| Epoch [  8/ 60] Iter[ 11/126]\t\tLoss: 0.6065 \n",
            "| Epoch [  8/ 60] Iter[ 16/126]\t\tLoss: 0.1417 \n",
            "| Epoch [  8/ 60] Iter[ 21/126]\t\tLoss: 0.0591 \n",
            "| Epoch [  8/ 60] Iter[ 26/126]\t\tLoss: 0.2487 \n",
            "| Epoch [  8/ 60] Iter[ 31/126]\t\tLoss: 0.6038 \n",
            "| Epoch [  8/ 60] Iter[ 36/126]\t\tLoss: 0.7715 \n",
            "| Epoch [  8/ 60] Iter[ 41/126]\t\tLoss: 0.7278 \n",
            "| Epoch [  8/ 60] Iter[ 46/126]\t\tLoss: 1.2954 \n",
            "| Epoch [  8/ 60] Iter[ 51/126]\t\tLoss: 0.5955 \n",
            "| Epoch [  8/ 60] Iter[ 56/126]\t\tLoss: 0.4937 \n",
            "| Epoch [  8/ 60] Iter[ 61/126]\t\tLoss: 0.2815 \n",
            "| Epoch [  8/ 60] Iter[ 66/126]\t\tLoss: 3.6689 \n",
            "| Epoch [  8/ 60] Iter[ 71/126]\t\tLoss: 0.3886 \n",
            "| Epoch [  8/ 60] Iter[ 76/126]\t\tLoss: 0.6389 \n",
            "| Epoch [  8/ 60] Iter[ 81/126]\t\tLoss: 0.1420 \n",
            "| Epoch [  8/ 60] Iter[ 86/126]\t\tLoss: 0.6798 \n",
            "| Epoch [  8/ 60] Iter[ 91/126]\t\tLoss: 5.7036 \n",
            "| Epoch [  8/ 60] Iter[ 96/126]\t\tLoss: 0.4126 \n",
            "| Epoch [  8/ 60] Iter[101/126]\t\tLoss: 0.0201 \n",
            "| Epoch [  8/ 60] Iter[106/126]\t\tLoss: 0.4782 \n",
            "| Epoch [  8/ 60] Iter[111/126]\t\tLoss: 0.4547 \n",
            "| Epoch [  8/ 60] Iter[116/126]\t\tLoss: 0.6507 \n",
            "| Epoch [  8/ 60] Iter[121/126]\t\tLoss: 0.1220 \n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 0.2336 \n",
            "\n",
            "=> Training Epoch #9, LR=0.0010\n",
            "| Epoch [  9/ 60] Iter[  1/126]\t\tLoss: 5.8501 \n",
            "| Epoch [  9/ 60] Iter[  6/126]\t\tLoss: 0.1843 \n",
            "| Epoch [  9/ 60] Iter[ 11/126]\t\tLoss: 1.3881 \n",
            "| Epoch [  9/ 60] Iter[ 16/126]\t\tLoss: 5.9099 \n",
            "| Epoch [  9/ 60] Iter[ 21/126]\t\tLoss: 1.2197 \n",
            "| Epoch [  9/ 60] Iter[ 26/126]\t\tLoss: 0.2278 \n",
            "| Epoch [  9/ 60] Iter[ 31/126]\t\tLoss: 0.5531 \n",
            "| Epoch [  9/ 60] Iter[ 36/126]\t\tLoss: 0.3824 \n",
            "| Epoch [  9/ 60] Iter[ 41/126]\t\tLoss: 0.6138 \n",
            "| Epoch [  9/ 60] Iter[ 46/126]\t\tLoss: 1.5120 \n",
            "| Epoch [  9/ 60] Iter[ 51/126]\t\tLoss: 0.1944 \n",
            "| Epoch [  9/ 60] Iter[ 56/126]\t\tLoss: 1.4358 \n",
            "| Epoch [  9/ 60] Iter[ 61/126]\t\tLoss: 0.5776 \n",
            "| Epoch [  9/ 60] Iter[ 66/126]\t\tLoss: 0.6116 \n",
            "| Epoch [  9/ 60] Iter[ 71/126]\t\tLoss: 0.8484 \n",
            "| Epoch [  9/ 60] Iter[ 76/126]\t\tLoss: 0.5115 \n",
            "| Epoch [  9/ 60] Iter[ 81/126]\t\tLoss: 0.7557 \n",
            "| Epoch [  9/ 60] Iter[ 86/126]\t\tLoss: 0.6900 \n",
            "| Epoch [  9/ 60] Iter[ 91/126]\t\tLoss: 6.1546 \n",
            "| Epoch [  9/ 60] Iter[ 96/126]\t\tLoss: 0.1124 \n",
            "| Epoch [  9/ 60] Iter[101/126]\t\tLoss: 0.6783 \n",
            "| Epoch [  9/ 60] Iter[106/126]\t\tLoss: 0.3087 \n",
            "| Epoch [  9/ 60] Iter[111/126]\t\tLoss: 0.4070 \n",
            "| Epoch [  9/ 60] Iter[116/126]\t\tLoss: 5.8295 \n",
            "| Epoch [  9/ 60] Iter[121/126]\t\tLoss: 0.2973 \n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 0.1610 \n",
            "\n",
            "=> Training Epoch #10, LR=0.0010\n",
            "| Epoch [ 10/ 60] Iter[  1/126]\t\tLoss: 0.1441 \n",
            "| Epoch [ 10/ 60] Iter[  6/126]\t\tLoss: 6.4499 \n",
            "| Epoch [ 10/ 60] Iter[ 11/126]\t\tLoss: 1.0322 \n",
            "| Epoch [ 10/ 60] Iter[ 16/126]\t\tLoss: 0.5188 \n",
            "| Epoch [ 10/ 60] Iter[ 21/126]\t\tLoss: 1.5006 \n",
            "| Epoch [ 10/ 60] Iter[ 26/126]\t\tLoss: 2.6578 \n",
            "| Epoch [ 10/ 60] Iter[ 31/126]\t\tLoss: 0.7518 \n",
            "| Epoch [ 10/ 60] Iter[ 36/126]\t\tLoss: 1.1334 \n",
            "| Epoch [ 10/ 60] Iter[ 41/126]\t\tLoss: 0.4039 \n",
            "| Epoch [ 10/ 60] Iter[ 46/126]\t\tLoss: 0.4974 \n",
            "| Epoch [ 10/ 60] Iter[ 51/126]\t\tLoss: 1.3040 \n",
            "| Epoch [ 10/ 60] Iter[ 56/126]\t\tLoss: 4.1857 \n",
            "| Epoch [ 10/ 60] Iter[ 61/126]\t\tLoss: 0.2443 \n",
            "| Epoch [ 10/ 60] Iter[ 66/126]\t\tLoss: 0.0268 \n",
            "| Epoch [ 10/ 60] Iter[ 71/126]\t\tLoss: 0.2509 \n",
            "| Epoch [ 10/ 60] Iter[ 76/126]\t\tLoss: 0.2855 \n",
            "| Epoch [ 10/ 60] Iter[ 81/126]\t\tLoss: 0.1821 \n",
            "| Epoch [ 10/ 60] Iter[ 86/126]\t\tLoss: 0.0275 \n",
            "| Epoch [ 10/ 60] Iter[ 91/126]\t\tLoss: 0.2203 \n",
            "| Epoch [ 10/ 60] Iter[ 96/126]\t\tLoss: 6.2808 \n",
            "| Epoch [ 10/ 60] Iter[101/126]\t\tLoss: 6.1096 \n",
            "| Epoch [ 10/ 60] Iter[106/126]\t\tLoss: 0.6593 \n",
            "| Epoch [ 10/ 60] Iter[111/126]\t\tLoss: 0.2834 \n",
            "| Epoch [ 10/ 60] Iter[116/126]\t\tLoss: 1.3382 \n",
            "| Epoch [ 10/ 60] Iter[121/126]\t\tLoss: 0.3176 \n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 0.2308 \n",
            "\n",
            "=> Training Epoch #11, LR=0.0001\n",
            "| Epoch [ 11/ 60] Iter[  1/126]\t\tLoss: 0.3721 \n",
            "| Epoch [ 11/ 60] Iter[  6/126]\t\tLoss: 0.5634 \n",
            "| Epoch [ 11/ 60] Iter[ 11/126]\t\tLoss: 0.1638 \n",
            "| Epoch [ 11/ 60] Iter[ 16/126]\t\tLoss: 0.2021 \n",
            "| Epoch [ 11/ 60] Iter[ 21/126]\t\tLoss: 0.3730 \n",
            "| Epoch [ 11/ 60] Iter[ 26/126]\t\tLoss: 0.3070 \n",
            "| Epoch [ 11/ 60] Iter[ 31/126]\t\tLoss: 0.3416 \n",
            "| Epoch [ 11/ 60] Iter[ 36/126]\t\tLoss: 0.9295 \n",
            "| Epoch [ 11/ 60] Iter[ 41/126]\t\tLoss: 2.2317 \n",
            "| Epoch [ 11/ 60] Iter[ 46/126]\t\tLoss: 0.8857 \n",
            "| Epoch [ 11/ 60] Iter[ 51/126]\t\tLoss: 5.5997 \n",
            "| Epoch [ 11/ 60] Iter[ 56/126]\t\tLoss: 0.8626 \n",
            "| Epoch [ 11/ 60] Iter[ 61/126]\t\tLoss: 5.5029 \n",
            "| Epoch [ 11/ 60] Iter[ 66/126]\t\tLoss: 5.9086 \n",
            "| Epoch [ 11/ 60] Iter[ 71/126]\t\tLoss: 1.9246 \n",
            "| Epoch [ 11/ 60] Iter[ 76/126]\t\tLoss: 0.1851 \n",
            "| Epoch [ 11/ 60] Iter[ 81/126]\t\tLoss: 5.3689 \n",
            "| Epoch [ 11/ 60] Iter[ 86/126]\t\tLoss: 0.3441 \n",
            "| Epoch [ 11/ 60] Iter[ 91/126]\t\tLoss: 0.1307 \n",
            "| Epoch [ 11/ 60] Iter[ 96/126]\t\tLoss: 0.4301 \n",
            "| Epoch [ 11/ 60] Iter[101/126]\t\tLoss: 5.4700 \n",
            "| Epoch [ 11/ 60] Iter[106/126]\t\tLoss: 1.5590 \n",
            "| Epoch [ 11/ 60] Iter[111/126]\t\tLoss: 0.3922 \n",
            "| Epoch [ 11/ 60] Iter[116/126]\t\tLoss: 0.5569 \n",
            "| Epoch [ 11/ 60] Iter[121/126]\t\tLoss: 1.7163 \n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 0.2310 \n",
            "\n",
            "=> Training Epoch #12, LR=0.0001\n",
            "| Epoch [ 12/ 60] Iter[  1/126]\t\tLoss: 1.9354 \n",
            "| Epoch [ 12/ 60] Iter[  6/126]\t\tLoss: 0.8637 \n",
            "| Epoch [ 12/ 60] Iter[ 11/126]\t\tLoss: 3.6315 \n",
            "| Epoch [ 12/ 60] Iter[ 16/126]\t\tLoss: 0.7733 \n",
            "| Epoch [ 12/ 60] Iter[ 21/126]\t\tLoss: 0.7490 \n",
            "| Epoch [ 12/ 60] Iter[ 26/126]\t\tLoss: 0.7920 \n",
            "| Epoch [ 12/ 60] Iter[ 31/126]\t\tLoss: 0.7297 \n",
            "| Epoch [ 12/ 60] Iter[ 36/126]\t\tLoss: 0.3895 \n",
            "| Epoch [ 12/ 60] Iter[ 41/126]\t\tLoss: 2.1571 \n",
            "| Epoch [ 12/ 60] Iter[ 46/126]\t\tLoss: 0.4746 \n",
            "| Epoch [ 12/ 60] Iter[ 51/126]\t\tLoss: 2.7396 \n",
            "| Epoch [ 12/ 60] Iter[ 56/126]\t\tLoss: 0.2993 \n",
            "| Epoch [ 12/ 60] Iter[ 61/126]\t\tLoss: 0.6084 \n",
            "| Epoch [ 12/ 60] Iter[ 66/126]\t\tLoss: 0.3557 \n",
            "| Epoch [ 12/ 60] Iter[ 71/126]\t\tLoss: 0.2461 \n",
            "| Epoch [ 12/ 60] Iter[ 76/126]\t\tLoss: 0.3015 \n",
            "| Epoch [ 12/ 60] Iter[ 81/126]\t\tLoss: 0.4724 \n",
            "| Epoch [ 12/ 60] Iter[ 86/126]\t\tLoss: 0.4331 \n",
            "| Epoch [ 12/ 60] Iter[ 91/126]\t\tLoss: 0.2661 \n",
            "| Epoch [ 12/ 60] Iter[ 96/126]\t\tLoss: 0.4380 \n",
            "| Epoch [ 12/ 60] Iter[101/126]\t\tLoss: 0.1480 \n",
            "| Epoch [ 12/ 60] Iter[106/126]\t\tLoss: 0.1044 \n",
            "| Epoch [ 12/ 60] Iter[111/126]\t\tLoss: 0.8303 \n",
            "| Epoch [ 12/ 60] Iter[116/126]\t\tLoss: 1.4377 \n",
            "| Epoch [ 12/ 60] Iter[121/126]\t\tLoss: 1.5327 \n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 0.2293 \n",
            "\n",
            "=> Training Epoch #13, LR=0.0001\n",
            "| Epoch [ 13/ 60] Iter[  1/126]\t\tLoss: 0.9029 \n",
            "| Epoch [ 13/ 60] Iter[  6/126]\t\tLoss: 0.4194 \n",
            "| Epoch [ 13/ 60] Iter[ 11/126]\t\tLoss: 1.3257 \n",
            "| Epoch [ 13/ 60] Iter[ 16/126]\t\tLoss: 0.5612 \n",
            "| Epoch [ 13/ 60] Iter[ 21/126]\t\tLoss: 5.2013 \n",
            "| Epoch [ 13/ 60] Iter[ 26/126]\t\tLoss: 0.3900 \n",
            "| Epoch [ 13/ 60] Iter[ 31/126]\t\tLoss: 0.7793 \n",
            "| Epoch [ 13/ 60] Iter[ 36/126]\t\tLoss: 3.6273 \n",
            "| Epoch [ 13/ 60] Iter[ 41/126]\t\tLoss: 0.4852 \n",
            "| Epoch [ 13/ 60] Iter[ 46/126]\t\tLoss: 0.9882 \n",
            "| Epoch [ 13/ 60] Iter[ 51/126]\t\tLoss: 0.6078 \n",
            "| Epoch [ 13/ 60] Iter[ 56/126]\t\tLoss: 0.2251 \n",
            "| Epoch [ 13/ 60] Iter[ 61/126]\t\tLoss: 0.5904 \n",
            "| Epoch [ 13/ 60] Iter[ 66/126]\t\tLoss: 0.4751 \n",
            "| Epoch [ 13/ 60] Iter[ 71/126]\t\tLoss: 0.3142 \n",
            "| Epoch [ 13/ 60] Iter[ 76/126]\t\tLoss: 0.8575 \n",
            "| Epoch [ 13/ 60] Iter[ 81/126]\t\tLoss: 0.3917 \n",
            "| Epoch [ 13/ 60] Iter[ 86/126]\t\tLoss: 0.6442 \n",
            "| Epoch [ 13/ 60] Iter[ 91/126]\t\tLoss: 0.7845 \n",
            "| Epoch [ 13/ 60] Iter[ 96/126]\t\tLoss: 1.8655 \n",
            "| Epoch [ 13/ 60] Iter[101/126]\t\tLoss: 0.1382 \n",
            "| Epoch [ 13/ 60] Iter[106/126]\t\tLoss: 0.6534 \n",
            "| Epoch [ 13/ 60] Iter[111/126]\t\tLoss: 6.0763 \n",
            "| Epoch [ 13/ 60] Iter[116/126]\t\tLoss: 0.4794 \n",
            "| Epoch [ 13/ 60] Iter[121/126]\t\tLoss: 0.2660 \n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 0.2317 \n",
            "\n",
            "=> Training Epoch #14, LR=0.0001\n",
            "| Epoch [ 14/ 60] Iter[  1/126]\t\tLoss: 0.2553 \n",
            "| Epoch [ 14/ 60] Iter[  6/126]\t\tLoss: 6.1772 \n",
            "| Epoch [ 14/ 60] Iter[ 11/126]\t\tLoss: 0.7253 \n",
            "| Epoch [ 14/ 60] Iter[ 16/126]\t\tLoss: 2.4832 \n",
            "| Epoch [ 14/ 60] Iter[ 21/126]\t\tLoss: 0.2235 \n",
            "| Epoch [ 14/ 60] Iter[ 26/126]\t\tLoss: 0.2212 \n",
            "| Epoch [ 14/ 60] Iter[ 31/126]\t\tLoss: 1.4429 \n",
            "| Epoch [ 14/ 60] Iter[ 36/126]\t\tLoss: 0.2559 \n",
            "| Epoch [ 14/ 60] Iter[ 41/126]\t\tLoss: 5.5745 \n",
            "| Epoch [ 14/ 60] Iter[ 46/126]\t\tLoss: 0.5313 \n",
            "| Epoch [ 14/ 60] Iter[ 51/126]\t\tLoss: 0.3052 \n",
            "| Epoch [ 14/ 60] Iter[ 56/126]\t\tLoss: 0.3022 \n",
            "| Epoch [ 14/ 60] Iter[ 61/126]\t\tLoss: 2.8694 \n",
            "| Epoch [ 14/ 60] Iter[ 66/126]\t\tLoss: 0.1604 \n",
            "| Epoch [ 14/ 60] Iter[ 71/126]\t\tLoss: 0.6373 \n",
            "| Epoch [ 14/ 60] Iter[ 76/126]\t\tLoss: 0.4741 \n",
            "| Epoch [ 14/ 60] Iter[ 81/126]\t\tLoss: 0.3151 \n",
            "| Epoch [ 14/ 60] Iter[ 86/126]\t\tLoss: 0.4319 \n",
            "| Epoch [ 14/ 60] Iter[ 91/126]\t\tLoss: 0.3553 \n",
            "| Epoch [ 14/ 60] Iter[ 96/126]\t\tLoss: 0.1841 \n",
            "| Epoch [ 14/ 60] Iter[101/126]\t\tLoss: 1.0348 \n",
            "| Epoch [ 14/ 60] Iter[106/126]\t\tLoss: 5.7300 \n",
            "| Epoch [ 14/ 60] Iter[111/126]\t\tLoss: 1.7861 \n",
            "| Epoch [ 14/ 60] Iter[116/126]\t\tLoss: 0.3763 \n",
            "| Epoch [ 14/ 60] Iter[121/126]\t\tLoss: 0.7798 \n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 0.2340 \n",
            "\n",
            "=> Training Epoch #15, LR=0.0001\n",
            "| Epoch [ 15/ 60] Iter[  1/126]\t\tLoss: 1.3576 \n",
            "| Epoch [ 15/ 60] Iter[  6/126]\t\tLoss: 0.8920 \n",
            "| Epoch [ 15/ 60] Iter[ 11/126]\t\tLoss: 5.4670 \n",
            "| Epoch [ 15/ 60] Iter[ 16/126]\t\tLoss: 1.0655 \n",
            "| Epoch [ 15/ 60] Iter[ 21/126]\t\tLoss: 0.7038 \n",
            "| Epoch [ 15/ 60] Iter[ 26/126]\t\tLoss: 1.0408 \n",
            "| Epoch [ 15/ 60] Iter[ 31/126]\t\tLoss: 6.1148 \n",
            "| Epoch [ 15/ 60] Iter[ 36/126]\t\tLoss: 0.5528 \n",
            "| Epoch [ 15/ 60] Iter[ 41/126]\t\tLoss: 0.3803 \n",
            "| Epoch [ 15/ 60] Iter[ 46/126]\t\tLoss: 0.9250 \n",
            "| Epoch [ 15/ 60] Iter[ 51/126]\t\tLoss: 0.6671 \n",
            "| Epoch [ 15/ 60] Iter[ 56/126]\t\tLoss: 0.8307 \n",
            "| Epoch [ 15/ 60] Iter[ 61/126]\t\tLoss: 0.8942 \n",
            "| Epoch [ 15/ 60] Iter[ 66/126]\t\tLoss: 6.4269 \n",
            "| Epoch [ 15/ 60] Iter[ 71/126]\t\tLoss: 0.4619 \n",
            "| Epoch [ 15/ 60] Iter[ 76/126]\t\tLoss: 0.3620 \n",
            "| Epoch [ 15/ 60] Iter[ 81/126]\t\tLoss: 0.0941 \n",
            "| Epoch [ 15/ 60] Iter[ 86/126]\t\tLoss: 0.5362 \n",
            "| Epoch [ 15/ 60] Iter[ 91/126]\t\tLoss: 0.3234 \n",
            "| Epoch [ 15/ 60] Iter[ 96/126]\t\tLoss: 0.5526 \n",
            "| Epoch [ 15/ 60] Iter[101/126]\t\tLoss: 5.4419 \n",
            "| Epoch [ 15/ 60] Iter[106/126]\t\tLoss: 0.2792 \n",
            "| Epoch [ 15/ 60] Iter[111/126]\t\tLoss: 0.2621 \n",
            "| Epoch [ 15/ 60] Iter[116/126]\t\tLoss: 6.6167 \n",
            "| Epoch [ 15/ 60] Iter[121/126]\t\tLoss: 5.7643 \n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 0.2335 \n",
            "\n",
            "=> Training Epoch #16, LR=0.0001\n",
            "| Epoch [ 16/ 60] Iter[  1/126]\t\tLoss: 0.4235 \n",
            "| Epoch [ 16/ 60] Iter[  6/126]\t\tLoss: 0.7576 \n",
            "| Epoch [ 16/ 60] Iter[ 11/126]\t\tLoss: 0.5511 \n",
            "| Epoch [ 16/ 60] Iter[ 16/126]\t\tLoss: 0.8581 \n",
            "| Epoch [ 16/ 60] Iter[ 21/126]\t\tLoss: 0.3683 \n",
            "| Epoch [ 16/ 60] Iter[ 26/126]\t\tLoss: 0.6634 \n",
            "| Epoch [ 16/ 60] Iter[ 31/126]\t\tLoss: 0.3682 \n",
            "| Epoch [ 16/ 60] Iter[ 36/126]\t\tLoss: 0.4394 \n",
            "| Epoch [ 16/ 60] Iter[ 41/126]\t\tLoss: 0.6929 \n",
            "| Epoch [ 16/ 60] Iter[ 46/126]\t\tLoss: 0.5527 \n",
            "| Epoch [ 16/ 60] Iter[ 51/126]\t\tLoss: 0.8805 \n",
            "| Epoch [ 16/ 60] Iter[ 56/126]\t\tLoss: 0.0607 \n",
            "| Epoch [ 16/ 60] Iter[ 61/126]\t\tLoss: 0.5252 \n",
            "| Epoch [ 16/ 60] Iter[ 66/126]\t\tLoss: 0.2595 \n",
            "| Epoch [ 16/ 60] Iter[ 71/126]\t\tLoss: 1.2392 \n",
            "| Epoch [ 16/ 60] Iter[ 76/126]\t\tLoss: 1.0784 \n",
            "| Epoch [ 16/ 60] Iter[ 81/126]\t\tLoss: 0.5120 \n",
            "| Epoch [ 16/ 60] Iter[ 86/126]\t\tLoss: 0.3848 \n",
            "| Epoch [ 16/ 60] Iter[ 91/126]\t\tLoss: 0.7005 \n",
            "| Epoch [ 16/ 60] Iter[ 96/126]\t\tLoss: 0.3051 \n",
            "| Epoch [ 16/ 60] Iter[101/126]\t\tLoss: 0.4345 \n",
            "| Epoch [ 16/ 60] Iter[106/126]\t\tLoss: 0.1950 \n",
            "| Epoch [ 16/ 60] Iter[111/126]\t\tLoss: 0.8430 \n",
            "| Epoch [ 16/ 60] Iter[116/126]\t\tLoss: 0.4892 \n",
            "| Epoch [ 16/ 60] Iter[121/126]\t\tLoss: 0.2313 \n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 0.2330 \n",
            "\n",
            "=> Training Epoch #17, LR=0.0001\n",
            "| Epoch [ 17/ 60] Iter[  1/126]\t\tLoss: 0.6970 \n",
            "| Epoch [ 17/ 60] Iter[  6/126]\t\tLoss: 0.6411 \n",
            "| Epoch [ 17/ 60] Iter[ 11/126]\t\tLoss: 0.4471 \n",
            "| Epoch [ 17/ 60] Iter[ 16/126]\t\tLoss: 0.3584 \n",
            "| Epoch [ 17/ 60] Iter[ 21/126]\t\tLoss: 1.4645 \n",
            "| Epoch [ 17/ 60] Iter[ 26/126]\t\tLoss: 0.4641 \n",
            "| Epoch [ 17/ 60] Iter[ 31/126]\t\tLoss: 1.1030 \n",
            "| Epoch [ 17/ 60] Iter[ 36/126]\t\tLoss: 0.5397 \n",
            "| Epoch [ 17/ 60] Iter[ 41/126]\t\tLoss: 0.7464 \n",
            "| Epoch [ 17/ 60] Iter[ 46/126]\t\tLoss: 1.0743 \n",
            "| Epoch [ 17/ 60] Iter[ 51/126]\t\tLoss: 0.5454 \n",
            "| Epoch [ 17/ 60] Iter[ 56/126]\t\tLoss: 0.2486 \n",
            "| Epoch [ 17/ 60] Iter[ 61/126]\t\tLoss: 0.6031 \n",
            "| Epoch [ 17/ 60] Iter[ 66/126]\t\tLoss: 0.3293 \n",
            "| Epoch [ 17/ 60] Iter[ 71/126]\t\tLoss: 6.0394 \n",
            "| Epoch [ 17/ 60] Iter[ 76/126]\t\tLoss: 0.3887 \n",
            "| Epoch [ 17/ 60] Iter[ 81/126]\t\tLoss: 0.0334 \n",
            "| Epoch [ 17/ 60] Iter[ 86/126]\t\tLoss: 0.5168 \n",
            "| Epoch [ 17/ 60] Iter[ 91/126]\t\tLoss: 0.2939 \n",
            "| Epoch [ 17/ 60] Iter[ 96/126]\t\tLoss: 0.3594 \n",
            "| Epoch [ 17/ 60] Iter[101/126]\t\tLoss: 0.3567 \n",
            "| Epoch [ 17/ 60] Iter[106/126]\t\tLoss: 0.8234 \n",
            "| Epoch [ 17/ 60] Iter[111/126]\t\tLoss: 0.4808 \n",
            "| Epoch [ 17/ 60] Iter[116/126]\t\tLoss: 0.4495 \n",
            "| Epoch [ 17/ 60] Iter[121/126]\t\tLoss: 0.4245 \n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 0.2285 \n",
            "\n",
            "=> Training Epoch #18, LR=0.0001\n",
            "| Epoch [ 18/ 60] Iter[  1/126]\t\tLoss: 0.0955 \n",
            "| Epoch [ 18/ 60] Iter[  6/126]\t\tLoss: 0.6071 \n",
            "| Epoch [ 18/ 60] Iter[ 11/126]\t\tLoss: 0.4707 \n",
            "| Epoch [ 18/ 60] Iter[ 16/126]\t\tLoss: 1.3555 \n",
            "| Epoch [ 18/ 60] Iter[ 21/126]\t\tLoss: 0.0948 \n",
            "| Epoch [ 18/ 60] Iter[ 26/126]\t\tLoss: 0.6211 \n",
            "| Epoch [ 18/ 60] Iter[ 31/126]\t\tLoss: 0.2929 \n",
            "| Epoch [ 18/ 60] Iter[ 36/126]\t\tLoss: 0.5361 \n",
            "| Epoch [ 18/ 60] Iter[ 41/126]\t\tLoss: 0.6614 \n",
            "| Epoch [ 18/ 60] Iter[ 46/126]\t\tLoss: 0.6911 \n",
            "| Epoch [ 18/ 60] Iter[ 51/126]\t\tLoss: 1.0325 \n",
            "| Epoch [ 18/ 60] Iter[ 56/126]\t\tLoss: 3.5386 \n",
            "| Epoch [ 18/ 60] Iter[ 61/126]\t\tLoss: 0.4693 \n",
            "| Epoch [ 18/ 60] Iter[ 66/126]\t\tLoss: 0.2103 \n",
            "| Epoch [ 18/ 60] Iter[ 71/126]\t\tLoss: 0.0753 \n",
            "| Epoch [ 18/ 60] Iter[ 76/126]\t\tLoss: 0.5197 \n",
            "| Epoch [ 18/ 60] Iter[ 81/126]\t\tLoss: 0.7465 \n",
            "| Epoch [ 18/ 60] Iter[ 86/126]\t\tLoss: 0.7432 \n",
            "| Epoch [ 18/ 60] Iter[ 91/126]\t\tLoss: 0.7556 \n",
            "| Epoch [ 18/ 60] Iter[ 96/126]\t\tLoss: 0.2455 \n",
            "| Epoch [ 18/ 60] Iter[101/126]\t\tLoss: 0.4077 \n",
            "| Epoch [ 18/ 60] Iter[106/126]\t\tLoss: 0.9802 \n",
            "| Epoch [ 18/ 60] Iter[111/126]\t\tLoss: 5.8058 \n",
            "| Epoch [ 18/ 60] Iter[116/126]\t\tLoss: 0.5233 \n",
            "| Epoch [ 18/ 60] Iter[121/126]\t\tLoss: 0.6985 \n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 0.2245 \n",
            "\n",
            "=> Training Epoch #19, LR=0.0001\n",
            "| Epoch [ 19/ 60] Iter[  1/126]\t\tLoss: 6.2653 \n",
            "| Epoch [ 19/ 60] Iter[  6/126]\t\tLoss: 0.4134 \n",
            "| Epoch [ 19/ 60] Iter[ 11/126]\t\tLoss: 0.9102 \n",
            "| Epoch [ 19/ 60] Iter[ 16/126]\t\tLoss: 0.6490 \n",
            "| Epoch [ 19/ 60] Iter[ 21/126]\t\tLoss: 0.9087 \n",
            "| Epoch [ 19/ 60] Iter[ 26/126]\t\tLoss: 5.7080 \n",
            "| Epoch [ 19/ 60] Iter[ 31/126]\t\tLoss: 0.3927 \n",
            "| Epoch [ 19/ 60] Iter[ 36/126]\t\tLoss: 0.8823 \n",
            "| Epoch [ 19/ 60] Iter[ 41/126]\t\tLoss: 5.6291 \n",
            "| Epoch [ 19/ 60] Iter[ 46/126]\t\tLoss: 0.9280 \n",
            "| Epoch [ 19/ 60] Iter[ 51/126]\t\tLoss: 0.8847 \n",
            "| Epoch [ 19/ 60] Iter[ 56/126]\t\tLoss: 0.4727 \n",
            "| Epoch [ 19/ 60] Iter[ 61/126]\t\tLoss: 0.1686 \n",
            "| Epoch [ 19/ 60] Iter[ 66/126]\t\tLoss: 0.7691 \n",
            "| Epoch [ 19/ 60] Iter[ 71/126]\t\tLoss: 1.4144 \n",
            "| Epoch [ 19/ 60] Iter[ 76/126]\t\tLoss: 0.5284 \n",
            "| Epoch [ 19/ 60] Iter[ 81/126]\t\tLoss: 0.6223 \n",
            "| Epoch [ 19/ 60] Iter[ 86/126]\t\tLoss: 0.8458 \n",
            "| Epoch [ 19/ 60] Iter[ 91/126]\t\tLoss: 0.1729 \n",
            "| Epoch [ 19/ 60] Iter[ 96/126]\t\tLoss: 0.8539 \n",
            "| Epoch [ 19/ 60] Iter[101/126]\t\tLoss: 0.9687 \n",
            "| Epoch [ 19/ 60] Iter[106/126]\t\tLoss: 0.4208 \n",
            "| Epoch [ 19/ 60] Iter[111/126]\t\tLoss: 1.8101 \n",
            "| Epoch [ 19/ 60] Iter[116/126]\t\tLoss: 0.6418 \n",
            "| Epoch [ 19/ 60] Iter[121/126]\t\tLoss: 0.4785 \n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #20, LR=0.0001\n",
            "| Epoch [ 20/ 60] Iter[  1/126]\t\tLoss: 0.6419 \n",
            "| Epoch [ 20/ 60] Iter[  6/126]\t\tLoss: 0.4206 \n",
            "| Epoch [ 20/ 60] Iter[ 11/126]\t\tLoss: 5.7973 \n",
            "| Epoch [ 20/ 60] Iter[ 16/126]\t\tLoss: 2.6843 \n",
            "| Epoch [ 20/ 60] Iter[ 21/126]\t\tLoss: 0.8167 \n",
            "| Epoch [ 20/ 60] Iter[ 26/126]\t\tLoss: 5.8206 \n",
            "| Epoch [ 20/ 60] Iter[ 31/126]\t\tLoss: 0.5493 \n",
            "| Epoch [ 20/ 60] Iter[ 36/126]\t\tLoss: 0.2431 \n",
            "| Epoch [ 20/ 60] Iter[ 41/126]\t\tLoss: 0.4597 \n",
            "| Epoch [ 20/ 60] Iter[ 46/126]\t\tLoss: 0.2842 \n",
            "| Epoch [ 20/ 60] Iter[ 51/126]\t\tLoss: 0.3211 \n",
            "| Epoch [ 20/ 60] Iter[ 56/126]\t\tLoss: 0.5378 \n",
            "| Epoch [ 20/ 60] Iter[ 61/126]\t\tLoss: 0.2288 \n",
            "| Epoch [ 20/ 60] Iter[ 66/126]\t\tLoss: 0.4058 \n",
            "| Epoch [ 20/ 60] Iter[ 71/126]\t\tLoss: 6.2445 \n",
            "| Epoch [ 20/ 60] Iter[ 76/126]\t\tLoss: 0.4382 \n",
            "| Epoch [ 20/ 60] Iter[ 81/126]\t\tLoss: 0.5298 \n",
            "| Epoch [ 20/ 60] Iter[ 86/126]\t\tLoss: 0.0819 \n",
            "| Epoch [ 20/ 60] Iter[ 91/126]\t\tLoss: 0.6565 \n",
            "| Epoch [ 20/ 60] Iter[ 96/126]\t\tLoss: 0.6011 \n",
            "| Epoch [ 20/ 60] Iter[101/126]\t\tLoss: 0.7654 \n",
            "| Epoch [ 20/ 60] Iter[106/126]\t\tLoss: 0.3838 \n",
            "| Epoch [ 20/ 60] Iter[111/126]\t\tLoss: 0.2642 \n",
            "| Epoch [ 20/ 60] Iter[116/126]\t\tLoss: 0.3342 \n",
            "| Epoch [ 20/ 60] Iter[121/126]\t\tLoss: 0.5778 \n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 0.2318 \n",
            "\n",
            "=> Training Epoch #21, LR=0.0000\n",
            "| Epoch [ 21/ 60] Iter[  1/126]\t\tLoss: 0.5317 \n",
            "| Epoch [ 21/ 60] Iter[  6/126]\t\tLoss: 0.3063 \n",
            "| Epoch [ 21/ 60] Iter[ 11/126]\t\tLoss: 1.8993 \n",
            "| Epoch [ 21/ 60] Iter[ 16/126]\t\tLoss: 0.4762 \n",
            "| Epoch [ 21/ 60] Iter[ 21/126]\t\tLoss: 0.2564 \n",
            "| Epoch [ 21/ 60] Iter[ 26/126]\t\tLoss: 0.3273 \n",
            "| Epoch [ 21/ 60] Iter[ 31/126]\t\tLoss: 0.0612 \n",
            "| Epoch [ 21/ 60] Iter[ 36/126]\t\tLoss: 0.1159 \n",
            "| Epoch [ 21/ 60] Iter[ 41/126]\t\tLoss: 0.2380 \n",
            "| Epoch [ 21/ 60] Iter[ 46/126]\t\tLoss: 0.6112 \n",
            "| Epoch [ 21/ 60] Iter[ 51/126]\t\tLoss: 0.3183 \n",
            "| Epoch [ 21/ 60] Iter[ 56/126]\t\tLoss: 0.7749 \n",
            "| Epoch [ 21/ 60] Iter[ 61/126]\t\tLoss: 0.9396 \n",
            "| Epoch [ 21/ 60] Iter[ 66/126]\t\tLoss: 0.2126 \n",
            "| Epoch [ 21/ 60] Iter[ 71/126]\t\tLoss: 6.3284 \n",
            "| Epoch [ 21/ 60] Iter[ 76/126]\t\tLoss: 0.9116 \n",
            "| Epoch [ 21/ 60] Iter[ 81/126]\t\tLoss: 0.6326 \n",
            "| Epoch [ 21/ 60] Iter[ 86/126]\t\tLoss: 0.7865 \n",
            "| Epoch [ 21/ 60] Iter[ 91/126]\t\tLoss: 0.4521 \n",
            "| Epoch [ 21/ 60] Iter[ 96/126]\t\tLoss: 0.5942 \n",
            "| Epoch [ 21/ 60] Iter[101/126]\t\tLoss: 1.0797 \n",
            "| Epoch [ 21/ 60] Iter[106/126]\t\tLoss: 0.1944 \n",
            "| Epoch [ 21/ 60] Iter[111/126]\t\tLoss: 0.2614 \n",
            "| Epoch [ 21/ 60] Iter[116/126]\t\tLoss: 5.5612 \n",
            "| Epoch [ 21/ 60] Iter[121/126]\t\tLoss: 0.5911 \n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 0.2321 \n",
            "\n",
            "=> Training Epoch #22, LR=0.0000\n",
            "| Epoch [ 22/ 60] Iter[  1/126]\t\tLoss: 0.1026 \n",
            "| Epoch [ 22/ 60] Iter[  6/126]\t\tLoss: 0.4438 \n",
            "| Epoch [ 22/ 60] Iter[ 11/126]\t\tLoss: 5.5662 \n",
            "| Epoch [ 22/ 60] Iter[ 16/126]\t\tLoss: 0.3823 \n",
            "| Epoch [ 22/ 60] Iter[ 21/126]\t\tLoss: 1.1858 \n",
            "| Epoch [ 22/ 60] Iter[ 26/126]\t\tLoss: 0.6648 \n",
            "| Epoch [ 22/ 60] Iter[ 31/126]\t\tLoss: 0.3881 \n",
            "| Epoch [ 22/ 60] Iter[ 36/126]\t\tLoss: 1.7667 \n",
            "| Epoch [ 22/ 60] Iter[ 41/126]\t\tLoss: 0.6530 \n",
            "| Epoch [ 22/ 60] Iter[ 46/126]\t\tLoss: 0.1189 \n",
            "| Epoch [ 22/ 60] Iter[ 51/126]\t\tLoss: 0.3554 \n",
            "| Epoch [ 22/ 60] Iter[ 56/126]\t\tLoss: 0.2501 \n",
            "| Epoch [ 22/ 60] Iter[ 61/126]\t\tLoss: 2.6497 \n",
            "| Epoch [ 22/ 60] Iter[ 66/126]\t\tLoss: 0.8300 \n",
            "| Epoch [ 22/ 60] Iter[ 71/126]\t\tLoss: 0.8026 \n",
            "| Epoch [ 22/ 60] Iter[ 76/126]\t\tLoss: 0.8842 \n",
            "| Epoch [ 22/ 60] Iter[ 81/126]\t\tLoss: 0.8231 \n",
            "| Epoch [ 22/ 60] Iter[ 86/126]\t\tLoss: 0.4603 \n",
            "| Epoch [ 22/ 60] Iter[ 91/126]\t\tLoss: 0.6449 \n",
            "| Epoch [ 22/ 60] Iter[ 96/126]\t\tLoss: 1.2866 \n",
            "| Epoch [ 22/ 60] Iter[101/126]\t\tLoss: 5.9360 \n",
            "| Epoch [ 22/ 60] Iter[106/126]\t\tLoss: 1.3504 \n",
            "| Epoch [ 22/ 60] Iter[111/126]\t\tLoss: 0.6916 \n",
            "| Epoch [ 22/ 60] Iter[116/126]\t\tLoss: 0.0673 \n",
            "| Epoch [ 22/ 60] Iter[121/126]\t\tLoss: 0.5137 \n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 0.2321 \n",
            "\n",
            "=> Training Epoch #23, LR=0.0000\n",
            "| Epoch [ 23/ 60] Iter[  1/126]\t\tLoss: 0.4743 \n",
            "| Epoch [ 23/ 60] Iter[  6/126]\t\tLoss: 0.8639 \n",
            "| Epoch [ 23/ 60] Iter[ 11/126]\t\tLoss: 0.0461 \n",
            "| Epoch [ 23/ 60] Iter[ 16/126]\t\tLoss: 5.8004 \n",
            "| Epoch [ 23/ 60] Iter[ 21/126]\t\tLoss: 0.1482 \n",
            "| Epoch [ 23/ 60] Iter[ 26/126]\t\tLoss: 1.5677 \n",
            "| Epoch [ 23/ 60] Iter[ 31/126]\t\tLoss: 0.1780 \n",
            "| Epoch [ 23/ 60] Iter[ 36/126]\t\tLoss: 0.5370 \n",
            "| Epoch [ 23/ 60] Iter[ 41/126]\t\tLoss: 0.6904 \n",
            "| Epoch [ 23/ 60] Iter[ 46/126]\t\tLoss: 0.5570 \n",
            "| Epoch [ 23/ 60] Iter[ 51/126]\t\tLoss: 0.4951 \n",
            "| Epoch [ 23/ 60] Iter[ 56/126]\t\tLoss: 0.7845 \n",
            "| Epoch [ 23/ 60] Iter[ 61/126]\t\tLoss: 0.4275 \n",
            "| Epoch [ 23/ 60] Iter[ 66/126]\t\tLoss: 0.2857 \n",
            "| Epoch [ 23/ 60] Iter[ 71/126]\t\tLoss: 0.1196 \n",
            "| Epoch [ 23/ 60] Iter[ 76/126]\t\tLoss: 0.3275 \n",
            "| Epoch [ 23/ 60] Iter[ 81/126]\t\tLoss: 0.6640 \n",
            "| Epoch [ 23/ 60] Iter[ 86/126]\t\tLoss: 0.3940 \n",
            "| Epoch [ 23/ 60] Iter[ 91/126]\t\tLoss: 0.1795 \n",
            "| Epoch [ 23/ 60] Iter[ 96/126]\t\tLoss: 8.4282 \n",
            "| Epoch [ 23/ 60] Iter[101/126]\t\tLoss: 0.3895 \n",
            "| Epoch [ 23/ 60] Iter[106/126]\t\tLoss: 0.3226 \n",
            "| Epoch [ 23/ 60] Iter[111/126]\t\tLoss: 1.1088 \n",
            "| Epoch [ 23/ 60] Iter[116/126]\t\tLoss: 0.6533 \n",
            "| Epoch [ 23/ 60] Iter[121/126]\t\tLoss: 0.5467 \n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 0.2318 \n",
            "\n",
            "=> Training Epoch #24, LR=0.0000\n",
            "| Epoch [ 24/ 60] Iter[  1/126]\t\tLoss: 0.5241 \n",
            "| Epoch [ 24/ 60] Iter[  6/126]\t\tLoss: 0.5136 \n",
            "| Epoch [ 24/ 60] Iter[ 11/126]\t\tLoss: 0.3009 \n",
            "| Epoch [ 24/ 60] Iter[ 16/126]\t\tLoss: 0.3357 \n",
            "| Epoch [ 24/ 60] Iter[ 21/126]\t\tLoss: 0.1293 \n",
            "| Epoch [ 24/ 60] Iter[ 26/126]\t\tLoss: 1.1287 \n",
            "| Epoch [ 24/ 60] Iter[ 31/126]\t\tLoss: 0.3514 \n",
            "| Epoch [ 24/ 60] Iter[ 36/126]\t\tLoss: 1.8214 \n",
            "| Epoch [ 24/ 60] Iter[ 41/126]\t\tLoss: 1.0141 \n",
            "| Epoch [ 24/ 60] Iter[ 46/126]\t\tLoss: 1.0031 \n",
            "| Epoch [ 24/ 60] Iter[ 51/126]\t\tLoss: 0.3464 \n",
            "| Epoch [ 24/ 60] Iter[ 56/126]\t\tLoss: 0.5424 \n",
            "| Epoch [ 24/ 60] Iter[ 61/126]\t\tLoss: 1.1253 \n",
            "| Epoch [ 24/ 60] Iter[ 66/126]\t\tLoss: 0.0614 \n",
            "| Epoch [ 24/ 60] Iter[ 71/126]\t\tLoss: 0.4184 \n",
            "| Epoch [ 24/ 60] Iter[ 76/126]\t\tLoss: 0.9490 \n",
            "| Epoch [ 24/ 60] Iter[ 81/126]\t\tLoss: 1.1515 \n",
            "| Epoch [ 24/ 60] Iter[ 86/126]\t\tLoss: 0.2867 \n",
            "| Epoch [ 24/ 60] Iter[ 91/126]\t\tLoss: 0.1454 \n",
            "| Epoch [ 24/ 60] Iter[ 96/126]\t\tLoss: 0.5821 \n",
            "| Epoch [ 24/ 60] Iter[101/126]\t\tLoss: 0.1943 \n",
            "| Epoch [ 24/ 60] Iter[106/126]\t\tLoss: 0.4064 \n",
            "| Epoch [ 24/ 60] Iter[111/126]\t\tLoss: 0.9142 \n",
            "| Epoch [ 24/ 60] Iter[116/126]\t\tLoss: 0.6848 \n",
            "| Epoch [ 24/ 60] Iter[121/126]\t\tLoss: 0.3378 \n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 0.2319 \n",
            "\n",
            "=> Training Epoch #25, LR=0.0000\n",
            "| Epoch [ 25/ 60] Iter[  1/126]\t\tLoss: 0.1717 \n",
            "| Epoch [ 25/ 60] Iter[  6/126]\t\tLoss: 0.7400 \n",
            "| Epoch [ 25/ 60] Iter[ 11/126]\t\tLoss: 0.2514 \n",
            "| Epoch [ 25/ 60] Iter[ 16/126]\t\tLoss: 0.3580 \n",
            "| Epoch [ 25/ 60] Iter[ 21/126]\t\tLoss: 0.7583 \n",
            "| Epoch [ 25/ 60] Iter[ 26/126]\t\tLoss: 0.2666 \n",
            "| Epoch [ 25/ 60] Iter[ 31/126]\t\tLoss: 5.9015 \n",
            "| Epoch [ 25/ 60] Iter[ 36/126]\t\tLoss: 0.3812 \n",
            "| Epoch [ 25/ 60] Iter[ 41/126]\t\tLoss: 0.9778 \n",
            "| Epoch [ 25/ 60] Iter[ 46/126]\t\tLoss: 0.8525 \n",
            "| Epoch [ 25/ 60] Iter[ 51/126]\t\tLoss: 6.0811 \n",
            "| Epoch [ 25/ 60] Iter[ 56/126]\t\tLoss: 0.6110 \n",
            "| Epoch [ 25/ 60] Iter[ 61/126]\t\tLoss: 0.2897 \n",
            "| Epoch [ 25/ 60] Iter[ 66/126]\t\tLoss: 0.9441 \n",
            "| Epoch [ 25/ 60] Iter[ 71/126]\t\tLoss: 0.2296 \n",
            "| Epoch [ 25/ 60] Iter[ 76/126]\t\tLoss: 1.3791 \n",
            "| Epoch [ 25/ 60] Iter[ 81/126]\t\tLoss: 0.5517 \n",
            "| Epoch [ 25/ 60] Iter[ 86/126]\t\tLoss: 6.3254 \n",
            "| Epoch [ 25/ 60] Iter[ 91/126]\t\tLoss: 0.2382 \n",
            "| Epoch [ 25/ 60] Iter[ 96/126]\t\tLoss: 6.9924 \n",
            "| Epoch [ 25/ 60] Iter[101/126]\t\tLoss: 0.2127 \n",
            "| Epoch [ 25/ 60] Iter[106/126]\t\tLoss: 0.6116 \n",
            "| Epoch [ 25/ 60] Iter[111/126]\t\tLoss: 0.2207 \n",
            "| Epoch [ 25/ 60] Iter[116/126]\t\tLoss: 0.9050 \n",
            "| Epoch [ 25/ 60] Iter[121/126]\t\tLoss: 0.3122 \n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 0.2316 \n",
            "\n",
            "=> Training Epoch #26, LR=0.0000\n",
            "| Epoch [ 26/ 60] Iter[  1/126]\t\tLoss: 0.4387 \n",
            "| Epoch [ 26/ 60] Iter[  6/126]\t\tLoss: 0.2276 \n",
            "| Epoch [ 26/ 60] Iter[ 11/126]\t\tLoss: 0.7213 \n",
            "| Epoch [ 26/ 60] Iter[ 16/126]\t\tLoss: 2.5550 \n",
            "| Epoch [ 26/ 60] Iter[ 21/126]\t\tLoss: 0.3790 \n",
            "| Epoch [ 26/ 60] Iter[ 26/126]\t\tLoss: 0.1016 \n",
            "| Epoch [ 26/ 60] Iter[ 31/126]\t\tLoss: 0.8343 \n",
            "| Epoch [ 26/ 60] Iter[ 36/126]\t\tLoss: 0.6185 \n",
            "| Epoch [ 26/ 60] Iter[ 41/126]\t\tLoss: 0.5859 \n",
            "| Epoch [ 26/ 60] Iter[ 46/126]\t\tLoss: 0.7213 \n",
            "| Epoch [ 26/ 60] Iter[ 51/126]\t\tLoss: 0.8924 \n",
            "| Epoch [ 26/ 60] Iter[ 56/126]\t\tLoss: 6.6514 \n",
            "| Epoch [ 26/ 60] Iter[ 61/126]\t\tLoss: 0.3631 \n",
            "| Epoch [ 26/ 60] Iter[ 66/126]\t\tLoss: 0.3801 \n",
            "| Epoch [ 26/ 60] Iter[ 71/126]\t\tLoss: 0.0639 \n",
            "| Epoch [ 26/ 60] Iter[ 76/126]\t\tLoss: 6.1612 \n",
            "| Epoch [ 26/ 60] Iter[ 81/126]\t\tLoss: 0.6058 \n",
            "| Epoch [ 26/ 60] Iter[ 86/126]\t\tLoss: 5.6306 \n",
            "| Epoch [ 26/ 60] Iter[ 91/126]\t\tLoss: 0.5449 \n",
            "| Epoch [ 26/ 60] Iter[ 96/126]\t\tLoss: 0.2832 \n",
            "| Epoch [ 26/ 60] Iter[101/126]\t\tLoss: 0.2060 \n",
            "| Epoch [ 26/ 60] Iter[106/126]\t\tLoss: 0.7291 \n",
            "| Epoch [ 26/ 60] Iter[111/126]\t\tLoss: 6.1324 \n",
            "| Epoch [ 26/ 60] Iter[116/126]\t\tLoss: 0.7293 \n",
            "| Epoch [ 26/ 60] Iter[121/126]\t\tLoss: 0.3155 \n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 0.2302 \n",
            "\n",
            "=> Training Epoch #27, LR=0.0000\n",
            "| Epoch [ 27/ 60] Iter[  1/126]\t\tLoss: 6.1659 \n",
            "| Epoch [ 27/ 60] Iter[  6/126]\t\tLoss: 1.3268 \n",
            "| Epoch [ 27/ 60] Iter[ 11/126]\t\tLoss: 0.5200 \n",
            "| Epoch [ 27/ 60] Iter[ 16/126]\t\tLoss: 0.3163 \n",
            "| Epoch [ 27/ 60] Iter[ 21/126]\t\tLoss: 0.5557 \n",
            "| Epoch [ 27/ 60] Iter[ 26/126]\t\tLoss: 0.7822 \n",
            "| Epoch [ 27/ 60] Iter[ 31/126]\t\tLoss: 0.4168 \n",
            "| Epoch [ 27/ 60] Iter[ 36/126]\t\tLoss: 0.6574 \n",
            "| Epoch [ 27/ 60] Iter[ 41/126]\t\tLoss: 0.2125 \n",
            "| Epoch [ 27/ 60] Iter[ 46/126]\t\tLoss: 0.9504 \n",
            "| Epoch [ 27/ 60] Iter[ 51/126]\t\tLoss: 0.2071 \n",
            "| Epoch [ 27/ 60] Iter[ 56/126]\t\tLoss: 0.1951 \n",
            "| Epoch [ 27/ 60] Iter[ 61/126]\t\tLoss: 0.7294 \n",
            "| Epoch [ 27/ 60] Iter[ 66/126]\t\tLoss: 0.1837 \n",
            "| Epoch [ 27/ 60] Iter[ 71/126]\t\tLoss: 0.5811 \n",
            "| Epoch [ 27/ 60] Iter[ 76/126]\t\tLoss: 0.8740 \n",
            "| Epoch [ 27/ 60] Iter[ 81/126]\t\tLoss: 0.6673 \n",
            "| Epoch [ 27/ 60] Iter[ 86/126]\t\tLoss: 0.3819 \n",
            "| Epoch [ 27/ 60] Iter[ 91/126]\t\tLoss: 0.3178 \n",
            "| Epoch [ 27/ 60] Iter[ 96/126]\t\tLoss: 0.3390 \n",
            "| Epoch [ 27/ 60] Iter[101/126]\t\tLoss: 6.5773 \n",
            "| Epoch [ 27/ 60] Iter[106/126]\t\tLoss: 0.1138 \n",
            "| Epoch [ 27/ 60] Iter[111/126]\t\tLoss: 6.2521 \n",
            "| Epoch [ 27/ 60] Iter[116/126]\t\tLoss: 0.5312 \n",
            "| Epoch [ 27/ 60] Iter[121/126]\t\tLoss: 0.5150 \n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 0.2302 \n",
            "\n",
            "=> Training Epoch #28, LR=0.0000\n",
            "| Epoch [ 28/ 60] Iter[  1/126]\t\tLoss: 0.5747 \n",
            "| Epoch [ 28/ 60] Iter[  6/126]\t\tLoss: 0.8334 \n",
            "| Epoch [ 28/ 60] Iter[ 11/126]\t\tLoss: 0.4825 \n",
            "| Epoch [ 28/ 60] Iter[ 16/126]\t\tLoss: 0.3738 \n",
            "| Epoch [ 28/ 60] Iter[ 21/126]\t\tLoss: 0.8354 \n",
            "| Epoch [ 28/ 60] Iter[ 26/126]\t\tLoss: 0.6222 \n",
            "| Epoch [ 28/ 60] Iter[ 31/126]\t\tLoss: 0.2980 \n",
            "| Epoch [ 28/ 60] Iter[ 36/126]\t\tLoss: 0.1845 \n",
            "| Epoch [ 28/ 60] Iter[ 41/126]\t\tLoss: 0.6476 \n",
            "| Epoch [ 28/ 60] Iter[ 46/126]\t\tLoss: 0.5867 \n",
            "| Epoch [ 28/ 60] Iter[ 51/126]\t\tLoss: 0.8187 \n",
            "| Epoch [ 28/ 60] Iter[ 56/126]\t\tLoss: 1.0155 \n",
            "| Epoch [ 28/ 60] Iter[ 61/126]\t\tLoss: 1.0833 \n",
            "| Epoch [ 28/ 60] Iter[ 66/126]\t\tLoss: 1.4399 \n",
            "| Epoch [ 28/ 60] Iter[ 71/126]\t\tLoss: 0.5875 \n",
            "| Epoch [ 28/ 60] Iter[ 76/126]\t\tLoss: 0.5567 \n",
            "| Epoch [ 28/ 60] Iter[ 81/126]\t\tLoss: 0.2690 \n",
            "| Epoch [ 28/ 60] Iter[ 86/126]\t\tLoss: 0.3146 \n",
            "| Epoch [ 28/ 60] Iter[ 91/126]\t\tLoss: 0.3853 \n",
            "| Epoch [ 28/ 60] Iter[ 96/126]\t\tLoss: 0.9938 \n",
            "| Epoch [ 28/ 60] Iter[101/126]\t\tLoss: 0.3994 \n",
            "| Epoch [ 28/ 60] Iter[106/126]\t\tLoss: 0.0845 \n",
            "| Epoch [ 28/ 60] Iter[111/126]\t\tLoss: 1.0319 \n",
            "| Epoch [ 28/ 60] Iter[116/126]\t\tLoss: 6.0087 \n",
            "| Epoch [ 28/ 60] Iter[121/126]\t\tLoss: 0.3060 \n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 0.2294 \n",
            "\n",
            "=> Training Epoch #29, LR=0.0000\n",
            "| Epoch [ 29/ 60] Iter[  1/126]\t\tLoss: 0.5927 \n",
            "| Epoch [ 29/ 60] Iter[  6/126]\t\tLoss: 5.2557 \n",
            "| Epoch [ 29/ 60] Iter[ 11/126]\t\tLoss: 0.2716 \n",
            "| Epoch [ 29/ 60] Iter[ 16/126]\t\tLoss: 0.2516 \n",
            "| Epoch [ 29/ 60] Iter[ 21/126]\t\tLoss: 0.8063 \n",
            "| Epoch [ 29/ 60] Iter[ 26/126]\t\tLoss: 0.3842 \n",
            "| Epoch [ 29/ 60] Iter[ 31/126]\t\tLoss: 2.5082 \n",
            "| Epoch [ 29/ 60] Iter[ 36/126]\t\tLoss: 0.1820 \n",
            "| Epoch [ 29/ 60] Iter[ 41/126]\t\tLoss: 0.4007 \n",
            "| Epoch [ 29/ 60] Iter[ 46/126]\t\tLoss: 0.7544 \n",
            "| Epoch [ 29/ 60] Iter[ 51/126]\t\tLoss: 0.4379 \n",
            "| Epoch [ 29/ 60] Iter[ 56/126]\t\tLoss: 0.0143 \n",
            "| Epoch [ 29/ 60] Iter[ 61/126]\t\tLoss: 0.6972 \n",
            "| Epoch [ 29/ 60] Iter[ 66/126]\t\tLoss: 0.1259 \n",
            "| Epoch [ 29/ 60] Iter[ 71/126]\t\tLoss: 6.3172 \n",
            "| Epoch [ 29/ 60] Iter[ 76/126]\t\tLoss: 1.2479 \n",
            "| Epoch [ 29/ 60] Iter[ 81/126]\t\tLoss: 0.2483 \n",
            "| Epoch [ 29/ 60] Iter[ 86/126]\t\tLoss: 0.4758 \n",
            "| Epoch [ 29/ 60] Iter[ 91/126]\t\tLoss: 0.7862 \n",
            "| Epoch [ 29/ 60] Iter[ 96/126]\t\tLoss: 0.6795 \n",
            "| Epoch [ 29/ 60] Iter[101/126]\t\tLoss: 6.0944 \n",
            "| Epoch [ 29/ 60] Iter[106/126]\t\tLoss: 2.3768 \n",
            "| Epoch [ 29/ 60] Iter[111/126]\t\tLoss: 0.2565 \n",
            "| Epoch [ 29/ 60] Iter[116/126]\t\tLoss: 0.4491 \n",
            "| Epoch [ 29/ 60] Iter[121/126]\t\tLoss: 0.5552 \n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 0.2288 \n",
            "\n",
            "=> Training Epoch #30, LR=0.0000\n",
            "| Epoch [ 30/ 60] Iter[  1/126]\t\tLoss: 0.8226 \n",
            "| Epoch [ 30/ 60] Iter[  6/126]\t\tLoss: 0.6307 \n",
            "| Epoch [ 30/ 60] Iter[ 11/126]\t\tLoss: 0.6583 \n",
            "| Epoch [ 30/ 60] Iter[ 16/126]\t\tLoss: 0.4765 \n",
            "| Epoch [ 30/ 60] Iter[ 21/126]\t\tLoss: 1.2434 \n",
            "| Epoch [ 30/ 60] Iter[ 26/126]\t\tLoss: 0.6249 \n",
            "| Epoch [ 30/ 60] Iter[ 31/126]\t\tLoss: 0.1476 \n",
            "| Epoch [ 30/ 60] Iter[ 36/126]\t\tLoss: 0.1960 \n",
            "| Epoch [ 30/ 60] Iter[ 41/126]\t\tLoss: 0.3314 \n",
            "| Epoch [ 30/ 60] Iter[ 46/126]\t\tLoss: 0.5271 \n",
            "| Epoch [ 30/ 60] Iter[ 51/126]\t\tLoss: 6.5246 \n",
            "| Epoch [ 30/ 60] Iter[ 56/126]\t\tLoss: 0.6423 \n",
            "| Epoch [ 30/ 60] Iter[ 61/126]\t\tLoss: 0.0760 \n",
            "| Epoch [ 30/ 60] Iter[ 66/126]\t\tLoss: 9.0926 \n",
            "| Epoch [ 30/ 60] Iter[ 71/126]\t\tLoss: 0.0458 \n",
            "| Epoch [ 30/ 60] Iter[ 76/126]\t\tLoss: 0.8745 \n",
            "| Epoch [ 30/ 60] Iter[ 81/126]\t\tLoss: 0.5989 \n",
            "| Epoch [ 30/ 60] Iter[ 86/126]\t\tLoss: 0.1246 \n",
            "| Epoch [ 30/ 60] Iter[ 91/126]\t\tLoss: 5.5950 \n",
            "| Epoch [ 30/ 60] Iter[ 96/126]\t\tLoss: 0.2493 \n",
            "| Epoch [ 30/ 60] Iter[101/126]\t\tLoss: 0.6423 \n",
            "| Epoch [ 30/ 60] Iter[106/126]\t\tLoss: 0.9095 \n",
            "| Epoch [ 30/ 60] Iter[111/126]\t\tLoss: 0.3537 \n",
            "| Epoch [ 30/ 60] Iter[116/126]\t\tLoss: 0.5391 \n",
            "| Epoch [ 30/ 60] Iter[121/126]\t\tLoss: 0.1135 \n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 0.2283 \n",
            "\n",
            "=> Training Epoch #31, LR=0.0000\n",
            "| Epoch [ 31/ 60] Iter[  1/126]\t\tLoss: 0.4768 \n",
            "| Epoch [ 31/ 60] Iter[  6/126]\t\tLoss: 0.0943 \n",
            "| Epoch [ 31/ 60] Iter[ 11/126]\t\tLoss: 0.2939 \n",
            "| Epoch [ 31/ 60] Iter[ 16/126]\t\tLoss: 0.7099 \n",
            "| Epoch [ 31/ 60] Iter[ 21/126]\t\tLoss: 0.8900 \n",
            "| Epoch [ 31/ 60] Iter[ 26/126]\t\tLoss: 0.1057 \n",
            "| Epoch [ 31/ 60] Iter[ 31/126]\t\tLoss: 0.3319 \n",
            "| Epoch [ 31/ 60] Iter[ 36/126]\t\tLoss: 0.1655 \n",
            "| Epoch [ 31/ 60] Iter[ 41/126]\t\tLoss: 0.0357 \n",
            "| Epoch [ 31/ 60] Iter[ 46/126]\t\tLoss: 3.8202 \n",
            "| Epoch [ 31/ 60] Iter[ 51/126]\t\tLoss: 0.7300 \n",
            "| Epoch [ 31/ 60] Iter[ 56/126]\t\tLoss: 0.3158 \n",
            "| Epoch [ 31/ 60] Iter[ 61/126]\t\tLoss: 1.8139 \n",
            "| Epoch [ 31/ 60] Iter[ 66/126]\t\tLoss: 0.6144 \n",
            "| Epoch [ 31/ 60] Iter[ 71/126]\t\tLoss: 0.4229 \n",
            "| Epoch [ 31/ 60] Iter[ 76/126]\t\tLoss: 0.2352 \n",
            "| Epoch [ 31/ 60] Iter[ 81/126]\t\tLoss: 6.5304 \n",
            "| Epoch [ 31/ 60] Iter[ 86/126]\t\tLoss: 5.7688 \n",
            "| Epoch [ 31/ 60] Iter[ 91/126]\t\tLoss: 0.4818 \n",
            "| Epoch [ 31/ 60] Iter[ 96/126]\t\tLoss: 0.2801 \n",
            "| Epoch [ 31/ 60] Iter[101/126]\t\tLoss: 0.2706 \n",
            "| Epoch [ 31/ 60] Iter[106/126]\t\tLoss: 0.7601 \n",
            "| Epoch [ 31/ 60] Iter[111/126]\t\tLoss: 0.4459 \n",
            "| Epoch [ 31/ 60] Iter[116/126]\t\tLoss: 0.2902 \n",
            "| Epoch [ 31/ 60] Iter[121/126]\t\tLoss: 0.2685 \n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #32, LR=0.0000\n",
            "| Epoch [ 32/ 60] Iter[  1/126]\t\tLoss: 0.7856 \n",
            "| Epoch [ 32/ 60] Iter[  6/126]\t\tLoss: 0.4355 \n",
            "| Epoch [ 32/ 60] Iter[ 11/126]\t\tLoss: 0.3203 \n",
            "| Epoch [ 32/ 60] Iter[ 16/126]\t\tLoss: 0.4442 \n",
            "| Epoch [ 32/ 60] Iter[ 21/126]\t\tLoss: 0.8529 \n",
            "| Epoch [ 32/ 60] Iter[ 26/126]\t\tLoss: 0.4971 \n",
            "| Epoch [ 32/ 60] Iter[ 31/126]\t\tLoss: 5.9253 \n",
            "| Epoch [ 32/ 60] Iter[ 36/126]\t\tLoss: 0.5599 \n",
            "| Epoch [ 32/ 60] Iter[ 41/126]\t\tLoss: 0.7781 \n",
            "| Epoch [ 32/ 60] Iter[ 46/126]\t\tLoss: 0.2477 \n",
            "| Epoch [ 32/ 60] Iter[ 51/126]\t\tLoss: 1.4240 \n",
            "| Epoch [ 32/ 60] Iter[ 56/126]\t\tLoss: 0.2712 \n",
            "| Epoch [ 32/ 60] Iter[ 61/126]\t\tLoss: 6.4614 \n",
            "| Epoch [ 32/ 60] Iter[ 66/126]\t\tLoss: 0.3900 \n",
            "| Epoch [ 32/ 60] Iter[ 71/126]\t\tLoss: 0.4650 \n",
            "| Epoch [ 32/ 60] Iter[ 76/126]\t\tLoss: 0.1812 \n",
            "| Epoch [ 32/ 60] Iter[ 81/126]\t\tLoss: 0.5361 \n",
            "| Epoch [ 32/ 60] Iter[ 86/126]\t\tLoss: 1.1686 \n",
            "| Epoch [ 32/ 60] Iter[ 91/126]\t\tLoss: 6.0740 \n",
            "| Epoch [ 32/ 60] Iter[ 96/126]\t\tLoss: 0.4743 \n",
            "| Epoch [ 32/ 60] Iter[101/126]\t\tLoss: 0.7881 \n",
            "| Epoch [ 32/ 60] Iter[106/126]\t\tLoss: 0.5196 \n",
            "| Epoch [ 32/ 60] Iter[111/126]\t\tLoss: 0.9354 \n",
            "| Epoch [ 32/ 60] Iter[116/126]\t\tLoss: 0.2705 \n",
            "| Epoch [ 32/ 60] Iter[121/126]\t\tLoss: 5.5857 \n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 0.2278 \n",
            "\n",
            "=> Training Epoch #33, LR=0.0000\n",
            "| Epoch [ 33/ 60] Iter[  1/126]\t\tLoss: 0.3373 \n",
            "| Epoch [ 33/ 60] Iter[  6/126]\t\tLoss: 0.2937 \n",
            "| Epoch [ 33/ 60] Iter[ 11/126]\t\tLoss: 0.3438 \n",
            "| Epoch [ 33/ 60] Iter[ 16/126]\t\tLoss: 1.1493 \n",
            "| Epoch [ 33/ 60] Iter[ 21/126]\t\tLoss: 0.2168 \n",
            "| Epoch [ 33/ 60] Iter[ 26/126]\t\tLoss: 0.7761 \n",
            "| Epoch [ 33/ 60] Iter[ 31/126]\t\tLoss: 5.5282 \n",
            "| Epoch [ 33/ 60] Iter[ 36/126]\t\tLoss: 0.8062 \n",
            "| Epoch [ 33/ 60] Iter[ 41/126]\t\tLoss: 0.7487 \n",
            "| Epoch [ 33/ 60] Iter[ 46/126]\t\tLoss: 0.8221 \n",
            "| Epoch [ 33/ 60] Iter[ 51/126]\t\tLoss: 0.5322 \n",
            "| Epoch [ 33/ 60] Iter[ 56/126]\t\tLoss: 5.8554 \n",
            "| Epoch [ 33/ 60] Iter[ 61/126]\t\tLoss: 0.2790 \n",
            "| Epoch [ 33/ 60] Iter[ 66/126]\t\tLoss: 0.6106 \n",
            "| Epoch [ 33/ 60] Iter[ 71/126]\t\tLoss: 2.8637 \n",
            "| Epoch [ 33/ 60] Iter[ 76/126]\t\tLoss: 0.1802 \n",
            "| Epoch [ 33/ 60] Iter[ 81/126]\t\tLoss: 0.3192 \n",
            "| Epoch [ 33/ 60] Iter[ 86/126]\t\tLoss: 0.3670 \n",
            "| Epoch [ 33/ 60] Iter[ 91/126]\t\tLoss: 2.2312 \n",
            "| Epoch [ 33/ 60] Iter[ 96/126]\t\tLoss: 0.3646 \n",
            "| Epoch [ 33/ 60] Iter[101/126]\t\tLoss: 6.5458 \n",
            "| Epoch [ 33/ 60] Iter[106/126]\t\tLoss: 6.0626 \n",
            "| Epoch [ 33/ 60] Iter[111/126]\t\tLoss: 0.5704 \n",
            "| Epoch [ 33/ 60] Iter[116/126]\t\tLoss: 0.1752 \n",
            "| Epoch [ 33/ 60] Iter[121/126]\t\tLoss: 6.3106 \n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 0.2268 \n",
            "\n",
            "=> Training Epoch #34, LR=0.0000\n",
            "| Epoch [ 34/ 60] Iter[  1/126]\t\tLoss: 0.5114 \n",
            "| Epoch [ 34/ 60] Iter[  6/126]\t\tLoss: 0.2273 \n",
            "| Epoch [ 34/ 60] Iter[ 11/126]\t\tLoss: 0.4659 \n",
            "| Epoch [ 34/ 60] Iter[ 16/126]\t\tLoss: 0.5423 \n",
            "| Epoch [ 34/ 60] Iter[ 21/126]\t\tLoss: 0.5968 \n",
            "| Epoch [ 34/ 60] Iter[ 26/126]\t\tLoss: 6.0379 \n",
            "| Epoch [ 34/ 60] Iter[ 31/126]\t\tLoss: 1.1729 \n",
            "| Epoch [ 34/ 60] Iter[ 36/126]\t\tLoss: 0.3401 \n",
            "| Epoch [ 34/ 60] Iter[ 41/126]\t\tLoss: 0.5840 \n",
            "| Epoch [ 34/ 60] Iter[ 46/126]\t\tLoss: 0.2528 \n",
            "| Epoch [ 34/ 60] Iter[ 51/126]\t\tLoss: 0.7328 \n",
            "| Epoch [ 34/ 60] Iter[ 56/126]\t\tLoss: 0.2325 \n",
            "| Epoch [ 34/ 60] Iter[ 61/126]\t\tLoss: 0.3106 \n",
            "| Epoch [ 34/ 60] Iter[ 66/126]\t\tLoss: 0.8231 \n",
            "| Epoch [ 34/ 60] Iter[ 71/126]\t\tLoss: 0.2088 \n",
            "| Epoch [ 34/ 60] Iter[ 76/126]\t\tLoss: 1.3968 \n",
            "| Epoch [ 34/ 60] Iter[ 81/126]\t\tLoss: 0.4429 \n",
            "| Epoch [ 34/ 60] Iter[ 86/126]\t\tLoss: 0.3584 \n",
            "| Epoch [ 34/ 60] Iter[ 91/126]\t\tLoss: 1.4894 \n",
            "| Epoch [ 34/ 60] Iter[ 96/126]\t\tLoss: 0.1695 \n",
            "| Epoch [ 34/ 60] Iter[101/126]\t\tLoss: 0.0273 \n",
            "| Epoch [ 34/ 60] Iter[106/126]\t\tLoss: 5.6419 \n",
            "| Epoch [ 34/ 60] Iter[111/126]\t\tLoss: 0.9126 \n",
            "| Epoch [ 34/ 60] Iter[116/126]\t\tLoss: 0.1081 \n",
            "| Epoch [ 34/ 60] Iter[121/126]\t\tLoss: 0.2103 \n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 0.2266 \n",
            "\n",
            "=> Training Epoch #35, LR=0.0000\n",
            "| Epoch [ 35/ 60] Iter[  1/126]\t\tLoss: 0.0626 \n",
            "| Epoch [ 35/ 60] Iter[  6/126]\t\tLoss: 0.7082 \n",
            "| Epoch [ 35/ 60] Iter[ 11/126]\t\tLoss: 0.5879 \n",
            "| Epoch [ 35/ 60] Iter[ 16/126]\t\tLoss: 0.2353 \n",
            "| Epoch [ 35/ 60] Iter[ 21/126]\t\tLoss: 0.1581 \n",
            "| Epoch [ 35/ 60] Iter[ 26/126]\t\tLoss: 0.2953 \n",
            "| Epoch [ 35/ 60] Iter[ 31/126]\t\tLoss: 0.6367 \n",
            "| Epoch [ 35/ 60] Iter[ 36/126]\t\tLoss: 0.5121 \n",
            "| Epoch [ 35/ 60] Iter[ 41/126]\t\tLoss: 0.6605 \n",
            "| Epoch [ 35/ 60] Iter[ 46/126]\t\tLoss: 0.4086 \n",
            "| Epoch [ 35/ 60] Iter[ 51/126]\t\tLoss: 5.6896 \n",
            "| Epoch [ 35/ 60] Iter[ 56/126]\t\tLoss: 0.3109 \n",
            "| Epoch [ 35/ 60] Iter[ 61/126]\t\tLoss: 1.2068 \n",
            "| Epoch [ 35/ 60] Iter[ 66/126]\t\tLoss: 0.4425 \n",
            "| Epoch [ 35/ 60] Iter[ 71/126]\t\tLoss: 3.6356 \n",
            "| Epoch [ 35/ 60] Iter[ 76/126]\t\tLoss: 0.9097 \n",
            "| Epoch [ 35/ 60] Iter[ 81/126]\t\tLoss: 0.6659 \n",
            "| Epoch [ 35/ 60] Iter[ 86/126]\t\tLoss: 0.4437 \n",
            "| Epoch [ 35/ 60] Iter[ 91/126]\t\tLoss: 0.6919 \n",
            "| Epoch [ 35/ 60] Iter[ 96/126]\t\tLoss: 0.1374 \n",
            "| Epoch [ 35/ 60] Iter[101/126]\t\tLoss: 0.8324 \n",
            "| Epoch [ 35/ 60] Iter[106/126]\t\tLoss: 0.1588 \n",
            "| Epoch [ 35/ 60] Iter[111/126]\t\tLoss: 0.2287 \n",
            "| Epoch [ 35/ 60] Iter[116/126]\t\tLoss: 0.4119 \n",
            "| Epoch [ 35/ 60] Iter[121/126]\t\tLoss: 0.3538 \n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 0.2266 \n",
            "\n",
            "=> Training Epoch #36, LR=0.0000\n",
            "| Epoch [ 36/ 60] Iter[  1/126]\t\tLoss: 0.8308 \n",
            "| Epoch [ 36/ 60] Iter[  6/126]\t\tLoss: 0.3193 \n",
            "| Epoch [ 36/ 60] Iter[ 11/126]\t\tLoss: 1.8033 \n",
            "| Epoch [ 36/ 60] Iter[ 16/126]\t\tLoss: 5.5743 \n",
            "| Epoch [ 36/ 60] Iter[ 21/126]\t\tLoss: 0.2987 \n",
            "| Epoch [ 36/ 60] Iter[ 26/126]\t\tLoss: 0.3416 \n",
            "| Epoch [ 36/ 60] Iter[ 31/126]\t\tLoss: 0.5709 \n",
            "| Epoch [ 36/ 60] Iter[ 36/126]\t\tLoss: 0.2573 \n",
            "| Epoch [ 36/ 60] Iter[ 41/126]\t\tLoss: 0.2339 \n",
            "| Epoch [ 36/ 60] Iter[ 46/126]\t\tLoss: 0.1725 \n",
            "| Epoch [ 36/ 60] Iter[ 51/126]\t\tLoss: 0.2288 \n",
            "| Epoch [ 36/ 60] Iter[ 56/126]\t\tLoss: 6.4220 \n",
            "| Epoch [ 36/ 60] Iter[ 61/126]\t\tLoss: 0.1698 \n",
            "| Epoch [ 36/ 60] Iter[ 66/126]\t\tLoss: 0.4623 \n",
            "| Epoch [ 36/ 60] Iter[ 71/126]\t\tLoss: 0.1542 \n",
            "| Epoch [ 36/ 60] Iter[ 76/126]\t\tLoss: 0.5699 \n",
            "| Epoch [ 36/ 60] Iter[ 81/126]\t\tLoss: 0.4381 \n",
            "| Epoch [ 36/ 60] Iter[ 86/126]\t\tLoss: 0.4615 \n",
            "| Epoch [ 36/ 60] Iter[ 91/126]\t\tLoss: 0.4458 \n",
            "| Epoch [ 36/ 60] Iter[ 96/126]\t\tLoss: 0.2549 \n",
            "| Epoch [ 36/ 60] Iter[101/126]\t\tLoss: 2.5631 \n",
            "| Epoch [ 36/ 60] Iter[106/126]\t\tLoss: 1.9304 \n",
            "| Epoch [ 36/ 60] Iter[111/126]\t\tLoss: 0.4441 \n",
            "| Epoch [ 36/ 60] Iter[116/126]\t\tLoss: 1.2848 \n",
            "| Epoch [ 36/ 60] Iter[121/126]\t\tLoss: 0.5175 \n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.2259 \n",
            "\n",
            "=> Training Epoch #37, LR=0.0000\n",
            "| Epoch [ 37/ 60] Iter[  1/126]\t\tLoss: 1.0765 \n",
            "| Epoch [ 37/ 60] Iter[  6/126]\t\tLoss: 0.4886 \n",
            "| Epoch [ 37/ 60] Iter[ 11/126]\t\tLoss: 3.1417 \n",
            "| Epoch [ 37/ 60] Iter[ 16/126]\t\tLoss: 0.4644 \n",
            "| Epoch [ 37/ 60] Iter[ 21/126]\t\tLoss: 0.3636 \n",
            "| Epoch [ 37/ 60] Iter[ 26/126]\t\tLoss: 0.2901 \n",
            "| Epoch [ 37/ 60] Iter[ 31/126]\t\tLoss: 0.3756 \n",
            "| Epoch [ 37/ 60] Iter[ 36/126]\t\tLoss: 0.6598 \n",
            "| Epoch [ 37/ 60] Iter[ 41/126]\t\tLoss: 0.4359 \n",
            "| Epoch [ 37/ 60] Iter[ 46/126]\t\tLoss: 0.4352 \n",
            "| Epoch [ 37/ 60] Iter[ 51/126]\t\tLoss: 0.5083 \n",
            "| Epoch [ 37/ 60] Iter[ 56/126]\t\tLoss: 0.2944 \n",
            "| Epoch [ 37/ 60] Iter[ 61/126]\t\tLoss: 0.3209 \n",
            "| Epoch [ 37/ 60] Iter[ 66/126]\t\tLoss: 1.2225 \n",
            "| Epoch [ 37/ 60] Iter[ 71/126]\t\tLoss: 0.4944 \n",
            "| Epoch [ 37/ 60] Iter[ 76/126]\t\tLoss: 0.2572 \n",
            "| Epoch [ 37/ 60] Iter[ 81/126]\t\tLoss: 0.2408 \n",
            "| Epoch [ 37/ 60] Iter[ 86/126]\t\tLoss: 2.4612 \n",
            "| Epoch [ 37/ 60] Iter[ 91/126]\t\tLoss: 0.0840 \n",
            "| Epoch [ 37/ 60] Iter[ 96/126]\t\tLoss: 0.4824 \n",
            "| Epoch [ 37/ 60] Iter[101/126]\t\tLoss: 0.0698 \n",
            "| Epoch [ 37/ 60] Iter[106/126]\t\tLoss: 0.4984 \n",
            "| Epoch [ 37/ 60] Iter[111/126]\t\tLoss: 1.0096 \n",
            "| Epoch [ 37/ 60] Iter[116/126]\t\tLoss: 0.5562 \n",
            "| Epoch [ 37/ 60] Iter[121/126]\t\tLoss: 0.6585 \n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 0.2253 \n",
            "\n",
            "=> Training Epoch #38, LR=0.0000\n",
            "| Epoch [ 38/ 60] Iter[  1/126]\t\tLoss: 6.4440 \n",
            "| Epoch [ 38/ 60] Iter[  6/126]\t\tLoss: 0.6576 \n",
            "| Epoch [ 38/ 60] Iter[ 11/126]\t\tLoss: 0.2307 \n",
            "| Epoch [ 38/ 60] Iter[ 16/126]\t\tLoss: 0.4519 \n",
            "| Epoch [ 38/ 60] Iter[ 21/126]\t\tLoss: 0.5680 \n",
            "| Epoch [ 38/ 60] Iter[ 26/126]\t\tLoss: 0.6459 \n",
            "| Epoch [ 38/ 60] Iter[ 31/126]\t\tLoss: 0.4101 \n",
            "| Epoch [ 38/ 60] Iter[ 36/126]\t\tLoss: 0.5537 \n",
            "| Epoch [ 38/ 60] Iter[ 41/126]\t\tLoss: 1.0283 \n",
            "| Epoch [ 38/ 60] Iter[ 46/126]\t\tLoss: 5.6798 \n",
            "| Epoch [ 38/ 60] Iter[ 51/126]\t\tLoss: 0.8252 \n",
            "| Epoch [ 38/ 60] Iter[ 56/126]\t\tLoss: 0.7637 \n",
            "| Epoch [ 38/ 60] Iter[ 61/126]\t\tLoss: 1.0170 \n",
            "| Epoch [ 38/ 60] Iter[ 66/126]\t\tLoss: 0.6611 \n",
            "| Epoch [ 38/ 60] Iter[ 71/126]\t\tLoss: 0.8539 \n",
            "| Epoch [ 38/ 60] Iter[ 76/126]\t\tLoss: 0.1000 \n",
            "| Epoch [ 38/ 60] Iter[ 81/126]\t\tLoss: 0.7763 \n",
            "| Epoch [ 38/ 60] Iter[ 86/126]\t\tLoss: 0.0915 \n",
            "| Epoch [ 38/ 60] Iter[ 91/126]\t\tLoss: 0.4516 \n",
            "| Epoch [ 38/ 60] Iter[ 96/126]\t\tLoss: 0.3829 \n",
            "| Epoch [ 38/ 60] Iter[101/126]\t\tLoss: 0.9464 \n",
            "| Epoch [ 38/ 60] Iter[106/126]\t\tLoss: 0.1092 \n",
            "| Epoch [ 38/ 60] Iter[111/126]\t\tLoss: 0.0374 \n",
            "| Epoch [ 38/ 60] Iter[116/126]\t\tLoss: 0.9169 \n",
            "| Epoch [ 38/ 60] Iter[121/126]\t\tLoss: 0.3603 \n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 0.2255 \n",
            "\n",
            "=> Training Epoch #39, LR=0.0000\n",
            "| Epoch [ 39/ 60] Iter[  1/126]\t\tLoss: 0.5652 \n",
            "| Epoch [ 39/ 60] Iter[  6/126]\t\tLoss: 0.5397 \n",
            "| Epoch [ 39/ 60] Iter[ 11/126]\t\tLoss: 0.5537 \n",
            "| Epoch [ 39/ 60] Iter[ 16/126]\t\tLoss: 1.0010 \n",
            "| Epoch [ 39/ 60] Iter[ 21/126]\t\tLoss: 0.1999 \n",
            "| Epoch [ 39/ 60] Iter[ 26/126]\t\tLoss: 5.5843 \n",
            "| Epoch [ 39/ 60] Iter[ 31/126]\t\tLoss: 1.0006 \n",
            "| Epoch [ 39/ 60] Iter[ 36/126]\t\tLoss: 0.2406 \n",
            "| Epoch [ 39/ 60] Iter[ 41/126]\t\tLoss: 1.3093 \n",
            "| Epoch [ 39/ 60] Iter[ 46/126]\t\tLoss: 6.6381 \n",
            "| Epoch [ 39/ 60] Iter[ 51/126]\t\tLoss: 5.6284 \n",
            "| Epoch [ 39/ 60] Iter[ 56/126]\t\tLoss: 11.7853 \n",
            "| Epoch [ 39/ 60] Iter[ 61/126]\t\tLoss: 0.3083 \n",
            "| Epoch [ 39/ 60] Iter[ 66/126]\t\tLoss: 1.8749 \n",
            "| Epoch [ 39/ 60] Iter[ 71/126]\t\tLoss: 0.3951 \n",
            "| Epoch [ 39/ 60] Iter[ 76/126]\t\tLoss: 1.1216 \n",
            "| Epoch [ 39/ 60] Iter[ 81/126]\t\tLoss: 0.6445 \n",
            "| Epoch [ 39/ 60] Iter[ 86/126]\t\tLoss: 0.2650 \n",
            "| Epoch [ 39/ 60] Iter[ 91/126]\t\tLoss: 0.3413 \n",
            "| Epoch [ 39/ 60] Iter[ 96/126]\t\tLoss: 2.4961 \n",
            "| Epoch [ 39/ 60] Iter[101/126]\t\tLoss: 0.5290 \n",
            "| Epoch [ 39/ 60] Iter[106/126]\t\tLoss: 6.0745 \n",
            "| Epoch [ 39/ 60] Iter[111/126]\t\tLoss: 0.4005 \n",
            "| Epoch [ 39/ 60] Iter[116/126]\t\tLoss: 0.5089 \n",
            "| Epoch [ 39/ 60] Iter[121/126]\t\tLoss: 1.6443 \n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 0.2250 \n",
            "\n",
            "=> Training Epoch #40, LR=0.0000\n",
            "| Epoch [ 40/ 60] Iter[  1/126]\t\tLoss: 0.4228 \n",
            "| Epoch [ 40/ 60] Iter[  6/126]\t\tLoss: 1.1570 \n",
            "| Epoch [ 40/ 60] Iter[ 11/126]\t\tLoss: 0.2602 \n",
            "| Epoch [ 40/ 60] Iter[ 16/126]\t\tLoss: 0.0130 \n",
            "| Epoch [ 40/ 60] Iter[ 21/126]\t\tLoss: 0.6897 \n",
            "| Epoch [ 40/ 60] Iter[ 26/126]\t\tLoss: 0.0509 \n",
            "| Epoch [ 40/ 60] Iter[ 31/126]\t\tLoss: 0.4658 \n",
            "| Epoch [ 40/ 60] Iter[ 36/126]\t\tLoss: 0.3307 \n",
            "| Epoch [ 40/ 60] Iter[ 41/126]\t\tLoss: 0.3240 \n",
            "| Epoch [ 40/ 60] Iter[ 46/126]\t\tLoss: 0.2789 \n",
            "| Epoch [ 40/ 60] Iter[ 51/126]\t\tLoss: 0.6560 \n",
            "| Epoch [ 40/ 60] Iter[ 56/126]\t\tLoss: 1.3293 \n",
            "| Epoch [ 40/ 60] Iter[ 61/126]\t\tLoss: 0.1160 \n",
            "| Epoch [ 40/ 60] Iter[ 66/126]\t\tLoss: 0.4832 \n",
            "| Epoch [ 40/ 60] Iter[ 71/126]\t\tLoss: 0.5349 \n",
            "| Epoch [ 40/ 60] Iter[ 76/126]\t\tLoss: 1.7985 \n",
            "| Epoch [ 40/ 60] Iter[ 81/126]\t\tLoss: 0.3301 \n",
            "| Epoch [ 40/ 60] Iter[ 86/126]\t\tLoss: 0.6621 \n",
            "| Epoch [ 40/ 60] Iter[ 91/126]\t\tLoss: 7.2192 \n",
            "| Epoch [ 40/ 60] Iter[ 96/126]\t\tLoss: 0.5587 \n",
            "| Epoch [ 40/ 60] Iter[101/126]\t\tLoss: 0.3091 \n",
            "| Epoch [ 40/ 60] Iter[106/126]\t\tLoss: 0.6992 \n",
            "| Epoch [ 40/ 60] Iter[111/126]\t\tLoss: 0.5463 \n",
            "| Epoch [ 40/ 60] Iter[116/126]\t\tLoss: 0.5931 \n",
            "| Epoch [ 40/ 60] Iter[121/126]\t\tLoss: 0.2742 \n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 0.2253 \n",
            "\n",
            "=> Training Epoch #41, LR=0.0000\n",
            "| Epoch [ 41/ 60] Iter[  1/126]\t\tLoss: 0.7023 \n",
            "| Epoch [ 41/ 60] Iter[  6/126]\t\tLoss: 0.2968 \n",
            "| Epoch [ 41/ 60] Iter[ 11/126]\t\tLoss: 0.7448 \n",
            "| Epoch [ 41/ 60] Iter[ 16/126]\t\tLoss: 6.1199 \n",
            "| Epoch [ 41/ 60] Iter[ 21/126]\t\tLoss: 0.7947 \n",
            "| Epoch [ 41/ 60] Iter[ 26/126]\t\tLoss: 3.5628 \n",
            "| Epoch [ 41/ 60] Iter[ 31/126]\t\tLoss: 0.1723 \n",
            "| Epoch [ 41/ 60] Iter[ 36/126]\t\tLoss: 0.5102 \n",
            "| Epoch [ 41/ 60] Iter[ 41/126]\t\tLoss: 6.6316 \n",
            "| Epoch [ 41/ 60] Iter[ 46/126]\t\tLoss: 5.8264 \n",
            "| Epoch [ 41/ 60] Iter[ 51/126]\t\tLoss: 0.2395 \n",
            "| Epoch [ 41/ 60] Iter[ 56/126]\t\tLoss: 0.1657 \n",
            "| Epoch [ 41/ 60] Iter[ 61/126]\t\tLoss: 5.9019 \n",
            "| Epoch [ 41/ 60] Iter[ 66/126]\t\tLoss: 0.6620 \n",
            "| Epoch [ 41/ 60] Iter[ 71/126]\t\tLoss: 0.9451 \n",
            "| Epoch [ 41/ 60] Iter[ 76/126]\t\tLoss: 0.3978 \n",
            "| Epoch [ 41/ 60] Iter[ 81/126]\t\tLoss: 0.3410 \n",
            "| Epoch [ 41/ 60] Iter[ 86/126]\t\tLoss: 0.6538 \n",
            "| Epoch [ 41/ 60] Iter[ 91/126]\t\tLoss: 0.8511 \n",
            "| Epoch [ 41/ 60] Iter[ 96/126]\t\tLoss: 1.1013 \n",
            "| Epoch [ 41/ 60] Iter[101/126]\t\tLoss: 2.5518 \n",
            "| Epoch [ 41/ 60] Iter[106/126]\t\tLoss: 0.2608 \n",
            "| Epoch [ 41/ 60] Iter[111/126]\t\tLoss: 6.2822 \n",
            "| Epoch [ 41/ 60] Iter[116/126]\t\tLoss: 0.4895 \n",
            "| Epoch [ 41/ 60] Iter[121/126]\t\tLoss: 0.5072 \n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 0.2259 \n",
            "\n",
            "=> Training Epoch #42, LR=0.0000\n",
            "| Epoch [ 42/ 60] Iter[  1/126]\t\tLoss: 0.5089 \n",
            "| Epoch [ 42/ 60] Iter[  6/126]\t\tLoss: 0.5403 \n",
            "| Epoch [ 42/ 60] Iter[ 11/126]\t\tLoss: 0.8872 \n",
            "| Epoch [ 42/ 60] Iter[ 16/126]\t\tLoss: 0.0546 \n",
            "| Epoch [ 42/ 60] Iter[ 21/126]\t\tLoss: 0.1750 \n",
            "| Epoch [ 42/ 60] Iter[ 26/126]\t\tLoss: 0.0587 \n",
            "| Epoch [ 42/ 60] Iter[ 31/126]\t\tLoss: 1.1378 \n",
            "| Epoch [ 42/ 60] Iter[ 36/126]\t\tLoss: 1.0359 \n",
            "| Epoch [ 42/ 60] Iter[ 41/126]\t\tLoss: 0.5554 \n",
            "| Epoch [ 42/ 60] Iter[ 46/126]\t\tLoss: 0.5697 \n",
            "| Epoch [ 42/ 60] Iter[ 51/126]\t\tLoss: 0.3915 \n",
            "| Epoch [ 42/ 60] Iter[ 56/126]\t\tLoss: 0.8926 \n",
            "| Epoch [ 42/ 60] Iter[ 61/126]\t\tLoss: 0.5263 \n",
            "| Epoch [ 42/ 60] Iter[ 66/126]\t\tLoss: 0.5738 \n",
            "| Epoch [ 42/ 60] Iter[ 71/126]\t\tLoss: 0.8812 \n",
            "| Epoch [ 42/ 60] Iter[ 76/126]\t\tLoss: 0.4600 \n",
            "| Epoch [ 42/ 60] Iter[ 81/126]\t\tLoss: 0.3559 \n",
            "| Epoch [ 42/ 60] Iter[ 86/126]\t\tLoss: 0.2626 \n",
            "| Epoch [ 42/ 60] Iter[ 91/126]\t\tLoss: 1.5477 \n",
            "| Epoch [ 42/ 60] Iter[ 96/126]\t\tLoss: 0.5095 \n",
            "| Epoch [ 42/ 60] Iter[101/126]\t\tLoss: 0.4687 \n",
            "| Epoch [ 42/ 60] Iter[106/126]\t\tLoss: 0.1183 \n",
            "| Epoch [ 42/ 60] Iter[111/126]\t\tLoss: 0.2916 \n",
            "| Epoch [ 42/ 60] Iter[116/126]\t\tLoss: 1.1151 \n",
            "| Epoch [ 42/ 60] Iter[121/126]\t\tLoss: 0.2360 \n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 0.2265 \n",
            "\n",
            "=> Training Epoch #43, LR=0.0000\n",
            "| Epoch [ 43/ 60] Iter[  1/126]\t\tLoss: 0.7483 \n",
            "| Epoch [ 43/ 60] Iter[  6/126]\t\tLoss: 1.6821 \n",
            "| Epoch [ 43/ 60] Iter[ 11/126]\t\tLoss: 1.4989 \n",
            "| Epoch [ 43/ 60] Iter[ 16/126]\t\tLoss: 5.7766 \n",
            "| Epoch [ 43/ 60] Iter[ 21/126]\t\tLoss: 0.2723 \n",
            "| Epoch [ 43/ 60] Iter[ 26/126]\t\tLoss: 0.1119 \n",
            "| Epoch [ 43/ 60] Iter[ 31/126]\t\tLoss: 0.7735 \n",
            "| Epoch [ 43/ 60] Iter[ 36/126]\t\tLoss: 0.9494 \n",
            "| Epoch [ 43/ 60] Iter[ 41/126]\t\tLoss: 0.6168 \n",
            "| Epoch [ 43/ 60] Iter[ 46/126]\t\tLoss: 0.8465 \n",
            "| Epoch [ 43/ 60] Iter[ 51/126]\t\tLoss: 0.6959 \n",
            "| Epoch [ 43/ 60] Iter[ 56/126]\t\tLoss: 1.0699 \n",
            "| Epoch [ 43/ 60] Iter[ 61/126]\t\tLoss: 1.4021 \n",
            "| Epoch [ 43/ 60] Iter[ 66/126]\t\tLoss: 0.5044 \n",
            "| Epoch [ 43/ 60] Iter[ 71/126]\t\tLoss: 0.4290 \n",
            "| Epoch [ 43/ 60] Iter[ 76/126]\t\tLoss: 0.1851 \n",
            "| Epoch [ 43/ 60] Iter[ 81/126]\t\tLoss: 6.2740 \n",
            "| Epoch [ 43/ 60] Iter[ 86/126]\t\tLoss: 0.3494 \n",
            "| Epoch [ 43/ 60] Iter[ 91/126]\t\tLoss: 0.5177 \n",
            "| Epoch [ 43/ 60] Iter[ 96/126]\t\tLoss: 0.5134 \n",
            "| Epoch [ 43/ 60] Iter[101/126]\t\tLoss: 0.5308 \n",
            "| Epoch [ 43/ 60] Iter[106/126]\t\tLoss: 6.0344 \n",
            "| Epoch [ 43/ 60] Iter[111/126]\t\tLoss: 0.1852 \n",
            "| Epoch [ 43/ 60] Iter[116/126]\t\tLoss: 0.1797 \n",
            "| Epoch [ 43/ 60] Iter[121/126]\t\tLoss: 0.1959 \n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #44, LR=0.0000\n",
            "| Epoch [ 44/ 60] Iter[  1/126]\t\tLoss: 0.9167 \n",
            "| Epoch [ 44/ 60] Iter[  6/126]\t\tLoss: 0.5503 \n",
            "| Epoch [ 44/ 60] Iter[ 11/126]\t\tLoss: 1.7320 \n",
            "| Epoch [ 44/ 60] Iter[ 16/126]\t\tLoss: 0.5022 \n",
            "| Epoch [ 44/ 60] Iter[ 21/126]\t\tLoss: 0.3694 \n",
            "| Epoch [ 44/ 60] Iter[ 26/126]\t\tLoss: 0.3059 \n",
            "| Epoch [ 44/ 60] Iter[ 31/126]\t\tLoss: 0.3757 \n",
            "| Epoch [ 44/ 60] Iter[ 36/126]\t\tLoss: 0.9840 \n",
            "| Epoch [ 44/ 60] Iter[ 41/126]\t\tLoss: 0.1886 \n",
            "| Epoch [ 44/ 60] Iter[ 46/126]\t\tLoss: 0.2491 \n",
            "| Epoch [ 44/ 60] Iter[ 51/126]\t\tLoss: 2.9419 \n",
            "| Epoch [ 44/ 60] Iter[ 56/126]\t\tLoss: 6.0101 \n",
            "| Epoch [ 44/ 60] Iter[ 61/126]\t\tLoss: 0.7020 \n",
            "| Epoch [ 44/ 60] Iter[ 66/126]\t\tLoss: 1.1347 \n",
            "| Epoch [ 44/ 60] Iter[ 71/126]\t\tLoss: 6.4723 \n",
            "| Epoch [ 44/ 60] Iter[ 76/126]\t\tLoss: 1.5404 \n",
            "| Epoch [ 44/ 60] Iter[ 81/126]\t\tLoss: 0.4394 \n",
            "| Epoch [ 44/ 60] Iter[ 86/126]\t\tLoss: 0.7992 \n",
            "| Epoch [ 44/ 60] Iter[ 91/126]\t\tLoss: 0.3635 \n",
            "| Epoch [ 44/ 60] Iter[ 96/126]\t\tLoss: 0.4843 \n",
            "| Epoch [ 44/ 60] Iter[101/126]\t\tLoss: 0.3355 \n",
            "| Epoch [ 44/ 60] Iter[106/126]\t\tLoss: 0.5506 \n",
            "| Epoch [ 44/ 60] Iter[111/126]\t\tLoss: 0.3601 \n",
            "| Epoch [ 44/ 60] Iter[116/126]\t\tLoss: 1.0733 \n",
            "| Epoch [ 44/ 60] Iter[121/126]\t\tLoss: 4.2086 \n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 0.2259 \n",
            "\n",
            "=> Training Epoch #45, LR=0.0000\n",
            "| Epoch [ 45/ 60] Iter[  1/126]\t\tLoss: 0.0849 \n",
            "| Epoch [ 45/ 60] Iter[  6/126]\t\tLoss: 0.1625 \n",
            "| Epoch [ 45/ 60] Iter[ 11/126]\t\tLoss: 0.5277 \n",
            "| Epoch [ 45/ 60] Iter[ 16/126]\t\tLoss: 0.2668 \n",
            "| Epoch [ 45/ 60] Iter[ 21/126]\t\tLoss: 0.4931 \n",
            "| Epoch [ 45/ 60] Iter[ 26/126]\t\tLoss: 0.3263 \n",
            "| Epoch [ 45/ 60] Iter[ 31/126]\t\tLoss: 0.7147 \n",
            "| Epoch [ 45/ 60] Iter[ 36/126]\t\tLoss: 0.9564 \n",
            "| Epoch [ 45/ 60] Iter[ 41/126]\t\tLoss: 0.9008 \n",
            "| Epoch [ 45/ 60] Iter[ 46/126]\t\tLoss: 0.6039 \n",
            "| Epoch [ 45/ 60] Iter[ 51/126]\t\tLoss: 6.1093 \n",
            "| Epoch [ 45/ 60] Iter[ 56/126]\t\tLoss: 0.2894 \n",
            "| Epoch [ 45/ 60] Iter[ 61/126]\t\tLoss: 0.4228 \n",
            "| Epoch [ 45/ 60] Iter[ 66/126]\t\tLoss: 0.2007 \n",
            "| Epoch [ 45/ 60] Iter[ 71/126]\t\tLoss: 0.5485 \n",
            "| Epoch [ 45/ 60] Iter[ 76/126]\t\tLoss: 0.0783 \n",
            "| Epoch [ 45/ 60] Iter[ 81/126]\t\tLoss: 6.0700 \n",
            "| Epoch [ 45/ 60] Iter[ 86/126]\t\tLoss: 0.2605 \n",
            "| Epoch [ 45/ 60] Iter[ 91/126]\t\tLoss: 0.5831 \n",
            "| Epoch [ 45/ 60] Iter[ 96/126]\t\tLoss: 0.0569 \n",
            "| Epoch [ 45/ 60] Iter[101/126]\t\tLoss: 1.0536 \n",
            "| Epoch [ 45/ 60] Iter[106/126]\t\tLoss: 1.0281 \n",
            "| Epoch [ 45/ 60] Iter[111/126]\t\tLoss: 0.8108 \n",
            "| Epoch [ 45/ 60] Iter[116/126]\t\tLoss: 0.4827 \n",
            "| Epoch [ 45/ 60] Iter[121/126]\t\tLoss: 0.3079 \n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 0.2257 \n",
            "\n",
            "=> Training Epoch #46, LR=0.0000\n",
            "| Epoch [ 46/ 60] Iter[  1/126]\t\tLoss: 0.3659 \n",
            "| Epoch [ 46/ 60] Iter[  6/126]\t\tLoss: 0.7473 \n",
            "| Epoch [ 46/ 60] Iter[ 11/126]\t\tLoss: 0.6669 \n",
            "| Epoch [ 46/ 60] Iter[ 16/126]\t\tLoss: 2.1935 \n",
            "| Epoch [ 46/ 60] Iter[ 21/126]\t\tLoss: 0.4133 \n",
            "| Epoch [ 46/ 60] Iter[ 26/126]\t\tLoss: 0.4943 \n",
            "| Epoch [ 46/ 60] Iter[ 31/126]\t\tLoss: 0.2235 \n",
            "| Epoch [ 46/ 60] Iter[ 36/126]\t\tLoss: 0.7222 \n",
            "| Epoch [ 46/ 60] Iter[ 41/126]\t\tLoss: 0.5585 \n",
            "| Epoch [ 46/ 60] Iter[ 46/126]\t\tLoss: 0.2011 \n",
            "| Epoch [ 46/ 60] Iter[ 51/126]\t\tLoss: 0.6671 \n",
            "| Epoch [ 46/ 60] Iter[ 56/126]\t\tLoss: 11.9367 \n",
            "| Epoch [ 46/ 60] Iter[ 61/126]\t\tLoss: 0.4743 \n",
            "| Epoch [ 46/ 60] Iter[ 66/126]\t\tLoss: 1.1864 \n",
            "| Epoch [ 46/ 60] Iter[ 71/126]\t\tLoss: 6.2712 \n",
            "| Epoch [ 46/ 60] Iter[ 76/126]\t\tLoss: 0.7272 \n",
            "| Epoch [ 46/ 60] Iter[ 81/126]\t\tLoss: 0.9814 \n",
            "| Epoch [ 46/ 60] Iter[ 86/126]\t\tLoss: 0.2017 \n",
            "| Epoch [ 46/ 60] Iter[ 91/126]\t\tLoss: 0.1070 \n",
            "| Epoch [ 46/ 60] Iter[ 96/126]\t\tLoss: 0.5225 \n",
            "| Epoch [ 46/ 60] Iter[101/126]\t\tLoss: 0.3011 \n",
            "| Epoch [ 46/ 60] Iter[106/126]\t\tLoss: 0.9446 \n",
            "| Epoch [ 46/ 60] Iter[111/126]\t\tLoss: 0.8374 \n",
            "| Epoch [ 46/ 60] Iter[116/126]\t\tLoss: 0.7827 \n",
            "| Epoch [ 46/ 60] Iter[121/126]\t\tLoss: 0.4547 \n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 0.2259 \n",
            "\n",
            "=> Training Epoch #47, LR=0.0000\n",
            "| Epoch [ 47/ 60] Iter[  1/126]\t\tLoss: 0.8520 \n",
            "| Epoch [ 47/ 60] Iter[  6/126]\t\tLoss: 0.5279 \n",
            "| Epoch [ 47/ 60] Iter[ 11/126]\t\tLoss: 0.2107 \n",
            "| Epoch [ 47/ 60] Iter[ 16/126]\t\tLoss: 0.3427 \n",
            "| Epoch [ 47/ 60] Iter[ 21/126]\t\tLoss: 3.6022 \n",
            "| Epoch [ 47/ 60] Iter[ 26/126]\t\tLoss: 0.5592 \n",
            "| Epoch [ 47/ 60] Iter[ 31/126]\t\tLoss: 6.0774 \n",
            "| Epoch [ 47/ 60] Iter[ 36/126]\t\tLoss: 0.6848 \n",
            "| Epoch [ 47/ 60] Iter[ 41/126]\t\tLoss: 0.2417 \n",
            "| Epoch [ 47/ 60] Iter[ 46/126]\t\tLoss: 0.0800 \n",
            "| Epoch [ 47/ 60] Iter[ 51/126]\t\tLoss: 1.7820 \n",
            "| Epoch [ 47/ 60] Iter[ 56/126]\t\tLoss: 0.6385 \n",
            "| Epoch [ 47/ 60] Iter[ 61/126]\t\tLoss: 6.1721 \n",
            "| Epoch [ 47/ 60] Iter[ 66/126]\t\tLoss: 0.2742 \n",
            "| Epoch [ 47/ 60] Iter[ 71/126]\t\tLoss: 0.1828 \n",
            "| Epoch [ 47/ 60] Iter[ 76/126]\t\tLoss: 0.7314 \n",
            "| Epoch [ 47/ 60] Iter[ 81/126]\t\tLoss: 0.7307 \n",
            "| Epoch [ 47/ 60] Iter[ 86/126]\t\tLoss: 0.5153 \n",
            "| Epoch [ 47/ 60] Iter[ 91/126]\t\tLoss: 0.6621 \n",
            "| Epoch [ 47/ 60] Iter[ 96/126]\t\tLoss: 6.4149 \n",
            "| Epoch [ 47/ 60] Iter[101/126]\t\tLoss: 0.3770 \n",
            "| Epoch [ 47/ 60] Iter[106/126]\t\tLoss: 0.4731 \n",
            "| Epoch [ 47/ 60] Iter[111/126]\t\tLoss: 0.6695 \n",
            "| Epoch [ 47/ 60] Iter[116/126]\t\tLoss: 0.1426 \n",
            "| Epoch [ 47/ 60] Iter[121/126]\t\tLoss: 0.2408 \n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 0.2258 \n",
            "\n",
            "=> Training Epoch #48, LR=0.0000\n",
            "| Epoch [ 48/ 60] Iter[  1/126]\t\tLoss: 0.7960 \n",
            "| Epoch [ 48/ 60] Iter[  6/126]\t\tLoss: 0.7320 \n",
            "| Epoch [ 48/ 60] Iter[ 11/126]\t\tLoss: 0.5615 \n",
            "| Epoch [ 48/ 60] Iter[ 16/126]\t\tLoss: 0.5563 \n",
            "| Epoch [ 48/ 60] Iter[ 21/126]\t\tLoss: 0.6901 \n",
            "| Epoch [ 48/ 60] Iter[ 26/126]\t\tLoss: 0.3245 \n",
            "| Epoch [ 48/ 60] Iter[ 31/126]\t\tLoss: 0.2244 \n",
            "| Epoch [ 48/ 60] Iter[ 36/126]\t\tLoss: 5.9456 \n",
            "| Epoch [ 48/ 60] Iter[ 41/126]\t\tLoss: 0.1854 \n",
            "| Epoch [ 48/ 60] Iter[ 46/126]\t\tLoss: 5.9231 \n",
            "| Epoch [ 48/ 60] Iter[ 51/126]\t\tLoss: 6.1184 \n",
            "| Epoch [ 48/ 60] Iter[ 56/126]\t\tLoss: 0.3620 \n",
            "| Epoch [ 48/ 60] Iter[ 61/126]\t\tLoss: 0.7759 \n",
            "| Epoch [ 48/ 60] Iter[ 66/126]\t\tLoss: 0.6188 \n",
            "| Epoch [ 48/ 60] Iter[ 71/126]\t\tLoss: 0.7582 \n",
            "| Epoch [ 48/ 60] Iter[ 76/126]\t\tLoss: 0.3879 \n",
            "| Epoch [ 48/ 60] Iter[ 81/126]\t\tLoss: 0.6139 \n",
            "| Epoch [ 48/ 60] Iter[ 86/126]\t\tLoss: 0.7298 \n",
            "| Epoch [ 48/ 60] Iter[ 91/126]\t\tLoss: 0.2954 \n",
            "| Epoch [ 48/ 60] Iter[ 96/126]\t\tLoss: 1.5930 \n",
            "| Epoch [ 48/ 60] Iter[101/126]\t\tLoss: 1.1077 \n",
            "| Epoch [ 48/ 60] Iter[106/126]\t\tLoss: 0.4900 \n",
            "| Epoch [ 48/ 60] Iter[111/126]\t\tLoss: 0.1690 \n",
            "| Epoch [ 48/ 60] Iter[116/126]\t\tLoss: 0.2842 \n",
            "| Epoch [ 48/ 60] Iter[121/126]\t\tLoss: 0.2634 \n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 0.2259 \n",
            "\n",
            "=> Training Epoch #49, LR=0.0000\n",
            "| Epoch [ 49/ 60] Iter[  1/126]\t\tLoss: 0.6875 \n",
            "| Epoch [ 49/ 60] Iter[  6/126]\t\tLoss: 0.8206 \n",
            "| Epoch [ 49/ 60] Iter[ 11/126]\t\tLoss: 1.1789 \n",
            "| Epoch [ 49/ 60] Iter[ 16/126]\t\tLoss: 1.2190 \n",
            "| Epoch [ 49/ 60] Iter[ 21/126]\t\tLoss: 5.2803 \n",
            "| Epoch [ 49/ 60] Iter[ 26/126]\t\tLoss: 0.3741 \n",
            "| Epoch [ 49/ 60] Iter[ 31/126]\t\tLoss: 0.3763 \n",
            "| Epoch [ 49/ 60] Iter[ 36/126]\t\tLoss: 6.0950 \n",
            "| Epoch [ 49/ 60] Iter[ 41/126]\t\tLoss: 1.1679 \n",
            "| Epoch [ 49/ 60] Iter[ 46/126]\t\tLoss: 0.5919 \n",
            "| Epoch [ 49/ 60] Iter[ 51/126]\t\tLoss: 0.8208 \n",
            "| Epoch [ 49/ 60] Iter[ 56/126]\t\tLoss: 0.4396 \n",
            "| Epoch [ 49/ 60] Iter[ 61/126]\t\tLoss: 0.3598 \n",
            "| Epoch [ 49/ 60] Iter[ 66/126]\t\tLoss: 0.5337 \n",
            "| Epoch [ 49/ 60] Iter[ 71/126]\t\tLoss: 0.8377 \n",
            "| Epoch [ 49/ 60] Iter[ 76/126]\t\tLoss: 0.4747 \n",
            "| Epoch [ 49/ 60] Iter[ 81/126]\t\tLoss: 0.0886 \n",
            "| Epoch [ 49/ 60] Iter[ 86/126]\t\tLoss: 0.5204 \n",
            "| Epoch [ 49/ 60] Iter[ 91/126]\t\tLoss: 0.7517 \n",
            "| Epoch [ 49/ 60] Iter[ 96/126]\t\tLoss: 0.4779 \n",
            "| Epoch [ 49/ 60] Iter[101/126]\t\tLoss: 0.2013 \n",
            "| Epoch [ 49/ 60] Iter[106/126]\t\tLoss: 6.5895 \n",
            "| Epoch [ 49/ 60] Iter[111/126]\t\tLoss: 0.6364 \n",
            "| Epoch [ 49/ 60] Iter[116/126]\t\tLoss: 0.7490 \n",
            "| Epoch [ 49/ 60] Iter[121/126]\t\tLoss: 0.5153 \n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #50, LR=0.0000\n",
            "| Epoch [ 50/ 60] Iter[  1/126]\t\tLoss: 6.2794 \n",
            "| Epoch [ 50/ 60] Iter[  6/126]\t\tLoss: 0.7412 \n",
            "| Epoch [ 50/ 60] Iter[ 11/126]\t\tLoss: 0.7939 \n",
            "| Epoch [ 50/ 60] Iter[ 16/126]\t\tLoss: 0.3660 \n",
            "| Epoch [ 50/ 60] Iter[ 21/126]\t\tLoss: 1.5704 \n",
            "| Epoch [ 50/ 60] Iter[ 26/126]\t\tLoss: 0.6736 \n",
            "| Epoch [ 50/ 60] Iter[ 31/126]\t\tLoss: 0.3460 \n",
            "| Epoch [ 50/ 60] Iter[ 36/126]\t\tLoss: 0.8545 \n",
            "| Epoch [ 50/ 60] Iter[ 41/126]\t\tLoss: 0.6544 \n",
            "| Epoch [ 50/ 60] Iter[ 46/126]\t\tLoss: 0.0291 \n",
            "| Epoch [ 50/ 60] Iter[ 51/126]\t\tLoss: 3.7706 \n",
            "| Epoch [ 50/ 60] Iter[ 56/126]\t\tLoss: 0.2538 \n",
            "| Epoch [ 50/ 60] Iter[ 61/126]\t\tLoss: 0.8548 \n",
            "| Epoch [ 50/ 60] Iter[ 66/126]\t\tLoss: 0.6040 \n",
            "| Epoch [ 50/ 60] Iter[ 71/126]\t\tLoss: 1.2304 \n",
            "| Epoch [ 50/ 60] Iter[ 76/126]\t\tLoss: 0.6127 \n",
            "| Epoch [ 50/ 60] Iter[ 81/126]\t\tLoss: 5.8149 \n",
            "| Epoch [ 50/ 60] Iter[ 86/126]\t\tLoss: 0.3032 \n",
            "| Epoch [ 50/ 60] Iter[ 91/126]\t\tLoss: 0.2690 \n",
            "| Epoch [ 50/ 60] Iter[ 96/126]\t\tLoss: 0.3068 \n",
            "| Epoch [ 50/ 60] Iter[101/126]\t\tLoss: 1.0914 \n",
            "| Epoch [ 50/ 60] Iter[106/126]\t\tLoss: 1.3288 \n",
            "| Epoch [ 50/ 60] Iter[111/126]\t\tLoss: 0.6410 \n",
            "| Epoch [ 50/ 60] Iter[116/126]\t\tLoss: 0.4868 \n",
            "| Epoch [ 50/ 60] Iter[121/126]\t\tLoss: 0.3932 \n",
            "\n",
            "| Validation Epoch #50\t\t\tLoss: 0.2259 \n",
            "\n",
            "=> Training Epoch #51, LR=0.0000\n",
            "| Epoch [ 51/ 60] Iter[  1/126]\t\tLoss: 0.9703 \n",
            "| Epoch [ 51/ 60] Iter[  6/126]\t\tLoss: 0.4568 \n",
            "| Epoch [ 51/ 60] Iter[ 11/126]\t\tLoss: 0.4179 \n",
            "| Epoch [ 51/ 60] Iter[ 16/126]\t\tLoss: 0.6351 \n",
            "| Epoch [ 51/ 60] Iter[ 21/126]\t\tLoss: 0.2294 \n",
            "| Epoch [ 51/ 60] Iter[ 26/126]\t\tLoss: 6.3853 \n",
            "| Epoch [ 51/ 60] Iter[ 31/126]\t\tLoss: 0.5619 \n",
            "| Epoch [ 51/ 60] Iter[ 36/126]\t\tLoss: 0.7074 \n",
            "| Epoch [ 51/ 60] Iter[ 41/126]\t\tLoss: 1.0385 \n",
            "| Epoch [ 51/ 60] Iter[ 46/126]\t\tLoss: 0.4263 \n",
            "| Epoch [ 51/ 60] Iter[ 51/126]\t\tLoss: 0.9765 \n",
            "| Epoch [ 51/ 60] Iter[ 56/126]\t\tLoss: 1.6564 \n",
            "| Epoch [ 51/ 60] Iter[ 61/126]\t\tLoss: 0.3921 \n",
            "| Epoch [ 51/ 60] Iter[ 66/126]\t\tLoss: 1.1581 \n",
            "| Epoch [ 51/ 60] Iter[ 71/126]\t\tLoss: 6.2116 \n",
            "| Epoch [ 51/ 60] Iter[ 76/126]\t\tLoss: 5.6943 \n",
            "| Epoch [ 51/ 60] Iter[ 81/126]\t\tLoss: 0.3881 \n",
            "| Epoch [ 51/ 60] Iter[ 86/126]\t\tLoss: 0.6700 \n",
            "| Epoch [ 51/ 60] Iter[ 91/126]\t\tLoss: 0.7703 \n",
            "| Epoch [ 51/ 60] Iter[ 96/126]\t\tLoss: 0.3107 \n",
            "| Epoch [ 51/ 60] Iter[101/126]\t\tLoss: 0.5423 \n",
            "| Epoch [ 51/ 60] Iter[106/126]\t\tLoss: 0.3405 \n",
            "| Epoch [ 51/ 60] Iter[111/126]\t\tLoss: 0.4065 \n",
            "| Epoch [ 51/ 60] Iter[116/126]\t\tLoss: 0.1420 \n",
            "| Epoch [ 51/ 60] Iter[121/126]\t\tLoss: 0.4011 \n",
            "\n",
            "| Validation Epoch #51\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #52, LR=0.0000\n",
            "| Epoch [ 52/ 60] Iter[  1/126]\t\tLoss: 0.9177 \n",
            "| Epoch [ 52/ 60] Iter[  6/126]\t\tLoss: 0.4845 \n",
            "| Epoch [ 52/ 60] Iter[ 11/126]\t\tLoss: 0.4665 \n",
            "| Epoch [ 52/ 60] Iter[ 16/126]\t\tLoss: 0.7679 \n",
            "| Epoch [ 52/ 60] Iter[ 21/126]\t\tLoss: 0.4485 \n",
            "| Epoch [ 52/ 60] Iter[ 26/126]\t\tLoss: 1.4039 \n",
            "| Epoch [ 52/ 60] Iter[ 31/126]\t\tLoss: 0.4840 \n",
            "| Epoch [ 52/ 60] Iter[ 36/126]\t\tLoss: 0.8911 \n",
            "| Epoch [ 52/ 60] Iter[ 41/126]\t\tLoss: 0.3503 \n",
            "| Epoch [ 52/ 60] Iter[ 46/126]\t\tLoss: 0.4090 \n",
            "| Epoch [ 52/ 60] Iter[ 51/126]\t\tLoss: 0.4308 \n",
            "| Epoch [ 52/ 60] Iter[ 56/126]\t\tLoss: 0.1313 \n",
            "| Epoch [ 52/ 60] Iter[ 61/126]\t\tLoss: 0.1145 \n",
            "| Epoch [ 52/ 60] Iter[ 66/126]\t\tLoss: 5.8440 \n",
            "| Epoch [ 52/ 60] Iter[ 71/126]\t\tLoss: 1.0438 \n",
            "| Epoch [ 52/ 60] Iter[ 76/126]\t\tLoss: 0.5827 \n",
            "| Epoch [ 52/ 60] Iter[ 81/126]\t\tLoss: 0.2311 \n",
            "| Epoch [ 52/ 60] Iter[ 86/126]\t\tLoss: 6.3215 \n",
            "| Epoch [ 52/ 60] Iter[ 91/126]\t\tLoss: 0.5393 \n",
            "| Epoch [ 52/ 60] Iter[ 96/126]\t\tLoss: 0.1110 \n",
            "| Epoch [ 52/ 60] Iter[101/126]\t\tLoss: 0.6823 \n",
            "| Epoch [ 52/ 60] Iter[106/126]\t\tLoss: 0.5055 \n",
            "| Epoch [ 52/ 60] Iter[111/126]\t\tLoss: 0.3024 \n",
            "| Epoch [ 52/ 60] Iter[116/126]\t\tLoss: 0.0693 \n",
            "| Epoch [ 52/ 60] Iter[121/126]\t\tLoss: 1.4907 \n",
            "\n",
            "| Validation Epoch #52\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #53, LR=0.0000\n",
            "| Epoch [ 53/ 60] Iter[  1/126]\t\tLoss: 0.2709 \n",
            "| Epoch [ 53/ 60] Iter[  6/126]\t\tLoss: 6.6164 \n",
            "| Epoch [ 53/ 60] Iter[ 11/126]\t\tLoss: 0.8762 \n",
            "| Epoch [ 53/ 60] Iter[ 16/126]\t\tLoss: 0.6564 \n",
            "| Epoch [ 53/ 60] Iter[ 21/126]\t\tLoss: 0.5652 \n",
            "| Epoch [ 53/ 60] Iter[ 26/126]\t\tLoss: 1.1586 \n",
            "| Epoch [ 53/ 60] Iter[ 31/126]\t\tLoss: 0.3637 \n",
            "| Epoch [ 53/ 60] Iter[ 36/126]\t\tLoss: 0.8536 \n",
            "| Epoch [ 53/ 60] Iter[ 41/126]\t\tLoss: 0.5508 \n",
            "| Epoch [ 53/ 60] Iter[ 46/126]\t\tLoss: 0.5198 \n",
            "| Epoch [ 53/ 60] Iter[ 51/126]\t\tLoss: 0.2954 \n",
            "| Epoch [ 53/ 60] Iter[ 56/126]\t\tLoss: 2.8678 \n",
            "| Epoch [ 53/ 60] Iter[ 61/126]\t\tLoss: 0.4358 \n",
            "| Epoch [ 53/ 60] Iter[ 66/126]\t\tLoss: 0.2349 \n",
            "| Epoch [ 53/ 60] Iter[ 71/126]\t\tLoss: 0.4560 \n",
            "| Epoch [ 53/ 60] Iter[ 76/126]\t\tLoss: 1.4237 \n",
            "| Epoch [ 53/ 60] Iter[ 81/126]\t\tLoss: 1.0398 \n",
            "| Epoch [ 53/ 60] Iter[ 86/126]\t\tLoss: 0.1624 \n",
            "| Epoch [ 53/ 60] Iter[ 91/126]\t\tLoss: 0.0739 \n",
            "| Epoch [ 53/ 60] Iter[ 96/126]\t\tLoss: 0.5600 \n",
            "| Epoch [ 53/ 60] Iter[101/126]\t\tLoss: 0.7877 \n",
            "| Epoch [ 53/ 60] Iter[106/126]\t\tLoss: 0.1353 \n",
            "| Epoch [ 53/ 60] Iter[111/126]\t\tLoss: 0.3554 \n",
            "| Epoch [ 53/ 60] Iter[116/126]\t\tLoss: 0.1204 \n",
            "| Epoch [ 53/ 60] Iter[121/126]\t\tLoss: 0.8811 \n",
            "\n",
            "| Validation Epoch #53\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #54, LR=0.0000\n",
            "| Epoch [ 54/ 60] Iter[  1/126]\t\tLoss: 0.6047 \n",
            "| Epoch [ 54/ 60] Iter[  6/126]\t\tLoss: 0.3011 \n",
            "| Epoch [ 54/ 60] Iter[ 11/126]\t\tLoss: 0.1991 \n",
            "| Epoch [ 54/ 60] Iter[ 16/126]\t\tLoss: 0.0704 \n",
            "| Epoch [ 54/ 60] Iter[ 21/126]\t\tLoss: 0.3486 \n",
            "| Epoch [ 54/ 60] Iter[ 26/126]\t\tLoss: 5.6541 \n",
            "| Epoch [ 54/ 60] Iter[ 31/126]\t\tLoss: 0.6621 \n",
            "| Epoch [ 54/ 60] Iter[ 36/126]\t\tLoss: 0.7278 \n",
            "| Epoch [ 54/ 60] Iter[ 41/126]\t\tLoss: 0.4034 \n",
            "| Epoch [ 54/ 60] Iter[ 46/126]\t\tLoss: 0.3270 \n",
            "| Epoch [ 54/ 60] Iter[ 51/126]\t\tLoss: 0.5064 \n",
            "| Epoch [ 54/ 60] Iter[ 56/126]\t\tLoss: 0.4428 \n",
            "| Epoch [ 54/ 60] Iter[ 61/126]\t\tLoss: 0.5563 \n",
            "| Epoch [ 54/ 60] Iter[ 66/126]\t\tLoss: 0.1879 \n",
            "| Epoch [ 54/ 60] Iter[ 71/126]\t\tLoss: 0.3239 \n",
            "| Epoch [ 54/ 60] Iter[ 76/126]\t\tLoss: 0.4788 \n",
            "| Epoch [ 54/ 60] Iter[ 81/126]\t\tLoss: 0.9816 \n",
            "| Epoch [ 54/ 60] Iter[ 86/126]\t\tLoss: 0.5882 \n",
            "| Epoch [ 54/ 60] Iter[ 91/126]\t\tLoss: 0.0962 \n",
            "| Epoch [ 54/ 60] Iter[ 96/126]\t\tLoss: 0.1066 \n",
            "| Epoch [ 54/ 60] Iter[101/126]\t\tLoss: 0.0817 \n",
            "| Epoch [ 54/ 60] Iter[106/126]\t\tLoss: 0.5407 \n",
            "| Epoch [ 54/ 60] Iter[111/126]\t\tLoss: 0.1698 \n",
            "| Epoch [ 54/ 60] Iter[116/126]\t\tLoss: 0.7421 \n",
            "| Epoch [ 54/ 60] Iter[121/126]\t\tLoss: 0.0891 \n",
            "\n",
            "| Validation Epoch #54\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #55, LR=0.0000\n",
            "| Epoch [ 55/ 60] Iter[  1/126]\t\tLoss: 0.2080 \n",
            "| Epoch [ 55/ 60] Iter[  6/126]\t\tLoss: 0.5845 \n",
            "| Epoch [ 55/ 60] Iter[ 11/126]\t\tLoss: 0.4118 \n",
            "| Epoch [ 55/ 60] Iter[ 16/126]\t\tLoss: 0.1053 \n",
            "| Epoch [ 55/ 60] Iter[ 21/126]\t\tLoss: 0.2042 \n",
            "| Epoch [ 55/ 60] Iter[ 26/126]\t\tLoss: 0.7551 \n",
            "| Epoch [ 55/ 60] Iter[ 31/126]\t\tLoss: 0.2375 \n",
            "| Epoch [ 55/ 60] Iter[ 36/126]\t\tLoss: 0.8503 \n",
            "| Epoch [ 55/ 60] Iter[ 41/126]\t\tLoss: 1.4650 \n",
            "| Epoch [ 55/ 60] Iter[ 46/126]\t\tLoss: 1.4617 \n",
            "| Epoch [ 55/ 60] Iter[ 51/126]\t\tLoss: 0.3443 \n",
            "| Epoch [ 55/ 60] Iter[ 56/126]\t\tLoss: 1.2475 \n",
            "| Epoch [ 55/ 60] Iter[ 61/126]\t\tLoss: 6.4052 \n",
            "| Epoch [ 55/ 60] Iter[ 66/126]\t\tLoss: 0.1081 \n",
            "| Epoch [ 55/ 60] Iter[ 71/126]\t\tLoss: 0.7028 \n",
            "| Epoch [ 55/ 60] Iter[ 76/126]\t\tLoss: 0.9075 \n",
            "| Epoch [ 55/ 60] Iter[ 81/126]\t\tLoss: 0.3864 \n",
            "| Epoch [ 55/ 60] Iter[ 86/126]\t\tLoss: 0.0920 \n",
            "| Epoch [ 55/ 60] Iter[ 91/126]\t\tLoss: 0.3112 \n",
            "| Epoch [ 55/ 60] Iter[ 96/126]\t\tLoss: 0.4260 \n",
            "| Epoch [ 55/ 60] Iter[101/126]\t\tLoss: 0.5348 \n",
            "| Epoch [ 55/ 60] Iter[106/126]\t\tLoss: 0.2952 \n",
            "| Epoch [ 55/ 60] Iter[111/126]\t\tLoss: 0.7039 \n",
            "| Epoch [ 55/ 60] Iter[116/126]\t\tLoss: 1.0262 \n",
            "| Epoch [ 55/ 60] Iter[121/126]\t\tLoss: 5.6271 \n",
            "\n",
            "| Validation Epoch #55\t\t\tLoss: 0.2261 \n",
            "\n",
            "=> Training Epoch #56, LR=0.0000\n",
            "| Epoch [ 56/ 60] Iter[  1/126]\t\tLoss: 1.4906 \n",
            "| Epoch [ 56/ 60] Iter[  6/126]\t\tLoss: 0.1092 \n",
            "| Epoch [ 56/ 60] Iter[ 11/126]\t\tLoss: 1.0031 \n",
            "| Epoch [ 56/ 60] Iter[ 16/126]\t\tLoss: 0.3412 \n",
            "| Epoch [ 56/ 60] Iter[ 21/126]\t\tLoss: 0.4845 \n",
            "| Epoch [ 56/ 60] Iter[ 26/126]\t\tLoss: 0.2117 \n",
            "| Epoch [ 56/ 60] Iter[ 31/126]\t\tLoss: 0.6566 \n",
            "| Epoch [ 56/ 60] Iter[ 36/126]\t\tLoss: 0.7922 \n",
            "| Epoch [ 56/ 60] Iter[ 41/126]\t\tLoss: 0.5497 \n",
            "| Epoch [ 56/ 60] Iter[ 46/126]\t\tLoss: 6.2340 \n",
            "| Epoch [ 56/ 60] Iter[ 51/126]\t\tLoss: 0.6215 \n",
            "| Epoch [ 56/ 60] Iter[ 56/126]\t\tLoss: 0.5963 \n",
            "| Epoch [ 56/ 60] Iter[ 61/126]\t\tLoss: 0.1139 \n",
            "| Epoch [ 56/ 60] Iter[ 66/126]\t\tLoss: 0.3544 \n",
            "| Epoch [ 56/ 60] Iter[ 71/126]\t\tLoss: 0.8738 \n",
            "| Epoch [ 56/ 60] Iter[ 76/126]\t\tLoss: 1.6868 \n",
            "| Epoch [ 56/ 60] Iter[ 81/126]\t\tLoss: 0.4131 \n",
            "| Epoch [ 56/ 60] Iter[ 86/126]\t\tLoss: 0.4442 \n",
            "| Epoch [ 56/ 60] Iter[ 91/126]\t\tLoss: 6.2835 \n",
            "| Epoch [ 56/ 60] Iter[ 96/126]\t\tLoss: 0.6177 \n",
            "| Epoch [ 56/ 60] Iter[101/126]\t\tLoss: 0.1390 \n",
            "| Epoch [ 56/ 60] Iter[106/126]\t\tLoss: 0.5633 \n",
            "| Epoch [ 56/ 60] Iter[111/126]\t\tLoss: 0.3739 \n",
            "| Epoch [ 56/ 60] Iter[116/126]\t\tLoss: 0.8486 \n",
            "| Epoch [ 56/ 60] Iter[121/126]\t\tLoss: 0.2585 \n",
            "\n",
            "| Validation Epoch #56\t\t\tLoss: 0.2261 \n",
            "\n",
            "=> Training Epoch #57, LR=0.0000\n",
            "| Epoch [ 57/ 60] Iter[  1/126]\t\tLoss: 0.3811 \n",
            "| Epoch [ 57/ 60] Iter[  6/126]\t\tLoss: 0.5403 \n",
            "| Epoch [ 57/ 60] Iter[ 11/126]\t\tLoss: 1.8348 \n",
            "| Epoch [ 57/ 60] Iter[ 16/126]\t\tLoss: 6.2398 \n",
            "| Epoch [ 57/ 60] Iter[ 21/126]\t\tLoss: 0.4372 \n",
            "| Epoch [ 57/ 60] Iter[ 26/126]\t\tLoss: 0.0405 \n",
            "| Epoch [ 57/ 60] Iter[ 31/126]\t\tLoss: 0.1015 \n",
            "| Epoch [ 57/ 60] Iter[ 36/126]\t\tLoss: 0.4520 \n",
            "| Epoch [ 57/ 60] Iter[ 41/126]\t\tLoss: 0.2381 \n",
            "| Epoch [ 57/ 60] Iter[ 46/126]\t\tLoss: 0.2800 \n",
            "| Epoch [ 57/ 60] Iter[ 51/126]\t\tLoss: 1.0853 \n",
            "| Epoch [ 57/ 60] Iter[ 56/126]\t\tLoss: 0.2771 \n",
            "| Epoch [ 57/ 60] Iter[ 61/126]\t\tLoss: 0.6183 \n",
            "| Epoch [ 57/ 60] Iter[ 66/126]\t\tLoss: 0.5531 \n",
            "| Epoch [ 57/ 60] Iter[ 71/126]\t\tLoss: 1.1460 \n",
            "| Epoch [ 57/ 60] Iter[ 76/126]\t\tLoss: 0.2145 \n",
            "| Epoch [ 57/ 60] Iter[ 81/126]\t\tLoss: 0.3472 \n",
            "| Epoch [ 57/ 60] Iter[ 86/126]\t\tLoss: 0.2352 \n",
            "| Epoch [ 57/ 60] Iter[ 91/126]\t\tLoss: 0.1117 \n",
            "| Epoch [ 57/ 60] Iter[ 96/126]\t\tLoss: 0.4303 \n",
            "| Epoch [ 57/ 60] Iter[101/126]\t\tLoss: 1.0895 \n",
            "| Epoch [ 57/ 60] Iter[106/126]\t\tLoss: 0.3951 \n",
            "| Epoch [ 57/ 60] Iter[111/126]\t\tLoss: 1.6105 \n",
            "| Epoch [ 57/ 60] Iter[116/126]\t\tLoss: 0.1672 \n",
            "| Epoch [ 57/ 60] Iter[121/126]\t\tLoss: 0.0609 \n",
            "\n",
            "| Validation Epoch #57\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #58, LR=0.0000\n",
            "| Epoch [ 58/ 60] Iter[  1/126]\t\tLoss: 0.2442 \n",
            "| Epoch [ 58/ 60] Iter[  6/126]\t\tLoss: 0.7157 \n",
            "| Epoch [ 58/ 60] Iter[ 11/126]\t\tLoss: 0.0644 \n",
            "| Epoch [ 58/ 60] Iter[ 16/126]\t\tLoss: 0.2205 \n",
            "| Epoch [ 58/ 60] Iter[ 21/126]\t\tLoss: 0.0969 \n",
            "| Epoch [ 58/ 60] Iter[ 26/126]\t\tLoss: 0.1160 \n",
            "| Epoch [ 58/ 60] Iter[ 31/126]\t\tLoss: 5.5883 \n",
            "| Epoch [ 58/ 60] Iter[ 36/126]\t\tLoss: 1.0165 \n",
            "| Epoch [ 58/ 60] Iter[ 41/126]\t\tLoss: 0.8855 \n",
            "| Epoch [ 58/ 60] Iter[ 46/126]\t\tLoss: 6.5149 \n",
            "| Epoch [ 58/ 60] Iter[ 51/126]\t\tLoss: 0.0478 \n",
            "| Epoch [ 58/ 60] Iter[ 56/126]\t\tLoss: 0.2000 \n",
            "| Epoch [ 58/ 60] Iter[ 61/126]\t\tLoss: 0.6783 \n",
            "| Epoch [ 58/ 60] Iter[ 66/126]\t\tLoss: 5.9505 \n",
            "| Epoch [ 58/ 60] Iter[ 71/126]\t\tLoss: 5.6357 \n",
            "| Epoch [ 58/ 60] Iter[ 76/126]\t\tLoss: 0.7446 \n",
            "| Epoch [ 58/ 60] Iter[ 81/126]\t\tLoss: 0.2435 \n",
            "| Epoch [ 58/ 60] Iter[ 86/126]\t\tLoss: 0.4040 \n",
            "| Epoch [ 58/ 60] Iter[ 91/126]\t\tLoss: 0.5487 \n",
            "| Epoch [ 58/ 60] Iter[ 96/126]\t\tLoss: 0.2239 \n",
            "| Epoch [ 58/ 60] Iter[101/126]\t\tLoss: 0.2870 \n",
            "| Epoch [ 58/ 60] Iter[106/126]\t\tLoss: 5.8195 \n",
            "| Epoch [ 58/ 60] Iter[111/126]\t\tLoss: 0.1951 \n",
            "| Epoch [ 58/ 60] Iter[116/126]\t\tLoss: 0.5142 \n",
            "| Epoch [ 58/ 60] Iter[121/126]\t\tLoss: 0.2895 \n",
            "\n",
            "| Validation Epoch #58\t\t\tLoss: 0.2261 \n",
            "\n",
            "=> Training Epoch #59, LR=0.0000\n",
            "| Epoch [ 59/ 60] Iter[  1/126]\t\tLoss: 5.9626 \n",
            "| Epoch [ 59/ 60] Iter[  6/126]\t\tLoss: 0.1844 \n",
            "| Epoch [ 59/ 60] Iter[ 11/126]\t\tLoss: 0.8374 \n",
            "| Epoch [ 59/ 60] Iter[ 16/126]\t\tLoss: 0.0814 \n",
            "| Epoch [ 59/ 60] Iter[ 21/126]\t\tLoss: 0.4850 \n",
            "| Epoch [ 59/ 60] Iter[ 26/126]\t\tLoss: 0.1163 \n",
            "| Epoch [ 59/ 60] Iter[ 31/126]\t\tLoss: 0.8067 \n",
            "| Epoch [ 59/ 60] Iter[ 36/126]\t\tLoss: 0.7001 \n",
            "| Epoch [ 59/ 60] Iter[ 41/126]\t\tLoss: 0.6609 \n",
            "| Epoch [ 59/ 60] Iter[ 46/126]\t\tLoss: 0.6768 \n",
            "| Epoch [ 59/ 60] Iter[ 51/126]\t\tLoss: 1.2102 \n",
            "| Epoch [ 59/ 60] Iter[ 56/126]\t\tLoss: 0.4178 \n",
            "| Epoch [ 59/ 60] Iter[ 61/126]\t\tLoss: 0.3669 \n",
            "| Epoch [ 59/ 60] Iter[ 66/126]\t\tLoss: 0.3022 \n",
            "| Epoch [ 59/ 60] Iter[ 71/126]\t\tLoss: 0.0158 \n",
            "| Epoch [ 59/ 60] Iter[ 76/126]\t\tLoss: 0.8484 \n",
            "| Epoch [ 59/ 60] Iter[ 81/126]\t\tLoss: 5.8450 \n",
            "| Epoch [ 59/ 60] Iter[ 86/126]\t\tLoss: 0.5283 \n",
            "| Epoch [ 59/ 60] Iter[ 91/126]\t\tLoss: 0.2522 \n",
            "| Epoch [ 59/ 60] Iter[ 96/126]\t\tLoss: 0.5129 \n",
            "| Epoch [ 59/ 60] Iter[101/126]\t\tLoss: 0.1924 \n",
            "| Epoch [ 59/ 60] Iter[106/126]\t\tLoss: 0.3344 \n",
            "| Epoch [ 59/ 60] Iter[111/126]\t\tLoss: 0.6449 \n",
            "| Epoch [ 59/ 60] Iter[116/126]\t\tLoss: 0.2313 \n",
            "| Epoch [ 59/ 60] Iter[121/126]\t\tLoss: 0.6108 \n",
            "\n",
            "| Validation Epoch #59\t\t\tLoss: 0.2260 \n",
            "\n",
            "=> Training Epoch #0, LR=0.0010\n",
            "| Epoch [  0/ 60] Iter[  1/ 84]\t\tLoss: 10.1721 \n",
            "| Epoch [  0/ 60] Iter[  6/ 84]\t\tLoss: 9.1012 \n",
            "| Epoch [  0/ 60] Iter[ 11/ 84]\t\tLoss: 3.8706 \n",
            "| Epoch [  0/ 60] Iter[ 16/ 84]\t\tLoss: 1.7801 \n",
            "| Epoch [  0/ 60] Iter[ 21/ 84]\t\tLoss: 0.4251 \n",
            "| Epoch [  0/ 60] Iter[ 26/ 84]\t\tLoss: 1.0571 \n",
            "| Epoch [  0/ 60] Iter[ 31/ 84]\t\tLoss: 0.2219 \n",
            "| Epoch [  0/ 60] Iter[ 36/ 84]\t\tLoss: 1.4516 \n",
            "| Epoch [  0/ 60] Iter[ 41/ 84]\t\tLoss: 0.6849 \n",
            "| Epoch [  0/ 60] Iter[ 46/ 84]\t\tLoss: 0.7528 \n",
            "| Epoch [  0/ 60] Iter[ 51/ 84]\t\tLoss: 0.8621 \n",
            "| Epoch [  0/ 60] Iter[ 56/ 84]\t\tLoss: 0.8076 \n",
            "| Epoch [  0/ 60] Iter[ 61/ 84]\t\tLoss: 0.8673 \n",
            "| Epoch [  0/ 60] Iter[ 66/ 84]\t\tLoss: 0.2286 \n",
            "| Epoch [  0/ 60] Iter[ 71/ 84]\t\tLoss: 0.0978 \n",
            "| Epoch [  0/ 60] Iter[ 76/ 84]\t\tLoss: 0.1808 \n",
            "| Epoch [  0/ 60] Iter[ 81/ 84]\t\tLoss: 0.7494 \n",
            "\n",
            "| Validation Epoch #0\t\t\tLoss: 0.1855 \n",
            "\n",
            "=> Training Epoch #1, LR=0.0010\n",
            "| Epoch [  1/ 60] Iter[  1/ 84]\t\tLoss: 6.5896 \n",
            "| Epoch [  1/ 60] Iter[  6/ 84]\t\tLoss: 0.6259 \n",
            "| Epoch [  1/ 60] Iter[ 11/ 84]\t\tLoss: 1.4908 \n",
            "| Epoch [  1/ 60] Iter[ 16/ 84]\t\tLoss: 0.4830 \n",
            "| Epoch [  1/ 60] Iter[ 21/ 84]\t\tLoss: 0.6245 \n",
            "| Epoch [  1/ 60] Iter[ 26/ 84]\t\tLoss: 0.8712 \n",
            "| Epoch [  1/ 60] Iter[ 31/ 84]\t\tLoss: 0.8789 \n",
            "| Epoch [  1/ 60] Iter[ 36/ 84]\t\tLoss: 0.1903 \n",
            "| Epoch [  1/ 60] Iter[ 41/ 84]\t\tLoss: 0.9155 \n",
            "| Epoch [  1/ 60] Iter[ 46/ 84]\t\tLoss: 0.4110 \n",
            "| Epoch [  1/ 60] Iter[ 51/ 84]\t\tLoss: 0.7815 \n",
            "| Epoch [  1/ 60] Iter[ 56/ 84]\t\tLoss: 0.1856 \n",
            "| Epoch [  1/ 60] Iter[ 61/ 84]\t\tLoss: 0.5206 \n",
            "| Epoch [  1/ 60] Iter[ 66/ 84]\t\tLoss: 0.1792 \n",
            "| Epoch [  1/ 60] Iter[ 71/ 84]\t\tLoss: 0.9452 \n",
            "| Epoch [  1/ 60] Iter[ 76/ 84]\t\tLoss: 0.4133 \n",
            "| Epoch [  1/ 60] Iter[ 81/ 84]\t\tLoss: 6.7732 \n",
            "\n",
            "| Validation Epoch #1\t\t\tLoss: 0.1410 \n",
            "\n",
            "=> Training Epoch #2, LR=0.0010\n",
            "| Epoch [  2/ 60] Iter[  1/ 84]\t\tLoss: 0.3115 \n",
            "| Epoch [  2/ 60] Iter[  6/ 84]\t\tLoss: 0.7164 \n",
            "| Epoch [  2/ 60] Iter[ 11/ 84]\t\tLoss: 0.1828 \n",
            "| Epoch [  2/ 60] Iter[ 16/ 84]\t\tLoss: 1.1694 \n",
            "| Epoch [  2/ 60] Iter[ 21/ 84]\t\tLoss: 0.3084 \n",
            "| Epoch [  2/ 60] Iter[ 26/ 84]\t\tLoss: 0.1715 \n",
            "| Epoch [  2/ 60] Iter[ 31/ 84]\t\tLoss: 0.6615 \n",
            "| Epoch [  2/ 60] Iter[ 36/ 84]\t\tLoss: 1.1294 \n",
            "| Epoch [  2/ 60] Iter[ 41/ 84]\t\tLoss: 0.2123 \n",
            "| Epoch [  2/ 60] Iter[ 46/ 84]\t\tLoss: 0.4982 \n",
            "| Epoch [  2/ 60] Iter[ 51/ 84]\t\tLoss: 0.5565 \n",
            "| Epoch [  2/ 60] Iter[ 56/ 84]\t\tLoss: 0.1159 \n",
            "| Epoch [  2/ 60] Iter[ 61/ 84]\t\tLoss: 0.4756 \n",
            "| Epoch [  2/ 60] Iter[ 66/ 84]\t\tLoss: 6.4090 \n",
            "| Epoch [  2/ 60] Iter[ 71/ 84]\t\tLoss: 0.5710 \n",
            "| Epoch [  2/ 60] Iter[ 76/ 84]\t\tLoss: 0.7398 \n",
            "| Epoch [  2/ 60] Iter[ 81/ 84]\t\tLoss: 0.2431 \n",
            "\n",
            "| Validation Epoch #2\t\t\tLoss: 0.2176 \n",
            "\n",
            "=> Training Epoch #3, LR=0.0010\n",
            "| Epoch [  3/ 60] Iter[  1/ 84]\t\tLoss: 1.7901 \n",
            "| Epoch [  3/ 60] Iter[  6/ 84]\t\tLoss: 0.3030 \n",
            "| Epoch [  3/ 60] Iter[ 11/ 84]\t\tLoss: 1.0635 \n",
            "| Epoch [  3/ 60] Iter[ 16/ 84]\t\tLoss: 0.5168 \n",
            "| Epoch [  3/ 60] Iter[ 21/ 84]\t\tLoss: 0.6160 \n",
            "| Epoch [  3/ 60] Iter[ 26/ 84]\t\tLoss: 1.7025 \n",
            "| Epoch [  3/ 60] Iter[ 31/ 84]\t\tLoss: 0.4503 \n",
            "| Epoch [  3/ 60] Iter[ 36/ 84]\t\tLoss: 0.3482 \n",
            "| Epoch [  3/ 60] Iter[ 41/ 84]\t\tLoss: 0.0248 \n",
            "| Epoch [  3/ 60] Iter[ 46/ 84]\t\tLoss: 0.2961 \n",
            "| Epoch [  3/ 60] Iter[ 51/ 84]\t\tLoss: 1.3897 \n",
            "| Epoch [  3/ 60] Iter[ 56/ 84]\t\tLoss: 5.8205 \n",
            "| Epoch [  3/ 60] Iter[ 61/ 84]\t\tLoss: 0.6478 \n",
            "| Epoch [  3/ 60] Iter[ 66/ 84]\t\tLoss: 0.4396 \n",
            "| Epoch [  3/ 60] Iter[ 71/ 84]\t\tLoss: 0.2513 \n",
            "| Epoch [  3/ 60] Iter[ 76/ 84]\t\tLoss: 5.9525 \n",
            "| Epoch [  3/ 60] Iter[ 81/ 84]\t\tLoss: 0.6961 \n",
            "\n",
            "| Validation Epoch #3\t\t\tLoss: 0.2195 \n",
            "\n",
            "=> Training Epoch #4, LR=0.0010\n",
            "| Epoch [  4/ 60] Iter[  1/ 84]\t\tLoss: 1.5564 \n",
            "| Epoch [  4/ 60] Iter[  6/ 84]\t\tLoss: 0.6856 \n",
            "| Epoch [  4/ 60] Iter[ 11/ 84]\t\tLoss: 0.5731 \n",
            "| Epoch [  4/ 60] Iter[ 16/ 84]\t\tLoss: 0.5612 \n",
            "| Epoch [  4/ 60] Iter[ 21/ 84]\t\tLoss: 0.2401 \n",
            "| Epoch [  4/ 60] Iter[ 26/ 84]\t\tLoss: 0.1505 \n",
            "| Epoch [  4/ 60] Iter[ 31/ 84]\t\tLoss: 0.5864 \n",
            "| Epoch [  4/ 60] Iter[ 36/ 84]\t\tLoss: 1.0547 \n",
            "| Epoch [  4/ 60] Iter[ 41/ 84]\t\tLoss: 0.2325 \n",
            "| Epoch [  4/ 60] Iter[ 46/ 84]\t\tLoss: 0.5265 \n",
            "| Epoch [  4/ 60] Iter[ 51/ 84]\t\tLoss: 0.6983 \n",
            "| Epoch [  4/ 60] Iter[ 56/ 84]\t\tLoss: 0.4977 \n",
            "| Epoch [  4/ 60] Iter[ 61/ 84]\t\tLoss: 0.5759 \n",
            "| Epoch [  4/ 60] Iter[ 66/ 84]\t\tLoss: 0.5543 \n",
            "| Epoch [  4/ 60] Iter[ 71/ 84]\t\tLoss: 5.7797 \n",
            "| Epoch [  4/ 60] Iter[ 76/ 84]\t\tLoss: 7.0256 \n",
            "| Epoch [  4/ 60] Iter[ 81/ 84]\t\tLoss: 0.9003 \n",
            "\n",
            "| Validation Epoch #4\t\t\tLoss: 0.2466 \n",
            "\n",
            "=> Training Epoch #5, LR=0.0010\n",
            "| Epoch [  5/ 60] Iter[  1/ 84]\t\tLoss: 0.3308 \n",
            "| Epoch [  5/ 60] Iter[  6/ 84]\t\tLoss: 1.2226 \n",
            "| Epoch [  5/ 60] Iter[ 11/ 84]\t\tLoss: 0.6491 \n",
            "| Epoch [  5/ 60] Iter[ 16/ 84]\t\tLoss: 0.1691 \n",
            "| Epoch [  5/ 60] Iter[ 21/ 84]\t\tLoss: 0.4807 \n",
            "| Epoch [  5/ 60] Iter[ 26/ 84]\t\tLoss: 1.1742 \n",
            "| Epoch [  5/ 60] Iter[ 31/ 84]\t\tLoss: 0.6772 \n",
            "| Epoch [  5/ 60] Iter[ 36/ 84]\t\tLoss: 0.0895 \n",
            "| Epoch [  5/ 60] Iter[ 41/ 84]\t\tLoss: 1.2435 \n",
            "| Epoch [  5/ 60] Iter[ 46/ 84]\t\tLoss: 0.9096 \n",
            "| Epoch [  5/ 60] Iter[ 51/ 84]\t\tLoss: 1.1092 \n",
            "| Epoch [  5/ 60] Iter[ 56/ 84]\t\tLoss: 0.2529 \n",
            "| Epoch [  5/ 60] Iter[ 61/ 84]\t\tLoss: 0.5250 \n",
            "| Epoch [  5/ 60] Iter[ 66/ 84]\t\tLoss: 6.5800 \n",
            "| Epoch [  5/ 60] Iter[ 71/ 84]\t\tLoss: 0.9289 \n",
            "| Epoch [  5/ 60] Iter[ 76/ 84]\t\tLoss: 0.8821 \n",
            "| Epoch [  5/ 60] Iter[ 81/ 84]\t\tLoss: 0.4887 \n",
            "\n",
            "| Validation Epoch #5\t\t\tLoss: 0.2673 \n",
            "\n",
            "=> Training Epoch #6, LR=0.0010\n",
            "| Epoch [  6/ 60] Iter[  1/ 84]\t\tLoss: 0.4873 \n",
            "| Epoch [  6/ 60] Iter[  6/ 84]\t\tLoss: 0.9718 \n",
            "| Epoch [  6/ 60] Iter[ 11/ 84]\t\tLoss: 0.6779 \n",
            "| Epoch [  6/ 60] Iter[ 16/ 84]\t\tLoss: 0.1893 \n",
            "| Epoch [  6/ 60] Iter[ 21/ 84]\t\tLoss: 0.8851 \n",
            "| Epoch [  6/ 60] Iter[ 26/ 84]\t\tLoss: 0.1792 \n",
            "| Epoch [  6/ 60] Iter[ 31/ 84]\t\tLoss: 0.3221 \n",
            "| Epoch [  6/ 60] Iter[ 36/ 84]\t\tLoss: 0.5184 \n",
            "| Epoch [  6/ 60] Iter[ 41/ 84]\t\tLoss: 0.1140 \n",
            "| Epoch [  6/ 60] Iter[ 46/ 84]\t\tLoss: 0.5192 \n",
            "| Epoch [  6/ 60] Iter[ 51/ 84]\t\tLoss: 0.2579 \n",
            "| Epoch [  6/ 60] Iter[ 56/ 84]\t\tLoss: 0.7251 \n",
            "| Epoch [  6/ 60] Iter[ 61/ 84]\t\tLoss: 0.4567 \n",
            "| Epoch [  6/ 60] Iter[ 66/ 84]\t\tLoss: 0.1585 \n",
            "| Epoch [  6/ 60] Iter[ 71/ 84]\t\tLoss: 1.0272 \n",
            "| Epoch [  6/ 60] Iter[ 76/ 84]\t\tLoss: 0.4282 \n",
            "| Epoch [  6/ 60] Iter[ 81/ 84]\t\tLoss: 0.4920 \n",
            "\n",
            "| Validation Epoch #6\t\t\tLoss: 0.3072 \n",
            "\n",
            "=> Training Epoch #7, LR=0.0010\n",
            "| Epoch [  7/ 60] Iter[  1/ 84]\t\tLoss: 0.1423 \n",
            "| Epoch [  7/ 60] Iter[  6/ 84]\t\tLoss: 0.3055 \n",
            "| Epoch [  7/ 60] Iter[ 11/ 84]\t\tLoss: 0.2765 \n",
            "| Epoch [  7/ 60] Iter[ 16/ 84]\t\tLoss: 0.5295 \n",
            "| Epoch [  7/ 60] Iter[ 21/ 84]\t\tLoss: 0.4449 \n",
            "| Epoch [  7/ 60] Iter[ 26/ 84]\t\tLoss: 0.4103 \n",
            "| Epoch [  7/ 60] Iter[ 31/ 84]\t\tLoss: 1.2100 \n",
            "| Epoch [  7/ 60] Iter[ 36/ 84]\t\tLoss: 0.2385 \n",
            "| Epoch [  7/ 60] Iter[ 41/ 84]\t\tLoss: 0.4398 \n",
            "| Epoch [  7/ 60] Iter[ 46/ 84]\t\tLoss: 0.2745 \n",
            "| Epoch [  7/ 60] Iter[ 51/ 84]\t\tLoss: 0.9527 \n",
            "| Epoch [  7/ 60] Iter[ 56/ 84]\t\tLoss: 0.2279 \n",
            "| Epoch [  7/ 60] Iter[ 61/ 84]\t\tLoss: 0.5378 \n",
            "| Epoch [  7/ 60] Iter[ 66/ 84]\t\tLoss: 0.2803 \n",
            "| Epoch [  7/ 60] Iter[ 71/ 84]\t\tLoss: 0.6927 \n",
            "| Epoch [  7/ 60] Iter[ 76/ 84]\t\tLoss: 0.5594 \n",
            "| Epoch [  7/ 60] Iter[ 81/ 84]\t\tLoss: 1.0482 \n",
            "\n",
            "| Validation Epoch #7\t\t\tLoss: 0.2004 \n",
            "\n",
            "=> Training Epoch #8, LR=0.0010\n",
            "| Epoch [  8/ 60] Iter[  1/ 84]\t\tLoss: 0.2847 \n",
            "| Epoch [  8/ 60] Iter[  6/ 84]\t\tLoss: 0.9170 \n",
            "| Epoch [  8/ 60] Iter[ 11/ 84]\t\tLoss: 6.3919 \n",
            "| Epoch [  8/ 60] Iter[ 16/ 84]\t\tLoss: 0.5166 \n",
            "| Epoch [  8/ 60] Iter[ 21/ 84]\t\tLoss: 0.2359 \n",
            "| Epoch [  8/ 60] Iter[ 26/ 84]\t\tLoss: 1.1256 \n",
            "| Epoch [  8/ 60] Iter[ 31/ 84]\t\tLoss: 1.0549 \n",
            "| Epoch [  8/ 60] Iter[ 36/ 84]\t\tLoss: 0.6840 \n",
            "| Epoch [  8/ 60] Iter[ 41/ 84]\t\tLoss: 0.6502 \n",
            "| Epoch [  8/ 60] Iter[ 46/ 84]\t\tLoss: 0.9221 \n",
            "| Epoch [  8/ 60] Iter[ 51/ 84]\t\tLoss: 5.6705 \n",
            "| Epoch [  8/ 60] Iter[ 56/ 84]\t\tLoss: 0.1771 \n",
            "| Epoch [  8/ 60] Iter[ 61/ 84]\t\tLoss: 0.4205 \n",
            "| Epoch [  8/ 60] Iter[ 66/ 84]\t\tLoss: 0.2250 \n",
            "| Epoch [  8/ 60] Iter[ 71/ 84]\t\tLoss: 0.7901 \n",
            "| Epoch [  8/ 60] Iter[ 76/ 84]\t\tLoss: 1.4243 \n",
            "| Epoch [  8/ 60] Iter[ 81/ 84]\t\tLoss: 0.4634 \n",
            "\n",
            "| Validation Epoch #8\t\t\tLoss: 0.2365 \n",
            "\n",
            "=> Training Epoch #9, LR=0.0010\n",
            "| Epoch [  9/ 60] Iter[  1/ 84]\t\tLoss: 0.3689 \n",
            "| Epoch [  9/ 60] Iter[  6/ 84]\t\tLoss: 0.1701 \n",
            "| Epoch [  9/ 60] Iter[ 11/ 84]\t\tLoss: 0.5636 \n",
            "| Epoch [  9/ 60] Iter[ 16/ 84]\t\tLoss: 0.9003 \n",
            "| Epoch [  9/ 60] Iter[ 21/ 84]\t\tLoss: 1.5822 \n",
            "| Epoch [  9/ 60] Iter[ 26/ 84]\t\tLoss: 1.3439 \n",
            "| Epoch [  9/ 60] Iter[ 31/ 84]\t\tLoss: 0.8625 \n",
            "| Epoch [  9/ 60] Iter[ 36/ 84]\t\tLoss: 0.3221 \n",
            "| Epoch [  9/ 60] Iter[ 41/ 84]\t\tLoss: 0.0543 \n",
            "| Epoch [  9/ 60] Iter[ 46/ 84]\t\tLoss: 0.2516 \n",
            "| Epoch [  9/ 60] Iter[ 51/ 84]\t\tLoss: 1.2438 \n",
            "| Epoch [  9/ 60] Iter[ 56/ 84]\t\tLoss: 0.8245 \n",
            "| Epoch [  9/ 60] Iter[ 61/ 84]\t\tLoss: 0.6885 \n",
            "| Epoch [  9/ 60] Iter[ 66/ 84]\t\tLoss: 0.2866 \n",
            "| Epoch [  9/ 60] Iter[ 71/ 84]\t\tLoss: 0.4057 \n",
            "| Epoch [  9/ 60] Iter[ 76/ 84]\t\tLoss: 6.2556 \n",
            "| Epoch [  9/ 60] Iter[ 81/ 84]\t\tLoss: 0.2965 \n",
            "\n",
            "| Validation Epoch #9\t\t\tLoss: 0.2358 \n",
            "\n",
            "=> Training Epoch #10, LR=0.0010\n",
            "| Epoch [ 10/ 60] Iter[  1/ 84]\t\tLoss: 0.3615 \n",
            "| Epoch [ 10/ 60] Iter[  6/ 84]\t\tLoss: 0.1967 \n",
            "| Epoch [ 10/ 60] Iter[ 11/ 84]\t\tLoss: 0.4535 \n",
            "| Epoch [ 10/ 60] Iter[ 16/ 84]\t\tLoss: 0.5226 \n",
            "| Epoch [ 10/ 60] Iter[ 21/ 84]\t\tLoss: 0.3802 \n",
            "| Epoch [ 10/ 60] Iter[ 26/ 84]\t\tLoss: 0.2070 \n",
            "| Epoch [ 10/ 60] Iter[ 31/ 84]\t\tLoss: 6.3019 \n",
            "| Epoch [ 10/ 60] Iter[ 36/ 84]\t\tLoss: 0.6869 \n",
            "| Epoch [ 10/ 60] Iter[ 41/ 84]\t\tLoss: 0.9722 \n",
            "| Epoch [ 10/ 60] Iter[ 46/ 84]\t\tLoss: 0.1861 \n",
            "| Epoch [ 10/ 60] Iter[ 51/ 84]\t\tLoss: 0.5996 \n",
            "| Epoch [ 10/ 60] Iter[ 56/ 84]\t\tLoss: 0.4562 \n",
            "| Epoch [ 10/ 60] Iter[ 61/ 84]\t\tLoss: 0.3207 \n",
            "| Epoch [ 10/ 60] Iter[ 66/ 84]\t\tLoss: 0.0804 \n",
            "| Epoch [ 10/ 60] Iter[ 71/ 84]\t\tLoss: 0.3210 \n",
            "| Epoch [ 10/ 60] Iter[ 76/ 84]\t\tLoss: 1.0421 \n",
            "| Epoch [ 10/ 60] Iter[ 81/ 84]\t\tLoss: 0.1906 \n",
            "\n",
            "| Validation Epoch #10\t\t\tLoss: 0.2256 \n",
            "\n",
            "=> Training Epoch #11, LR=0.0001\n",
            "| Epoch [ 11/ 60] Iter[  1/ 84]\t\tLoss: 3.8066 \n",
            "| Epoch [ 11/ 60] Iter[  6/ 84]\t\tLoss: 0.8733 \n",
            "| Epoch [ 11/ 60] Iter[ 11/ 84]\t\tLoss: 6.2428 \n",
            "| Epoch [ 11/ 60] Iter[ 16/ 84]\t\tLoss: 0.1533 \n",
            "| Epoch [ 11/ 60] Iter[ 21/ 84]\t\tLoss: 0.6715 \n",
            "| Epoch [ 11/ 60] Iter[ 26/ 84]\t\tLoss: 0.1933 \n",
            "| Epoch [ 11/ 60] Iter[ 31/ 84]\t\tLoss: 0.5874 \n",
            "| Epoch [ 11/ 60] Iter[ 36/ 84]\t\tLoss: 0.7677 \n",
            "| Epoch [ 11/ 60] Iter[ 41/ 84]\t\tLoss: 0.7496 \n",
            "| Epoch [ 11/ 60] Iter[ 46/ 84]\t\tLoss: 0.4705 \n",
            "| Epoch [ 11/ 60] Iter[ 51/ 84]\t\tLoss: 0.7821 \n",
            "| Epoch [ 11/ 60] Iter[ 56/ 84]\t\tLoss: 0.5976 \n",
            "| Epoch [ 11/ 60] Iter[ 61/ 84]\t\tLoss: 0.6264 \n",
            "| Epoch [ 11/ 60] Iter[ 66/ 84]\t\tLoss: 0.9099 \n",
            "| Epoch [ 11/ 60] Iter[ 71/ 84]\t\tLoss: 0.4223 \n",
            "| Epoch [ 11/ 60] Iter[ 76/ 84]\t\tLoss: 0.0623 \n",
            "| Epoch [ 11/ 60] Iter[ 81/ 84]\t\tLoss: 2.6620 \n",
            "\n",
            "| Validation Epoch #11\t\t\tLoss: 0.2255 \n",
            "\n",
            "=> Training Epoch #12, LR=0.0001\n",
            "| Epoch [ 12/ 60] Iter[  1/ 84]\t\tLoss: 0.6911 \n",
            "| Epoch [ 12/ 60] Iter[  6/ 84]\t\tLoss: 0.3738 \n",
            "| Epoch [ 12/ 60] Iter[ 11/ 84]\t\tLoss: 0.3581 \n",
            "| Epoch [ 12/ 60] Iter[ 16/ 84]\t\tLoss: 0.2734 \n",
            "| Epoch [ 12/ 60] Iter[ 21/ 84]\t\tLoss: 0.4582 \n",
            "| Epoch [ 12/ 60] Iter[ 26/ 84]\t\tLoss: 3.5428 \n",
            "| Epoch [ 12/ 60] Iter[ 31/ 84]\t\tLoss: 0.1323 \n",
            "| Epoch [ 12/ 60] Iter[ 36/ 84]\t\tLoss: 0.1049 \n",
            "| Epoch [ 12/ 60] Iter[ 41/ 84]\t\tLoss: 0.7561 \n",
            "| Epoch [ 12/ 60] Iter[ 46/ 84]\t\tLoss: 0.9888 \n",
            "| Epoch [ 12/ 60] Iter[ 51/ 84]\t\tLoss: 0.4794 \n",
            "| Epoch [ 12/ 60] Iter[ 56/ 84]\t\tLoss: 0.1663 \n",
            "| Epoch [ 12/ 60] Iter[ 61/ 84]\t\tLoss: 0.2551 \n",
            "| Epoch [ 12/ 60] Iter[ 66/ 84]\t\tLoss: 0.3670 \n",
            "| Epoch [ 12/ 60] Iter[ 71/ 84]\t\tLoss: 0.6930 \n",
            "| Epoch [ 12/ 60] Iter[ 76/ 84]\t\tLoss: 0.3019 \n",
            "| Epoch [ 12/ 60] Iter[ 81/ 84]\t\tLoss: 0.7691 \n",
            "\n",
            "| Validation Epoch #12\t\t\tLoss: 0.2270 \n",
            "\n",
            "=> Training Epoch #13, LR=0.0001\n",
            "| Epoch [ 13/ 60] Iter[  1/ 84]\t\tLoss: 0.5664 \n",
            "| Epoch [ 13/ 60] Iter[  6/ 84]\t\tLoss: 0.3662 \n",
            "| Epoch [ 13/ 60] Iter[ 11/ 84]\t\tLoss: 0.5872 \n",
            "| Epoch [ 13/ 60] Iter[ 16/ 84]\t\tLoss: 0.7631 \n",
            "| Epoch [ 13/ 60] Iter[ 21/ 84]\t\tLoss: 0.6326 \n",
            "| Epoch [ 13/ 60] Iter[ 26/ 84]\t\tLoss: 6.1860 \n",
            "| Epoch [ 13/ 60] Iter[ 31/ 84]\t\tLoss: 0.0539 \n",
            "| Epoch [ 13/ 60] Iter[ 36/ 84]\t\tLoss: 0.0713 \n",
            "| Epoch [ 13/ 60] Iter[ 41/ 84]\t\tLoss: 0.7954 \n",
            "| Epoch [ 13/ 60] Iter[ 46/ 84]\t\tLoss: 0.3329 \n",
            "| Epoch [ 13/ 60] Iter[ 51/ 84]\t\tLoss: 1.3500 \n",
            "| Epoch [ 13/ 60] Iter[ 56/ 84]\t\tLoss: 0.4648 \n",
            "| Epoch [ 13/ 60] Iter[ 61/ 84]\t\tLoss: 0.2916 \n",
            "| Epoch [ 13/ 60] Iter[ 66/ 84]\t\tLoss: 0.2065 \n",
            "| Epoch [ 13/ 60] Iter[ 71/ 84]\t\tLoss: 0.2481 \n",
            "| Epoch [ 13/ 60] Iter[ 76/ 84]\t\tLoss: 0.7530 \n",
            "| Epoch [ 13/ 60] Iter[ 81/ 84]\t\tLoss: 0.5994 \n",
            "\n",
            "| Validation Epoch #13\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #14, LR=0.0001\n",
            "| Epoch [ 14/ 60] Iter[  1/ 84]\t\tLoss: 0.5033 \n",
            "| Epoch [ 14/ 60] Iter[  6/ 84]\t\tLoss: 1.0704 \n",
            "| Epoch [ 14/ 60] Iter[ 11/ 84]\t\tLoss: 0.3155 \n",
            "| Epoch [ 14/ 60] Iter[ 16/ 84]\t\tLoss: 0.4452 \n",
            "| Epoch [ 14/ 60] Iter[ 21/ 84]\t\tLoss: 1.2032 \n",
            "| Epoch [ 14/ 60] Iter[ 26/ 84]\t\tLoss: 0.3116 \n",
            "| Epoch [ 14/ 60] Iter[ 31/ 84]\t\tLoss: 0.5408 \n",
            "| Epoch [ 14/ 60] Iter[ 36/ 84]\t\tLoss: 0.5878 \n",
            "| Epoch [ 14/ 60] Iter[ 41/ 84]\t\tLoss: 0.0929 \n",
            "| Epoch [ 14/ 60] Iter[ 46/ 84]\t\tLoss: 0.2356 \n",
            "| Epoch [ 14/ 60] Iter[ 51/ 84]\t\tLoss: 0.1015 \n",
            "| Epoch [ 14/ 60] Iter[ 56/ 84]\t\tLoss: 6.1449 \n",
            "| Epoch [ 14/ 60] Iter[ 61/ 84]\t\tLoss: 0.1100 \n",
            "| Epoch [ 14/ 60] Iter[ 66/ 84]\t\tLoss: 5.9195 \n",
            "| Epoch [ 14/ 60] Iter[ 71/ 84]\t\tLoss: 0.0589 \n",
            "| Epoch [ 14/ 60] Iter[ 76/ 84]\t\tLoss: 0.1369 \n",
            "| Epoch [ 14/ 60] Iter[ 81/ 84]\t\tLoss: 1.1678 \n",
            "\n",
            "| Validation Epoch #14\t\t\tLoss: 0.2245 \n",
            "\n",
            "=> Training Epoch #15, LR=0.0001\n",
            "| Epoch [ 15/ 60] Iter[  1/ 84]\t\tLoss: 0.1678 \n",
            "| Epoch [ 15/ 60] Iter[  6/ 84]\t\tLoss: 0.2430 \n",
            "| Epoch [ 15/ 60] Iter[ 11/ 84]\t\tLoss: 0.3736 \n",
            "| Epoch [ 15/ 60] Iter[ 16/ 84]\t\tLoss: 0.4300 \n",
            "| Epoch [ 15/ 60] Iter[ 21/ 84]\t\tLoss: 0.6604 \n",
            "| Epoch [ 15/ 60] Iter[ 26/ 84]\t\tLoss: 0.0482 \n",
            "| Epoch [ 15/ 60] Iter[ 31/ 84]\t\tLoss: 0.8005 \n",
            "| Epoch [ 15/ 60] Iter[ 36/ 84]\t\tLoss: 0.6892 \n",
            "| Epoch [ 15/ 60] Iter[ 41/ 84]\t\tLoss: 1.4570 \n",
            "| Epoch [ 15/ 60] Iter[ 46/ 84]\t\tLoss: 0.7222 \n",
            "| Epoch [ 15/ 60] Iter[ 51/ 84]\t\tLoss: 2.5872 \n",
            "| Epoch [ 15/ 60] Iter[ 56/ 84]\t\tLoss: 0.7251 \n",
            "| Epoch [ 15/ 60] Iter[ 61/ 84]\t\tLoss: 0.1089 \n",
            "| Epoch [ 15/ 60] Iter[ 66/ 84]\t\tLoss: 0.5557 \n",
            "| Epoch [ 15/ 60] Iter[ 71/ 84]\t\tLoss: 0.7019 \n",
            "| Epoch [ 15/ 60] Iter[ 76/ 84]\t\tLoss: 0.4100 \n",
            "| Epoch [ 15/ 60] Iter[ 81/ 84]\t\tLoss: 0.2907 \n",
            "\n",
            "| Validation Epoch #15\t\t\tLoss: 0.2276 \n",
            "\n",
            "=> Training Epoch #16, LR=0.0001\n",
            "| Epoch [ 16/ 60] Iter[  1/ 84]\t\tLoss: 6.1071 \n",
            "| Epoch [ 16/ 60] Iter[  6/ 84]\t\tLoss: 0.3805 \n",
            "| Epoch [ 16/ 60] Iter[ 11/ 84]\t\tLoss: 0.8610 \n",
            "| Epoch [ 16/ 60] Iter[ 16/ 84]\t\tLoss: 0.7352 \n",
            "| Epoch [ 16/ 60] Iter[ 21/ 84]\t\tLoss: 0.6733 \n",
            "| Epoch [ 16/ 60] Iter[ 26/ 84]\t\tLoss: 2.5574 \n",
            "| Epoch [ 16/ 60] Iter[ 31/ 84]\t\tLoss: 0.8291 \n",
            "| Epoch [ 16/ 60] Iter[ 36/ 84]\t\tLoss: 0.4852 \n",
            "| Epoch [ 16/ 60] Iter[ 41/ 84]\t\tLoss: 0.4175 \n",
            "| Epoch [ 16/ 60] Iter[ 46/ 84]\t\tLoss: 0.7257 \n",
            "| Epoch [ 16/ 60] Iter[ 51/ 84]\t\tLoss: 6.1979 \n",
            "| Epoch [ 16/ 60] Iter[ 56/ 84]\t\tLoss: 0.4911 \n",
            "| Epoch [ 16/ 60] Iter[ 61/ 84]\t\tLoss: 0.3689 \n",
            "| Epoch [ 16/ 60] Iter[ 66/ 84]\t\tLoss: 0.4863 \n",
            "| Epoch [ 16/ 60] Iter[ 71/ 84]\t\tLoss: 0.6038 \n",
            "| Epoch [ 16/ 60] Iter[ 76/ 84]\t\tLoss: 0.5089 \n",
            "| Epoch [ 16/ 60] Iter[ 81/ 84]\t\tLoss: 0.4383 \n",
            "\n",
            "| Validation Epoch #16\t\t\tLoss: 0.2277 \n",
            "\n",
            "=> Training Epoch #17, LR=0.0001\n",
            "| Epoch [ 17/ 60] Iter[  1/ 84]\t\tLoss: 0.6944 \n",
            "| Epoch [ 17/ 60] Iter[  6/ 84]\t\tLoss: 0.6981 \n",
            "| Epoch [ 17/ 60] Iter[ 11/ 84]\t\tLoss: 0.4096 \n",
            "| Epoch [ 17/ 60] Iter[ 16/ 84]\t\tLoss: 0.5323 \n",
            "| Epoch [ 17/ 60] Iter[ 21/ 84]\t\tLoss: 0.7067 \n",
            "| Epoch [ 17/ 60] Iter[ 26/ 84]\t\tLoss: 0.3647 \n",
            "| Epoch [ 17/ 60] Iter[ 31/ 84]\t\tLoss: 1.1674 \n",
            "| Epoch [ 17/ 60] Iter[ 36/ 84]\t\tLoss: 0.6431 \n",
            "| Epoch [ 17/ 60] Iter[ 41/ 84]\t\tLoss: 0.7755 \n",
            "| Epoch [ 17/ 60] Iter[ 46/ 84]\t\tLoss: 0.4687 \n",
            "| Epoch [ 17/ 60] Iter[ 51/ 84]\t\tLoss: 0.3791 \n",
            "| Epoch [ 17/ 60] Iter[ 56/ 84]\t\tLoss: 0.2641 \n",
            "| Epoch [ 17/ 60] Iter[ 61/ 84]\t\tLoss: 0.1355 \n",
            "| Epoch [ 17/ 60] Iter[ 66/ 84]\t\tLoss: 0.3911 \n",
            "| Epoch [ 17/ 60] Iter[ 71/ 84]\t\tLoss: 0.4956 \n",
            "| Epoch [ 17/ 60] Iter[ 76/ 84]\t\tLoss: 0.6251 \n",
            "| Epoch [ 17/ 60] Iter[ 81/ 84]\t\tLoss: 0.8754 \n",
            "\n",
            "| Validation Epoch #17\t\t\tLoss: 0.2278 \n",
            "\n",
            "=> Training Epoch #18, LR=0.0001\n",
            "| Epoch [ 18/ 60] Iter[  1/ 84]\t\tLoss: 0.5024 \n",
            "| Epoch [ 18/ 60] Iter[  6/ 84]\t\tLoss: 0.5215 \n",
            "| Epoch [ 18/ 60] Iter[ 11/ 84]\t\tLoss: 0.7266 \n",
            "| Epoch [ 18/ 60] Iter[ 16/ 84]\t\tLoss: 0.2112 \n",
            "| Epoch [ 18/ 60] Iter[ 21/ 84]\t\tLoss: 1.4695 \n",
            "| Epoch [ 18/ 60] Iter[ 26/ 84]\t\tLoss: 0.1261 \n",
            "| Epoch [ 18/ 60] Iter[ 31/ 84]\t\tLoss: 0.1758 \n",
            "| Epoch [ 18/ 60] Iter[ 36/ 84]\t\tLoss: 0.2246 \n",
            "| Epoch [ 18/ 60] Iter[ 41/ 84]\t\tLoss: 0.1866 \n",
            "| Epoch [ 18/ 60] Iter[ 46/ 84]\t\tLoss: 0.4244 \n",
            "| Epoch [ 18/ 60] Iter[ 51/ 84]\t\tLoss: 0.2837 \n",
            "| Epoch [ 18/ 60] Iter[ 56/ 84]\t\tLoss: 0.5074 \n",
            "| Epoch [ 18/ 60] Iter[ 61/ 84]\t\tLoss: 0.1190 \n",
            "| Epoch [ 18/ 60] Iter[ 66/ 84]\t\tLoss: 0.2838 \n",
            "| Epoch [ 18/ 60] Iter[ 71/ 84]\t\tLoss: 0.2383 \n",
            "| Epoch [ 18/ 60] Iter[ 76/ 84]\t\tLoss: 1.2493 \n",
            "| Epoch [ 18/ 60] Iter[ 81/ 84]\t\tLoss: 0.1882 \n",
            "\n",
            "| Validation Epoch #18\t\t\tLoss: 0.2271 \n",
            "\n",
            "=> Training Epoch #19, LR=0.0001\n",
            "| Epoch [ 19/ 60] Iter[  1/ 84]\t\tLoss: 6.1002 \n",
            "| Epoch [ 19/ 60] Iter[  6/ 84]\t\tLoss: 0.1500 \n",
            "| Epoch [ 19/ 60] Iter[ 11/ 84]\t\tLoss: 0.7054 \n",
            "| Epoch [ 19/ 60] Iter[ 16/ 84]\t\tLoss: 1.0440 \n",
            "| Epoch [ 19/ 60] Iter[ 21/ 84]\t\tLoss: 5.7151 \n",
            "| Epoch [ 19/ 60] Iter[ 26/ 84]\t\tLoss: 0.0589 \n",
            "| Epoch [ 19/ 60] Iter[ 31/ 84]\t\tLoss: 0.3725 \n",
            "| Epoch [ 19/ 60] Iter[ 36/ 84]\t\tLoss: 0.1833 \n",
            "| Epoch [ 19/ 60] Iter[ 41/ 84]\t\tLoss: 0.4574 \n",
            "| Epoch [ 19/ 60] Iter[ 46/ 84]\t\tLoss: 0.2613 \n",
            "| Epoch [ 19/ 60] Iter[ 51/ 84]\t\tLoss: 0.3457 \n",
            "| Epoch [ 19/ 60] Iter[ 56/ 84]\t\tLoss: 0.8866 \n",
            "| Epoch [ 19/ 60] Iter[ 61/ 84]\t\tLoss: 1.3469 \n",
            "| Epoch [ 19/ 60] Iter[ 66/ 84]\t\tLoss: 1.7327 \n",
            "| Epoch [ 19/ 60] Iter[ 71/ 84]\t\tLoss: 0.5312 \n",
            "| Epoch [ 19/ 60] Iter[ 76/ 84]\t\tLoss: 0.6238 \n",
            "| Epoch [ 19/ 60] Iter[ 81/ 84]\t\tLoss: 0.2057 \n",
            "\n",
            "| Validation Epoch #19\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #20, LR=0.0001\n",
            "| Epoch [ 20/ 60] Iter[  1/ 84]\t\tLoss: 0.1756 \n",
            "| Epoch [ 20/ 60] Iter[  6/ 84]\t\tLoss: 0.4857 \n",
            "| Epoch [ 20/ 60] Iter[ 11/ 84]\t\tLoss: 0.5385 \n",
            "| Epoch [ 20/ 60] Iter[ 16/ 84]\t\tLoss: 0.1984 \n",
            "| Epoch [ 20/ 60] Iter[ 21/ 84]\t\tLoss: 0.4481 \n",
            "| Epoch [ 20/ 60] Iter[ 26/ 84]\t\tLoss: 0.1868 \n",
            "| Epoch [ 20/ 60] Iter[ 31/ 84]\t\tLoss: 6.3886 \n",
            "| Epoch [ 20/ 60] Iter[ 36/ 84]\t\tLoss: 5.9550 \n",
            "| Epoch [ 20/ 60] Iter[ 41/ 84]\t\tLoss: 0.6073 \n",
            "| Epoch [ 20/ 60] Iter[ 46/ 84]\t\tLoss: 2.3540 \n",
            "| Epoch [ 20/ 60] Iter[ 51/ 84]\t\tLoss: 0.4727 \n",
            "| Epoch [ 20/ 60] Iter[ 56/ 84]\t\tLoss: 0.7520 \n",
            "| Epoch [ 20/ 60] Iter[ 61/ 84]\t\tLoss: 0.2924 \n",
            "| Epoch [ 20/ 60] Iter[ 66/ 84]\t\tLoss: 0.4624 \n",
            "| Epoch [ 20/ 60] Iter[ 71/ 84]\t\tLoss: 6.6015 \n",
            "| Epoch [ 20/ 60] Iter[ 76/ 84]\t\tLoss: 0.7710 \n",
            "| Epoch [ 20/ 60] Iter[ 81/ 84]\t\tLoss: 6.0081 \n",
            "\n",
            "| Validation Epoch #20\t\t\tLoss: 0.2268 \n",
            "\n",
            "=> Training Epoch #21, LR=0.0000\n",
            "| Epoch [ 21/ 60] Iter[  1/ 84]\t\tLoss: 0.2142 \n",
            "| Epoch [ 21/ 60] Iter[  6/ 84]\t\tLoss: 0.1354 \n",
            "| Epoch [ 21/ 60] Iter[ 11/ 84]\t\tLoss: 6.6436 \n",
            "| Epoch [ 21/ 60] Iter[ 16/ 84]\t\tLoss: 0.1072 \n",
            "| Epoch [ 21/ 60] Iter[ 21/ 84]\t\tLoss: 0.1847 \n",
            "| Epoch [ 21/ 60] Iter[ 26/ 84]\t\tLoss: 0.7982 \n",
            "| Epoch [ 21/ 60] Iter[ 31/ 84]\t\tLoss: 0.1648 \n",
            "| Epoch [ 21/ 60] Iter[ 36/ 84]\t\tLoss: 0.7416 \n",
            "| Epoch [ 21/ 60] Iter[ 41/ 84]\t\tLoss: 0.8636 \n",
            "| Epoch [ 21/ 60] Iter[ 46/ 84]\t\tLoss: 0.8638 \n",
            "| Epoch [ 21/ 60] Iter[ 51/ 84]\t\tLoss: 0.0902 \n",
            "| Epoch [ 21/ 60] Iter[ 56/ 84]\t\tLoss: 0.3756 \n",
            "| Epoch [ 21/ 60] Iter[ 61/ 84]\t\tLoss: 0.5368 \n",
            "| Epoch [ 21/ 60] Iter[ 66/ 84]\t\tLoss: 1.0289 \n",
            "| Epoch [ 21/ 60] Iter[ 71/ 84]\t\tLoss: 6.3234 \n",
            "| Epoch [ 21/ 60] Iter[ 76/ 84]\t\tLoss: 0.4617 \n",
            "| Epoch [ 21/ 60] Iter[ 81/ 84]\t\tLoss: 0.1950 \n",
            "\n",
            "| Validation Epoch #21\t\t\tLoss: 0.2267 \n",
            "\n",
            "=> Training Epoch #22, LR=0.0000\n",
            "| Epoch [ 22/ 60] Iter[  1/ 84]\t\tLoss: 5.3270 \n",
            "| Epoch [ 22/ 60] Iter[  6/ 84]\t\tLoss: 0.3831 \n",
            "| Epoch [ 22/ 60] Iter[ 11/ 84]\t\tLoss: 1.2644 \n",
            "| Epoch [ 22/ 60] Iter[ 16/ 84]\t\tLoss: 0.4733 \n",
            "| Epoch [ 22/ 60] Iter[ 21/ 84]\t\tLoss: 0.4924 \n",
            "| Epoch [ 22/ 60] Iter[ 26/ 84]\t\tLoss: 0.8865 \n",
            "| Epoch [ 22/ 60] Iter[ 31/ 84]\t\tLoss: 1.3013 \n",
            "| Epoch [ 22/ 60] Iter[ 36/ 84]\t\tLoss: 0.3934 \n",
            "| Epoch [ 22/ 60] Iter[ 41/ 84]\t\tLoss: 0.2393 \n",
            "| Epoch [ 22/ 60] Iter[ 46/ 84]\t\tLoss: 0.0350 \n",
            "| Epoch [ 22/ 60] Iter[ 51/ 84]\t\tLoss: 0.6865 \n",
            "| Epoch [ 22/ 60] Iter[ 56/ 84]\t\tLoss: 1.0074 \n",
            "| Epoch [ 22/ 60] Iter[ 61/ 84]\t\tLoss: 0.4372 \n",
            "| Epoch [ 22/ 60] Iter[ 66/ 84]\t\tLoss: 1.1267 \n",
            "| Epoch [ 22/ 60] Iter[ 71/ 84]\t\tLoss: 0.3895 \n",
            "| Epoch [ 22/ 60] Iter[ 76/ 84]\t\tLoss: 0.8992 \n",
            "| Epoch [ 22/ 60] Iter[ 81/ 84]\t\tLoss: 1.6119 \n",
            "\n",
            "| Validation Epoch #22\t\t\tLoss: 0.2267 \n",
            "\n",
            "=> Training Epoch #23, LR=0.0000\n",
            "| Epoch [ 23/ 60] Iter[  1/ 84]\t\tLoss: 0.1531 \n",
            "| Epoch [ 23/ 60] Iter[  6/ 84]\t\tLoss: 0.1780 \n",
            "| Epoch [ 23/ 60] Iter[ 11/ 84]\t\tLoss: 0.5561 \n",
            "| Epoch [ 23/ 60] Iter[ 16/ 84]\t\tLoss: 5.3477 \n",
            "| Epoch [ 23/ 60] Iter[ 21/ 84]\t\tLoss: 0.5221 \n",
            "| Epoch [ 23/ 60] Iter[ 26/ 84]\t\tLoss: 3.5161 \n",
            "| Epoch [ 23/ 60] Iter[ 31/ 84]\t\tLoss: 0.2669 \n",
            "| Epoch [ 23/ 60] Iter[ 36/ 84]\t\tLoss: 0.8845 \n",
            "| Epoch [ 23/ 60] Iter[ 41/ 84]\t\tLoss: 0.4277 \n",
            "| Epoch [ 23/ 60] Iter[ 46/ 84]\t\tLoss: 1.1707 \n",
            "| Epoch [ 23/ 60] Iter[ 51/ 84]\t\tLoss: 2.6351 \n",
            "| Epoch [ 23/ 60] Iter[ 56/ 84]\t\tLoss: 0.6884 \n",
            "| Epoch [ 23/ 60] Iter[ 61/ 84]\t\tLoss: 0.4130 \n",
            "| Epoch [ 23/ 60] Iter[ 66/ 84]\t\tLoss: 0.3578 \n",
            "| Epoch [ 23/ 60] Iter[ 71/ 84]\t\tLoss: 0.5230 \n",
            "| Epoch [ 23/ 60] Iter[ 76/ 84]\t\tLoss: 1.1699 \n",
            "| Epoch [ 23/ 60] Iter[ 81/ 84]\t\tLoss: 0.3871 \n",
            "\n",
            "| Validation Epoch #23\t\t\tLoss: 0.2271 \n",
            "\n",
            "=> Training Epoch #24, LR=0.0000\n",
            "| Epoch [ 24/ 60] Iter[  1/ 84]\t\tLoss: 0.1739 \n",
            "| Epoch [ 24/ 60] Iter[  6/ 84]\t\tLoss: 0.4008 \n",
            "| Epoch [ 24/ 60] Iter[ 11/ 84]\t\tLoss: 2.5327 \n",
            "| Epoch [ 24/ 60] Iter[ 16/ 84]\t\tLoss: 0.4504 \n",
            "| Epoch [ 24/ 60] Iter[ 21/ 84]\t\tLoss: 0.3760 \n",
            "| Epoch [ 24/ 60] Iter[ 26/ 84]\t\tLoss: 0.7096 \n",
            "| Epoch [ 24/ 60] Iter[ 31/ 84]\t\tLoss: 0.5070 \n",
            "| Epoch [ 24/ 60] Iter[ 36/ 84]\t\tLoss: 6.2601 \n",
            "| Epoch [ 24/ 60] Iter[ 41/ 84]\t\tLoss: 0.8465 \n",
            "| Epoch [ 24/ 60] Iter[ 46/ 84]\t\tLoss: 0.0510 \n",
            "| Epoch [ 24/ 60] Iter[ 51/ 84]\t\tLoss: 0.1776 \n",
            "| Epoch [ 24/ 60] Iter[ 56/ 84]\t\tLoss: 0.2645 \n",
            "| Epoch [ 24/ 60] Iter[ 61/ 84]\t\tLoss: 0.7921 \n",
            "| Epoch [ 24/ 60] Iter[ 66/ 84]\t\tLoss: 0.8800 \n",
            "| Epoch [ 24/ 60] Iter[ 71/ 84]\t\tLoss: 0.1350 \n",
            "| Epoch [ 24/ 60] Iter[ 76/ 84]\t\tLoss: 1.0455 \n",
            "| Epoch [ 24/ 60] Iter[ 81/ 84]\t\tLoss: 1.1540 \n",
            "\n",
            "| Validation Epoch #24\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #25, LR=0.0000\n",
            "| Epoch [ 25/ 60] Iter[  1/ 84]\t\tLoss: 0.3807 \n",
            "| Epoch [ 25/ 60] Iter[  6/ 84]\t\tLoss: 0.3107 \n",
            "| Epoch [ 25/ 60] Iter[ 11/ 84]\t\tLoss: 0.0543 \n",
            "| Epoch [ 25/ 60] Iter[ 16/ 84]\t\tLoss: 0.4975 \n",
            "| Epoch [ 25/ 60] Iter[ 21/ 84]\t\tLoss: 0.3718 \n",
            "| Epoch [ 25/ 60] Iter[ 26/ 84]\t\tLoss: 0.9566 \n",
            "| Epoch [ 25/ 60] Iter[ 31/ 84]\t\tLoss: 5.7242 \n",
            "| Epoch [ 25/ 60] Iter[ 36/ 84]\t\tLoss: 0.5116 \n",
            "| Epoch [ 25/ 60] Iter[ 41/ 84]\t\tLoss: 0.5653 \n",
            "| Epoch [ 25/ 60] Iter[ 46/ 84]\t\tLoss: 1.2154 \n",
            "| Epoch [ 25/ 60] Iter[ 51/ 84]\t\tLoss: 5.8152 \n",
            "| Epoch [ 25/ 60] Iter[ 56/ 84]\t\tLoss: 0.6160 \n",
            "| Epoch [ 25/ 60] Iter[ 61/ 84]\t\tLoss: 0.6543 \n",
            "| Epoch [ 25/ 60] Iter[ 66/ 84]\t\tLoss: 0.3141 \n",
            "| Epoch [ 25/ 60] Iter[ 71/ 84]\t\tLoss: 0.7817 \n",
            "| Epoch [ 25/ 60] Iter[ 76/ 84]\t\tLoss: 0.2526 \n",
            "| Epoch [ 25/ 60] Iter[ 81/ 84]\t\tLoss: 0.8673 \n",
            "\n",
            "| Validation Epoch #25\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #26, LR=0.0000\n",
            "| Epoch [ 26/ 60] Iter[  1/ 84]\t\tLoss: 0.2050 \n",
            "| Epoch [ 26/ 60] Iter[  6/ 84]\t\tLoss: 0.9483 \n",
            "| Epoch [ 26/ 60] Iter[ 11/ 84]\t\tLoss: 0.4638 \n",
            "| Epoch [ 26/ 60] Iter[ 16/ 84]\t\tLoss: 0.5298 \n",
            "| Epoch [ 26/ 60] Iter[ 21/ 84]\t\tLoss: 0.2712 \n",
            "| Epoch [ 26/ 60] Iter[ 26/ 84]\t\tLoss: 0.9436 \n",
            "| Epoch [ 26/ 60] Iter[ 31/ 84]\t\tLoss: 1.1256 \n",
            "| Epoch [ 26/ 60] Iter[ 36/ 84]\t\tLoss: 0.1769 \n",
            "| Epoch [ 26/ 60] Iter[ 41/ 84]\t\tLoss: 0.5333 \n",
            "| Epoch [ 26/ 60] Iter[ 46/ 84]\t\tLoss: 5.8901 \n",
            "| Epoch [ 26/ 60] Iter[ 51/ 84]\t\tLoss: 0.5596 \n",
            "| Epoch [ 26/ 60] Iter[ 56/ 84]\t\tLoss: 0.4516 \n",
            "| Epoch [ 26/ 60] Iter[ 61/ 84]\t\tLoss: 0.6069 \n",
            "| Epoch [ 26/ 60] Iter[ 66/ 84]\t\tLoss: 0.5326 \n",
            "| Epoch [ 26/ 60] Iter[ 71/ 84]\t\tLoss: 0.9270 \n",
            "| Epoch [ 26/ 60] Iter[ 76/ 84]\t\tLoss: 0.6710 \n",
            "| Epoch [ 26/ 60] Iter[ 81/ 84]\t\tLoss: 6.8348 \n",
            "\n",
            "| Validation Epoch #26\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #27, LR=0.0000\n",
            "| Epoch [ 27/ 60] Iter[  1/ 84]\t\tLoss: 0.3484 \n",
            "| Epoch [ 27/ 60] Iter[  6/ 84]\t\tLoss: 0.5328 \n",
            "| Epoch [ 27/ 60] Iter[ 11/ 84]\t\tLoss: 0.9607 \n",
            "| Epoch [ 27/ 60] Iter[ 16/ 84]\t\tLoss: 0.4142 \n",
            "| Epoch [ 27/ 60] Iter[ 21/ 84]\t\tLoss: 0.6077 \n",
            "| Epoch [ 27/ 60] Iter[ 26/ 84]\t\tLoss: 0.4693 \n",
            "| Epoch [ 27/ 60] Iter[ 31/ 84]\t\tLoss: 0.1796 \n",
            "| Epoch [ 27/ 60] Iter[ 36/ 84]\t\tLoss: 0.9021 \n",
            "| Epoch [ 27/ 60] Iter[ 41/ 84]\t\tLoss: 0.9670 \n",
            "| Epoch [ 27/ 60] Iter[ 46/ 84]\t\tLoss: 0.1194 \n",
            "| Epoch [ 27/ 60] Iter[ 51/ 84]\t\tLoss: 0.8377 \n",
            "| Epoch [ 27/ 60] Iter[ 56/ 84]\t\tLoss: 1.6031 \n",
            "| Epoch [ 27/ 60] Iter[ 61/ 84]\t\tLoss: 0.4761 \n",
            "| Epoch [ 27/ 60] Iter[ 66/ 84]\t\tLoss: 0.3580 \n",
            "| Epoch [ 27/ 60] Iter[ 71/ 84]\t\tLoss: 0.5423 \n",
            "| Epoch [ 27/ 60] Iter[ 76/ 84]\t\tLoss: 0.3625 \n",
            "| Epoch [ 27/ 60] Iter[ 81/ 84]\t\tLoss: 0.6003 \n",
            "\n",
            "| Validation Epoch #27\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #28, LR=0.0000\n",
            "| Epoch [ 28/ 60] Iter[  1/ 84]\t\tLoss: 0.3129 \n",
            "| Epoch [ 28/ 60] Iter[  6/ 84]\t\tLoss: 0.3406 \n",
            "| Epoch [ 28/ 60] Iter[ 11/ 84]\t\tLoss: 0.4265 \n",
            "| Epoch [ 28/ 60] Iter[ 16/ 84]\t\tLoss: 0.6939 \n",
            "| Epoch [ 28/ 60] Iter[ 21/ 84]\t\tLoss: 3.1055 \n",
            "| Epoch [ 28/ 60] Iter[ 26/ 84]\t\tLoss: 0.2611 \n",
            "| Epoch [ 28/ 60] Iter[ 31/ 84]\t\tLoss: 3.4543 \n",
            "| Epoch [ 28/ 60] Iter[ 36/ 84]\t\tLoss: 0.2671 \n",
            "| Epoch [ 28/ 60] Iter[ 41/ 84]\t\tLoss: 0.5628 \n",
            "| Epoch [ 28/ 60] Iter[ 46/ 84]\t\tLoss: 0.0580 \n",
            "| Epoch [ 28/ 60] Iter[ 51/ 84]\t\tLoss: 1.1805 \n",
            "| Epoch [ 28/ 60] Iter[ 56/ 84]\t\tLoss: 5.5064 \n",
            "| Epoch [ 28/ 60] Iter[ 61/ 84]\t\tLoss: 0.3783 \n",
            "| Epoch [ 28/ 60] Iter[ 66/ 84]\t\tLoss: 1.1480 \n",
            "| Epoch [ 28/ 60] Iter[ 71/ 84]\t\tLoss: 0.5000 \n",
            "| Epoch [ 28/ 60] Iter[ 76/ 84]\t\tLoss: 0.6448 \n",
            "| Epoch [ 28/ 60] Iter[ 81/ 84]\t\tLoss: 6.9216 \n",
            "\n",
            "| Validation Epoch #28\t\t\tLoss: 0.2270 \n",
            "\n",
            "=> Training Epoch #29, LR=0.0000\n",
            "| Epoch [ 29/ 60] Iter[  1/ 84]\t\tLoss: 0.5192 \n",
            "| Epoch [ 29/ 60] Iter[  6/ 84]\t\tLoss: 6.3849 \n",
            "| Epoch [ 29/ 60] Iter[ 11/ 84]\t\tLoss: 0.5881 \n",
            "| Epoch [ 29/ 60] Iter[ 16/ 84]\t\tLoss: 0.9552 \n",
            "| Epoch [ 29/ 60] Iter[ 21/ 84]\t\tLoss: 6.2627 \n",
            "| Epoch [ 29/ 60] Iter[ 26/ 84]\t\tLoss: 6.4482 \n",
            "| Epoch [ 29/ 60] Iter[ 31/ 84]\t\tLoss: 1.2755 \n",
            "| Epoch [ 29/ 60] Iter[ 36/ 84]\t\tLoss: 0.2506 \n",
            "| Epoch [ 29/ 60] Iter[ 41/ 84]\t\tLoss: 1.6203 \n",
            "| Epoch [ 29/ 60] Iter[ 46/ 84]\t\tLoss: 0.3135 \n",
            "| Epoch [ 29/ 60] Iter[ 51/ 84]\t\tLoss: 0.0401 \n",
            "| Epoch [ 29/ 60] Iter[ 56/ 84]\t\tLoss: 0.0255 \n",
            "| Epoch [ 29/ 60] Iter[ 61/ 84]\t\tLoss: 0.1766 \n",
            "| Epoch [ 29/ 60] Iter[ 66/ 84]\t\tLoss: 0.8864 \n",
            "| Epoch [ 29/ 60] Iter[ 71/ 84]\t\tLoss: 0.1655 \n",
            "| Epoch [ 29/ 60] Iter[ 76/ 84]\t\tLoss: 0.6811 \n",
            "| Epoch [ 29/ 60] Iter[ 81/ 84]\t\tLoss: 0.9088 \n",
            "\n",
            "| Validation Epoch #29\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #30, LR=0.0000\n",
            "| Epoch [ 30/ 60] Iter[  1/ 84]\t\tLoss: 0.8793 \n",
            "| Epoch [ 30/ 60] Iter[  6/ 84]\t\tLoss: 0.4234 \n",
            "| Epoch [ 30/ 60] Iter[ 11/ 84]\t\tLoss: 0.2890 \n",
            "| Epoch [ 30/ 60] Iter[ 16/ 84]\t\tLoss: 0.2914 \n",
            "| Epoch [ 30/ 60] Iter[ 21/ 84]\t\tLoss: 0.1953 \n",
            "| Epoch [ 30/ 60] Iter[ 26/ 84]\t\tLoss: 0.7881 \n",
            "| Epoch [ 30/ 60] Iter[ 31/ 84]\t\tLoss: 6.2107 \n",
            "| Epoch [ 30/ 60] Iter[ 36/ 84]\t\tLoss: 0.4338 \n",
            "| Epoch [ 30/ 60] Iter[ 41/ 84]\t\tLoss: 0.7087 \n",
            "| Epoch [ 30/ 60] Iter[ 46/ 84]\t\tLoss: 1.1456 \n",
            "| Epoch [ 30/ 60] Iter[ 51/ 84]\t\tLoss: 6.5894 \n",
            "| Epoch [ 30/ 60] Iter[ 56/ 84]\t\tLoss: 6.6938 \n",
            "| Epoch [ 30/ 60] Iter[ 61/ 84]\t\tLoss: 6.0976 \n",
            "| Epoch [ 30/ 60] Iter[ 66/ 84]\t\tLoss: 1.0980 \n",
            "| Epoch [ 30/ 60] Iter[ 71/ 84]\t\tLoss: 0.3439 \n",
            "| Epoch [ 30/ 60] Iter[ 76/ 84]\t\tLoss: 0.1492 \n",
            "| Epoch [ 30/ 60] Iter[ 81/ 84]\t\tLoss: 0.8561 \n",
            "\n",
            "| Validation Epoch #30\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #31, LR=0.0000\n",
            "| Epoch [ 31/ 60] Iter[  1/ 84]\t\tLoss: 0.5019 \n",
            "| Epoch [ 31/ 60] Iter[  6/ 84]\t\tLoss: 0.6126 \n",
            "| Epoch [ 31/ 60] Iter[ 11/ 84]\t\tLoss: 0.3309 \n",
            "| Epoch [ 31/ 60] Iter[ 16/ 84]\t\tLoss: 6.1620 \n",
            "| Epoch [ 31/ 60] Iter[ 21/ 84]\t\tLoss: 0.4663 \n",
            "| Epoch [ 31/ 60] Iter[ 26/ 84]\t\tLoss: 0.3505 \n",
            "| Epoch [ 31/ 60] Iter[ 31/ 84]\t\tLoss: 0.4626 \n",
            "| Epoch [ 31/ 60] Iter[ 36/ 84]\t\tLoss: 0.3344 \n",
            "| Epoch [ 31/ 60] Iter[ 41/ 84]\t\tLoss: 1.3152 \n",
            "| Epoch [ 31/ 60] Iter[ 46/ 84]\t\tLoss: 3.9891 \n",
            "| Epoch [ 31/ 60] Iter[ 51/ 84]\t\tLoss: 0.4241 \n",
            "| Epoch [ 31/ 60] Iter[ 56/ 84]\t\tLoss: 6.1200 \n",
            "| Epoch [ 31/ 60] Iter[ 61/ 84]\t\tLoss: 0.9044 \n",
            "| Epoch [ 31/ 60] Iter[ 66/ 84]\t\tLoss: 1.1864 \n",
            "| Epoch [ 31/ 60] Iter[ 71/ 84]\t\tLoss: 0.5161 \n",
            "| Epoch [ 31/ 60] Iter[ 76/ 84]\t\tLoss: 0.5153 \n",
            "| Epoch [ 31/ 60] Iter[ 81/ 84]\t\tLoss: 0.2952 \n",
            "\n",
            "| Validation Epoch #31\t\t\tLoss: 0.2277 \n",
            "\n",
            "=> Training Epoch #32, LR=0.0000\n",
            "| Epoch [ 32/ 60] Iter[  1/ 84]\t\tLoss: 0.4954 \n",
            "| Epoch [ 32/ 60] Iter[  6/ 84]\t\tLoss: 0.6495 \n",
            "| Epoch [ 32/ 60] Iter[ 11/ 84]\t\tLoss: 0.9350 \n",
            "| Epoch [ 32/ 60] Iter[ 16/ 84]\t\tLoss: 0.0952 \n",
            "| Epoch [ 32/ 60] Iter[ 21/ 84]\t\tLoss: 1.8476 \n",
            "| Epoch [ 32/ 60] Iter[ 26/ 84]\t\tLoss: 0.8180 \n",
            "| Epoch [ 32/ 60] Iter[ 31/ 84]\t\tLoss: 1.3007 \n",
            "| Epoch [ 32/ 60] Iter[ 36/ 84]\t\tLoss: 0.6681 \n",
            "| Epoch [ 32/ 60] Iter[ 41/ 84]\t\tLoss: 0.6204 \n",
            "| Epoch [ 32/ 60] Iter[ 46/ 84]\t\tLoss: 0.1859 \n",
            "| Epoch [ 32/ 60] Iter[ 51/ 84]\t\tLoss: 1.1415 \n",
            "| Epoch [ 32/ 60] Iter[ 56/ 84]\t\tLoss: 6.4847 \n",
            "| Epoch [ 32/ 60] Iter[ 61/ 84]\t\tLoss: 0.9329 \n",
            "| Epoch [ 32/ 60] Iter[ 66/ 84]\t\tLoss: 0.7041 \n",
            "| Epoch [ 32/ 60] Iter[ 71/ 84]\t\tLoss: 0.0891 \n",
            "| Epoch [ 32/ 60] Iter[ 76/ 84]\t\tLoss: 0.1148 \n",
            "| Epoch [ 32/ 60] Iter[ 81/ 84]\t\tLoss: 0.6381 \n",
            "\n",
            "| Validation Epoch #32\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #33, LR=0.0000\n",
            "| Epoch [ 33/ 60] Iter[  1/ 84]\t\tLoss: 0.3614 \n",
            "| Epoch [ 33/ 60] Iter[  6/ 84]\t\tLoss: 0.6065 \n",
            "| Epoch [ 33/ 60] Iter[ 11/ 84]\t\tLoss: 0.0409 \n",
            "| Epoch [ 33/ 60] Iter[ 16/ 84]\t\tLoss: 5.8147 \n",
            "| Epoch [ 33/ 60] Iter[ 21/ 84]\t\tLoss: 0.2660 \n",
            "| Epoch [ 33/ 60] Iter[ 26/ 84]\t\tLoss: 0.9529 \n",
            "| Epoch [ 33/ 60] Iter[ 31/ 84]\t\tLoss: 0.7129 \n",
            "| Epoch [ 33/ 60] Iter[ 36/ 84]\t\tLoss: 0.9395 \n",
            "| Epoch [ 33/ 60] Iter[ 41/ 84]\t\tLoss: 0.1504 \n",
            "| Epoch [ 33/ 60] Iter[ 46/ 84]\t\tLoss: 6.6815 \n",
            "| Epoch [ 33/ 60] Iter[ 51/ 84]\t\tLoss: 0.0703 \n",
            "| Epoch [ 33/ 60] Iter[ 56/ 84]\t\tLoss: 0.3348 \n",
            "| Epoch [ 33/ 60] Iter[ 61/ 84]\t\tLoss: 0.8195 \n",
            "| Epoch [ 33/ 60] Iter[ 66/ 84]\t\tLoss: 0.5549 \n",
            "| Epoch [ 33/ 60] Iter[ 71/ 84]\t\tLoss: 6.0780 \n",
            "| Epoch [ 33/ 60] Iter[ 76/ 84]\t\tLoss: 0.2761 \n",
            "| Epoch [ 33/ 60] Iter[ 81/ 84]\t\tLoss: 0.5561 \n",
            "\n",
            "| Validation Epoch #33\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #34, LR=0.0000\n",
            "| Epoch [ 34/ 60] Iter[  1/ 84]\t\tLoss: 0.8654 \n",
            "| Epoch [ 34/ 60] Iter[  6/ 84]\t\tLoss: 0.3007 \n",
            "| Epoch [ 34/ 60] Iter[ 11/ 84]\t\tLoss: 0.5507 \n",
            "| Epoch [ 34/ 60] Iter[ 16/ 84]\t\tLoss: 0.4771 \n",
            "| Epoch [ 34/ 60] Iter[ 21/ 84]\t\tLoss: 1.0980 \n",
            "| Epoch [ 34/ 60] Iter[ 26/ 84]\t\tLoss: 0.3058 \n",
            "| Epoch [ 34/ 60] Iter[ 31/ 84]\t\tLoss: 3.5916 \n",
            "| Epoch [ 34/ 60] Iter[ 36/ 84]\t\tLoss: 0.3157 \n",
            "| Epoch [ 34/ 60] Iter[ 41/ 84]\t\tLoss: 0.2357 \n",
            "| Epoch [ 34/ 60] Iter[ 46/ 84]\t\tLoss: 0.6444 \n",
            "| Epoch [ 34/ 60] Iter[ 51/ 84]\t\tLoss: 0.2150 \n",
            "| Epoch [ 34/ 60] Iter[ 56/ 84]\t\tLoss: 0.3651 \n",
            "| Epoch [ 34/ 60] Iter[ 61/ 84]\t\tLoss: 0.1642 \n",
            "| Epoch [ 34/ 60] Iter[ 66/ 84]\t\tLoss: 0.1171 \n",
            "| Epoch [ 34/ 60] Iter[ 71/ 84]\t\tLoss: 0.4808 \n",
            "| Epoch [ 34/ 60] Iter[ 76/ 84]\t\tLoss: 0.6915 \n",
            "| Epoch [ 34/ 60] Iter[ 81/ 84]\t\tLoss: 0.2112 \n",
            "\n",
            "| Validation Epoch #34\t\t\tLoss: 0.2272 \n",
            "\n",
            "=> Training Epoch #35, LR=0.0000\n",
            "| Epoch [ 35/ 60] Iter[  1/ 84]\t\tLoss: 0.0884 \n",
            "| Epoch [ 35/ 60] Iter[  6/ 84]\t\tLoss: 0.4230 \n",
            "| Epoch [ 35/ 60] Iter[ 11/ 84]\t\tLoss: 0.5071 \n",
            "| Epoch [ 35/ 60] Iter[ 16/ 84]\t\tLoss: 1.0464 \n",
            "| Epoch [ 35/ 60] Iter[ 21/ 84]\t\tLoss: 0.3507 \n",
            "| Epoch [ 35/ 60] Iter[ 26/ 84]\t\tLoss: 0.9483 \n",
            "| Epoch [ 35/ 60] Iter[ 31/ 84]\t\tLoss: 1.0520 \n",
            "| Epoch [ 35/ 60] Iter[ 36/ 84]\t\tLoss: 0.0352 \n",
            "| Epoch [ 35/ 60] Iter[ 41/ 84]\t\tLoss: 0.3374 \n",
            "| Epoch [ 35/ 60] Iter[ 46/ 84]\t\tLoss: 0.4841 \n",
            "| Epoch [ 35/ 60] Iter[ 51/ 84]\t\tLoss: 0.4037 \n",
            "| Epoch [ 35/ 60] Iter[ 56/ 84]\t\tLoss: 2.7736 \n",
            "| Epoch [ 35/ 60] Iter[ 61/ 84]\t\tLoss: 0.3203 \n",
            "| Epoch [ 35/ 60] Iter[ 66/ 84]\t\tLoss: 6.2068 \n",
            "| Epoch [ 35/ 60] Iter[ 71/ 84]\t\tLoss: 0.5382 \n",
            "| Epoch [ 35/ 60] Iter[ 76/ 84]\t\tLoss: 0.1171 \n",
            "| Epoch [ 35/ 60] Iter[ 81/ 84]\t\tLoss: 1.0074 \n",
            "\n",
            "| Validation Epoch #35\t\t\tLoss: 0.2266 \n",
            "\n",
            "=> Training Epoch #36, LR=0.0000\n",
            "| Epoch [ 36/ 60] Iter[  1/ 84]\t\tLoss: 0.2923 \n",
            "| Epoch [ 36/ 60] Iter[  6/ 84]\t\tLoss: 0.3053 \n",
            "| Epoch [ 36/ 60] Iter[ 11/ 84]\t\tLoss: 5.8947 \n",
            "| Epoch [ 36/ 60] Iter[ 16/ 84]\t\tLoss: 0.1387 \n",
            "| Epoch [ 36/ 60] Iter[ 21/ 84]\t\tLoss: 0.0710 \n",
            "| Epoch [ 36/ 60] Iter[ 26/ 84]\t\tLoss: 1.3329 \n",
            "| Epoch [ 36/ 60] Iter[ 31/ 84]\t\tLoss: 0.7783 \n",
            "| Epoch [ 36/ 60] Iter[ 36/ 84]\t\tLoss: 0.4109 \n",
            "| Epoch [ 36/ 60] Iter[ 41/ 84]\t\tLoss: 0.6077 \n",
            "| Epoch [ 36/ 60] Iter[ 46/ 84]\t\tLoss: 0.8522 \n",
            "| Epoch [ 36/ 60] Iter[ 51/ 84]\t\tLoss: 0.2059 \n",
            "| Epoch [ 36/ 60] Iter[ 56/ 84]\t\tLoss: 1.3284 \n",
            "| Epoch [ 36/ 60] Iter[ 61/ 84]\t\tLoss: 0.6569 \n",
            "| Epoch [ 36/ 60] Iter[ 66/ 84]\t\tLoss: 0.2338 \n",
            "| Epoch [ 36/ 60] Iter[ 71/ 84]\t\tLoss: 0.4434 \n",
            "| Epoch [ 36/ 60] Iter[ 76/ 84]\t\tLoss: 0.1188 \n",
            "| Epoch [ 36/ 60] Iter[ 81/ 84]\t\tLoss: 1.7999 \n",
            "\n",
            "| Validation Epoch #36\t\t\tLoss: 0.2264 \n",
            "\n",
            "=> Training Epoch #37, LR=0.0000\n",
            "| Epoch [ 37/ 60] Iter[  1/ 84]\t\tLoss: 0.0276 \n",
            "| Epoch [ 37/ 60] Iter[  6/ 84]\t\tLoss: 1.0309 \n",
            "| Epoch [ 37/ 60] Iter[ 11/ 84]\t\tLoss: 0.8399 \n",
            "| Epoch [ 37/ 60] Iter[ 16/ 84]\t\tLoss: 6.0261 \n",
            "| Epoch [ 37/ 60] Iter[ 21/ 84]\t\tLoss: 0.3695 \n",
            "| Epoch [ 37/ 60] Iter[ 26/ 84]\t\tLoss: 3.9182 \n",
            "| Epoch [ 37/ 60] Iter[ 31/ 84]\t\tLoss: 0.4576 \n",
            "| Epoch [ 37/ 60] Iter[ 36/ 84]\t\tLoss: 0.4144 \n",
            "| Epoch [ 37/ 60] Iter[ 41/ 84]\t\tLoss: 6.2934 \n",
            "| Epoch [ 37/ 60] Iter[ 46/ 84]\t\tLoss: 0.6048 \n",
            "| Epoch [ 37/ 60] Iter[ 51/ 84]\t\tLoss: 0.1421 \n",
            "| Epoch [ 37/ 60] Iter[ 56/ 84]\t\tLoss: 0.2786 \n",
            "| Epoch [ 37/ 60] Iter[ 61/ 84]\t\tLoss: 0.9232 \n",
            "| Epoch [ 37/ 60] Iter[ 66/ 84]\t\tLoss: 0.9698 \n",
            "| Epoch [ 37/ 60] Iter[ 71/ 84]\t\tLoss: 0.5867 \n",
            "| Epoch [ 37/ 60] Iter[ 76/ 84]\t\tLoss: 0.2414 \n",
            "| Epoch [ 37/ 60] Iter[ 81/ 84]\t\tLoss: 2.4505 \n",
            "\n",
            "| Validation Epoch #37\t\t\tLoss: 0.2265 \n",
            "\n",
            "=> Training Epoch #38, LR=0.0000\n",
            "| Epoch [ 38/ 60] Iter[  1/ 84]\t\tLoss: 0.8590 \n",
            "| Epoch [ 38/ 60] Iter[  6/ 84]\t\tLoss: 0.3297 \n",
            "| Epoch [ 38/ 60] Iter[ 11/ 84]\t\tLoss: 0.7083 \n",
            "| Epoch [ 38/ 60] Iter[ 16/ 84]\t\tLoss: 0.1371 \n",
            "| Epoch [ 38/ 60] Iter[ 21/ 84]\t\tLoss: 3.4542 \n",
            "| Epoch [ 38/ 60] Iter[ 26/ 84]\t\tLoss: 5.7999 \n",
            "| Epoch [ 38/ 60] Iter[ 31/ 84]\t\tLoss: 0.8956 \n",
            "| Epoch [ 38/ 60] Iter[ 36/ 84]\t\tLoss: 0.6451 \n",
            "| Epoch [ 38/ 60] Iter[ 41/ 84]\t\tLoss: 0.2163 \n",
            "| Epoch [ 38/ 60] Iter[ 46/ 84]\t\tLoss: 0.1569 \n",
            "| Epoch [ 38/ 60] Iter[ 51/ 84]\t\tLoss: 0.2351 \n",
            "| Epoch [ 38/ 60] Iter[ 56/ 84]\t\tLoss: 0.2769 \n",
            "| Epoch [ 38/ 60] Iter[ 61/ 84]\t\tLoss: 0.7037 \n",
            "| Epoch [ 38/ 60] Iter[ 66/ 84]\t\tLoss: 0.1923 \n",
            "| Epoch [ 38/ 60] Iter[ 71/ 84]\t\tLoss: 0.7067 \n",
            "| Epoch [ 38/ 60] Iter[ 76/ 84]\t\tLoss: 0.5074 \n",
            "| Epoch [ 38/ 60] Iter[ 81/ 84]\t\tLoss: 0.7529 \n",
            "\n",
            "| Validation Epoch #38\t\t\tLoss: 0.2262 \n",
            "\n",
            "=> Training Epoch #39, LR=0.0000\n",
            "| Epoch [ 39/ 60] Iter[  1/ 84]\t\tLoss: 0.5576 \n",
            "| Epoch [ 39/ 60] Iter[  6/ 84]\t\tLoss: 1.5482 \n",
            "| Epoch [ 39/ 60] Iter[ 11/ 84]\t\tLoss: 0.5293 \n",
            "| Epoch [ 39/ 60] Iter[ 16/ 84]\t\tLoss: 0.4446 \n",
            "| Epoch [ 39/ 60] Iter[ 21/ 84]\t\tLoss: 0.3995 \n",
            "| Epoch [ 39/ 60] Iter[ 26/ 84]\t\tLoss: 0.7899 \n",
            "| Epoch [ 39/ 60] Iter[ 31/ 84]\t\tLoss: 0.6263 \n",
            "| Epoch [ 39/ 60] Iter[ 36/ 84]\t\tLoss: 5.4296 \n",
            "| Epoch [ 39/ 60] Iter[ 41/ 84]\t\tLoss: 0.3969 \n",
            "| Epoch [ 39/ 60] Iter[ 46/ 84]\t\tLoss: 0.4284 \n",
            "| Epoch [ 39/ 60] Iter[ 51/ 84]\t\tLoss: 0.6312 \n",
            "| Epoch [ 39/ 60] Iter[ 56/ 84]\t\tLoss: 0.7274 \n",
            "| Epoch [ 39/ 60] Iter[ 61/ 84]\t\tLoss: 0.1456 \n",
            "| Epoch [ 39/ 60] Iter[ 66/ 84]\t\tLoss: 0.7167 \n",
            "| Epoch [ 39/ 60] Iter[ 71/ 84]\t\tLoss: 0.3037 \n",
            "| Epoch [ 39/ 60] Iter[ 76/ 84]\t\tLoss: 1.1791 \n",
            "| Epoch [ 39/ 60] Iter[ 81/ 84]\t\tLoss: 0.6703 \n",
            "\n",
            "| Validation Epoch #39\t\t\tLoss: 0.2262 \n",
            "\n",
            "=> Training Epoch #40, LR=0.0000\n",
            "| Epoch [ 40/ 60] Iter[  1/ 84]\t\tLoss: 0.8448 \n",
            "| Epoch [ 40/ 60] Iter[  6/ 84]\t\tLoss: 1.4617 \n",
            "| Epoch [ 40/ 60] Iter[ 11/ 84]\t\tLoss: 0.6350 \n",
            "| Epoch [ 40/ 60] Iter[ 16/ 84]\t\tLoss: 0.1528 \n",
            "| Epoch [ 40/ 60] Iter[ 21/ 84]\t\tLoss: 0.2494 \n",
            "| Epoch [ 40/ 60] Iter[ 26/ 84]\t\tLoss: 6.3424 \n",
            "| Epoch [ 40/ 60] Iter[ 31/ 84]\t\tLoss: 0.8977 \n",
            "| Epoch [ 40/ 60] Iter[ 36/ 84]\t\tLoss: 1.8486 \n",
            "| Epoch [ 40/ 60] Iter[ 41/ 84]\t\tLoss: 0.9089 \n",
            "| Epoch [ 40/ 60] Iter[ 46/ 84]\t\tLoss: 0.2027 \n",
            "| Epoch [ 40/ 60] Iter[ 51/ 84]\t\tLoss: 0.3481 \n",
            "| Epoch [ 40/ 60] Iter[ 56/ 84]\t\tLoss: 0.6938 \n",
            "| Epoch [ 40/ 60] Iter[ 61/ 84]\t\tLoss: 0.1678 \n",
            "| Epoch [ 40/ 60] Iter[ 66/ 84]\t\tLoss: 0.5103 \n",
            "| Epoch [ 40/ 60] Iter[ 71/ 84]\t\tLoss: 0.3443 \n",
            "| Epoch [ 40/ 60] Iter[ 76/ 84]\t\tLoss: 6.0824 \n",
            "| Epoch [ 40/ 60] Iter[ 81/ 84]\t\tLoss: 0.4246 \n",
            "\n",
            "| Validation Epoch #40\t\t\tLoss: 0.2265 \n",
            "\n",
            "=> Training Epoch #41, LR=0.0000\n",
            "| Epoch [ 41/ 60] Iter[  1/ 84]\t\tLoss: 0.1202 \n",
            "| Epoch [ 41/ 60] Iter[  6/ 84]\t\tLoss: 0.1513 \n",
            "| Epoch [ 41/ 60] Iter[ 11/ 84]\t\tLoss: 0.1969 \n",
            "| Epoch [ 41/ 60] Iter[ 16/ 84]\t\tLoss: 0.2394 \n",
            "| Epoch [ 41/ 60] Iter[ 21/ 84]\t\tLoss: 0.0343 \n",
            "| Epoch [ 41/ 60] Iter[ 26/ 84]\t\tLoss: 0.3251 \n",
            "| Epoch [ 41/ 60] Iter[ 31/ 84]\t\tLoss: 0.3250 \n",
            "| Epoch [ 41/ 60] Iter[ 36/ 84]\t\tLoss: 5.9060 \n",
            "| Epoch [ 41/ 60] Iter[ 41/ 84]\t\tLoss: 6.6138 \n",
            "| Epoch [ 41/ 60] Iter[ 46/ 84]\t\tLoss: 6.3163 \n",
            "| Epoch [ 41/ 60] Iter[ 51/ 84]\t\tLoss: 5.4887 \n",
            "| Epoch [ 41/ 60] Iter[ 56/ 84]\t\tLoss: 0.5593 \n",
            "| Epoch [ 41/ 60] Iter[ 61/ 84]\t\tLoss: 1.2342 \n",
            "| Epoch [ 41/ 60] Iter[ 66/ 84]\t\tLoss: 0.2723 \n",
            "| Epoch [ 41/ 60] Iter[ 71/ 84]\t\tLoss: 0.5854 \n",
            "| Epoch [ 41/ 60] Iter[ 76/ 84]\t\tLoss: 0.1002 \n",
            "| Epoch [ 41/ 60] Iter[ 81/ 84]\t\tLoss: 0.0207 \n",
            "\n",
            "| Validation Epoch #41\t\t\tLoss: 0.2264 \n",
            "\n",
            "=> Training Epoch #42, LR=0.0000\n",
            "| Epoch [ 42/ 60] Iter[  1/ 84]\t\tLoss: 0.1682 \n",
            "| Epoch [ 42/ 60] Iter[  6/ 84]\t\tLoss: 0.3355 \n",
            "| Epoch [ 42/ 60] Iter[ 11/ 84]\t\tLoss: 0.2546 \n",
            "| Epoch [ 42/ 60] Iter[ 16/ 84]\t\tLoss: 0.5634 \n",
            "| Epoch [ 42/ 60] Iter[ 21/ 84]\t\tLoss: 0.0683 \n",
            "| Epoch [ 42/ 60] Iter[ 26/ 84]\t\tLoss: 0.0995 \n",
            "| Epoch [ 42/ 60] Iter[ 31/ 84]\t\tLoss: 0.1034 \n",
            "| Epoch [ 42/ 60] Iter[ 36/ 84]\t\tLoss: 0.5787 \n",
            "| Epoch [ 42/ 60] Iter[ 41/ 84]\t\tLoss: 0.3620 \n",
            "| Epoch [ 42/ 60] Iter[ 46/ 84]\t\tLoss: 5.8404 \n",
            "| Epoch [ 42/ 60] Iter[ 51/ 84]\t\tLoss: 0.8533 \n",
            "| Epoch [ 42/ 60] Iter[ 56/ 84]\t\tLoss: 0.8868 \n",
            "| Epoch [ 42/ 60] Iter[ 61/ 84]\t\tLoss: 0.2844 \n",
            "| Epoch [ 42/ 60] Iter[ 66/ 84]\t\tLoss: 0.2574 \n",
            "| Epoch [ 42/ 60] Iter[ 71/ 84]\t\tLoss: 0.9889 \n",
            "| Epoch [ 42/ 60] Iter[ 76/ 84]\t\tLoss: 0.4772 \n",
            "| Epoch [ 42/ 60] Iter[ 81/ 84]\t\tLoss: 0.8304 \n",
            "\n",
            "| Validation Epoch #42\t\t\tLoss: 0.2267 \n",
            "\n",
            "=> Training Epoch #43, LR=0.0000\n",
            "| Epoch [ 43/ 60] Iter[  1/ 84]\t\tLoss: 0.4305 \n",
            "| Epoch [ 43/ 60] Iter[  6/ 84]\t\tLoss: 0.5321 \n",
            "| Epoch [ 43/ 60] Iter[ 11/ 84]\t\tLoss: 0.0427 \n",
            "| Epoch [ 43/ 60] Iter[ 16/ 84]\t\tLoss: 1.2106 \n",
            "| Epoch [ 43/ 60] Iter[ 21/ 84]\t\tLoss: 6.2373 \n",
            "| Epoch [ 43/ 60] Iter[ 26/ 84]\t\tLoss: 0.6232 \n",
            "| Epoch [ 43/ 60] Iter[ 31/ 84]\t\tLoss: 0.7769 \n",
            "| Epoch [ 43/ 60] Iter[ 36/ 84]\t\tLoss: 0.2500 \n",
            "| Epoch [ 43/ 60] Iter[ 41/ 84]\t\tLoss: 0.1378 \n",
            "| Epoch [ 43/ 60] Iter[ 46/ 84]\t\tLoss: 0.1818 \n",
            "| Epoch [ 43/ 60] Iter[ 51/ 84]\t\tLoss: 0.3062 \n",
            "| Epoch [ 43/ 60] Iter[ 56/ 84]\t\tLoss: 0.0433 \n",
            "| Epoch [ 43/ 60] Iter[ 61/ 84]\t\tLoss: 0.4384 \n",
            "| Epoch [ 43/ 60] Iter[ 66/ 84]\t\tLoss: 0.9905 \n",
            "| Epoch [ 43/ 60] Iter[ 71/ 84]\t\tLoss: 3.4675 \n",
            "| Epoch [ 43/ 60] Iter[ 76/ 84]\t\tLoss: 0.8909 \n",
            "| Epoch [ 43/ 60] Iter[ 81/ 84]\t\tLoss: 0.2668 \n",
            "\n",
            "| Validation Epoch #43\t\t\tLoss: 0.2270 \n",
            "\n",
            "=> Training Epoch #44, LR=0.0000\n",
            "| Epoch [ 44/ 60] Iter[  1/ 84]\t\tLoss: 5.9721 \n",
            "| Epoch [ 44/ 60] Iter[  6/ 84]\t\tLoss: 0.7365 \n",
            "| Epoch [ 44/ 60] Iter[ 11/ 84]\t\tLoss: 5.5451 \n",
            "| Epoch [ 44/ 60] Iter[ 16/ 84]\t\tLoss: 1.2767 \n",
            "| Epoch [ 44/ 60] Iter[ 21/ 84]\t\tLoss: 0.8213 \n",
            "| Epoch [ 44/ 60] Iter[ 26/ 84]\t\tLoss: 5.8739 \n",
            "| Epoch [ 44/ 60] Iter[ 31/ 84]\t\tLoss: 0.2807 \n",
            "| Epoch [ 44/ 60] Iter[ 36/ 84]\t\tLoss: 0.5558 \n",
            "| Epoch [ 44/ 60] Iter[ 41/ 84]\t\tLoss: 0.3920 \n",
            "| Epoch [ 44/ 60] Iter[ 46/ 84]\t\tLoss: 0.5853 \n",
            "| Epoch [ 44/ 60] Iter[ 51/ 84]\t\tLoss: 0.3216 \n",
            "| Epoch [ 44/ 60] Iter[ 56/ 84]\t\tLoss: 0.4636 \n",
            "| Epoch [ 44/ 60] Iter[ 61/ 84]\t\tLoss: 0.9506 \n",
            "| Epoch [ 44/ 60] Iter[ 66/ 84]\t\tLoss: 0.6635 \n",
            "| Epoch [ 44/ 60] Iter[ 71/ 84]\t\tLoss: 1.0578 \n",
            "| Epoch [ 44/ 60] Iter[ 76/ 84]\t\tLoss: 0.8126 \n",
            "| Epoch [ 44/ 60] Iter[ 81/ 84]\t\tLoss: 3.5034 \n",
            "\n",
            "| Validation Epoch #44\t\t\tLoss: 0.2271 \n",
            "\n",
            "=> Training Epoch #45, LR=0.0000\n",
            "| Epoch [ 45/ 60] Iter[  1/ 84]\t\tLoss: 0.7801 \n",
            "| Epoch [ 45/ 60] Iter[  6/ 84]\t\tLoss: 0.6127 \n",
            "| Epoch [ 45/ 60] Iter[ 11/ 84]\t\tLoss: 0.7234 \n",
            "| Epoch [ 45/ 60] Iter[ 16/ 84]\t\tLoss: 0.1476 \n",
            "| Epoch [ 45/ 60] Iter[ 21/ 84]\t\tLoss: 6.0759 \n",
            "| Epoch [ 45/ 60] Iter[ 26/ 84]\t\tLoss: 6.2750 \n",
            "| Epoch [ 45/ 60] Iter[ 31/ 84]\t\tLoss: 0.3940 \n",
            "| Epoch [ 45/ 60] Iter[ 36/ 84]\t\tLoss: 0.2264 \n",
            "| Epoch [ 45/ 60] Iter[ 41/ 84]\t\tLoss: 0.7384 \n",
            "| Epoch [ 45/ 60] Iter[ 46/ 84]\t\tLoss: 0.3812 \n",
            "| Epoch [ 45/ 60] Iter[ 51/ 84]\t\tLoss: 0.2525 \n",
            "| Epoch [ 45/ 60] Iter[ 56/ 84]\t\tLoss: 2.4577 \n",
            "| Epoch [ 45/ 60] Iter[ 61/ 84]\t\tLoss: 0.8204 \n",
            "| Epoch [ 45/ 60] Iter[ 66/ 84]\t\tLoss: 0.2619 \n",
            "| Epoch [ 45/ 60] Iter[ 71/ 84]\t\tLoss: 0.6568 \n",
            "| Epoch [ 45/ 60] Iter[ 76/ 84]\t\tLoss: 6.4317 \n",
            "| Epoch [ 45/ 60] Iter[ 81/ 84]\t\tLoss: 0.4217 \n",
            "\n",
            "| Validation Epoch #45\t\t\tLoss: 0.2267 \n",
            "\n",
            "=> Training Epoch #46, LR=0.0000\n",
            "| Epoch [ 46/ 60] Iter[  1/ 84]\t\tLoss: 0.3437 \n",
            "| Epoch [ 46/ 60] Iter[  6/ 84]\t\tLoss: 0.2210 \n",
            "| Epoch [ 46/ 60] Iter[ 11/ 84]\t\tLoss: 0.0913 \n",
            "| Epoch [ 46/ 60] Iter[ 16/ 84]\t\tLoss: 6.1485 \n",
            "| Epoch [ 46/ 60] Iter[ 21/ 84]\t\tLoss: 6.3558 \n",
            "| Epoch [ 46/ 60] Iter[ 26/ 84]\t\tLoss: 0.5432 \n",
            "| Epoch [ 46/ 60] Iter[ 31/ 84]\t\tLoss: 0.6357 \n",
            "| Epoch [ 46/ 60] Iter[ 36/ 84]\t\tLoss: 0.7469 \n",
            "| Epoch [ 46/ 60] Iter[ 41/ 84]\t\tLoss: 0.1911 \n",
            "| Epoch [ 46/ 60] Iter[ 46/ 84]\t\tLoss: 1.0883 \n",
            "| Epoch [ 46/ 60] Iter[ 51/ 84]\t\tLoss: 0.3257 \n",
            "| Epoch [ 46/ 60] Iter[ 56/ 84]\t\tLoss: 6.5160 \n",
            "| Epoch [ 46/ 60] Iter[ 61/ 84]\t\tLoss: 0.1620 \n",
            "| Epoch [ 46/ 60] Iter[ 66/ 84]\t\tLoss: 0.2490 \n",
            "| Epoch [ 46/ 60] Iter[ 71/ 84]\t\tLoss: 0.3614 \n",
            "| Epoch [ 46/ 60] Iter[ 76/ 84]\t\tLoss: 0.4756 \n",
            "| Epoch [ 46/ 60] Iter[ 81/ 84]\t\tLoss: 0.5109 \n",
            "\n",
            "| Validation Epoch #46\t\t\tLoss: 0.2269 \n",
            "\n",
            "=> Training Epoch #47, LR=0.0000\n",
            "| Epoch [ 47/ 60] Iter[  1/ 84]\t\tLoss: 0.3353 \n",
            "| Epoch [ 47/ 60] Iter[  6/ 84]\t\tLoss: 0.9669 \n",
            "| Epoch [ 47/ 60] Iter[ 11/ 84]\t\tLoss: 5.9088 \n",
            "| Epoch [ 47/ 60] Iter[ 16/ 84]\t\tLoss: 1.0047 \n",
            "| Epoch [ 47/ 60] Iter[ 21/ 84]\t\tLoss: 0.2792 \n",
            "| Epoch [ 47/ 60] Iter[ 26/ 84]\t\tLoss: 12.3897 \n",
            "| Epoch [ 47/ 60] Iter[ 31/ 84]\t\tLoss: 1.3316 \n",
            "| Epoch [ 47/ 60] Iter[ 36/ 84]\t\tLoss: 0.5336 \n",
            "| Epoch [ 47/ 60] Iter[ 41/ 84]\t\tLoss: 5.3859 \n",
            "| Epoch [ 47/ 60] Iter[ 46/ 84]\t\tLoss: 0.4459 \n",
            "| Epoch [ 47/ 60] Iter[ 51/ 84]\t\tLoss: 0.2327 \n",
            "| Epoch [ 47/ 60] Iter[ 56/ 84]\t\tLoss: 0.2677 \n",
            "| Epoch [ 47/ 60] Iter[ 61/ 84]\t\tLoss: 0.3062 \n",
            "| Epoch [ 47/ 60] Iter[ 66/ 84]\t\tLoss: 0.5729 \n",
            "| Epoch [ 47/ 60] Iter[ 71/ 84]\t\tLoss: 0.0661 \n",
            "| Epoch [ 47/ 60] Iter[ 76/ 84]\t\tLoss: 0.2541 \n",
            "| Epoch [ 47/ 60] Iter[ 81/ 84]\t\tLoss: 1.3300 \n",
            "\n",
            "| Validation Epoch #47\t\t\tLoss: 0.2267 \n",
            "\n",
            "=> Training Epoch #48, LR=0.0000\n",
            "| Epoch [ 48/ 60] Iter[  1/ 84]\t\tLoss: 3.6229 \n",
            "| Epoch [ 48/ 60] Iter[  6/ 84]\t\tLoss: 0.1359 \n",
            "| Epoch [ 48/ 60] Iter[ 11/ 84]\t\tLoss: 6.2756 \n",
            "| Epoch [ 48/ 60] Iter[ 16/ 84]\t\tLoss: 1.5479 \n",
            "| Epoch [ 48/ 60] Iter[ 21/ 84]\t\tLoss: 0.4152 \n",
            "| Epoch [ 48/ 60] Iter[ 26/ 84]\t\tLoss: 0.8177 \n",
            "| Epoch [ 48/ 60] Iter[ 31/ 84]\t\tLoss: 0.5625 \n",
            "| Epoch [ 48/ 60] Iter[ 36/ 84]\t\tLoss: 0.6990 \n",
            "| Epoch [ 48/ 60] Iter[ 41/ 84]\t\tLoss: 0.9456 \n",
            "| Epoch [ 48/ 60] Iter[ 46/ 84]\t\tLoss: 0.2082 \n",
            "| Epoch [ 48/ 60] Iter[ 51/ 84]\t\tLoss: 6.0950 \n",
            "| Epoch [ 48/ 60] Iter[ 56/ 84]\t\tLoss: 0.3695 \n",
            "| Epoch [ 48/ 60] Iter[ 61/ 84]\t\tLoss: 0.5617 \n",
            "| Epoch [ 48/ 60] Iter[ 66/ 84]\t\tLoss: 0.3459 \n",
            "| Epoch [ 48/ 60] Iter[ 71/ 84]\t\tLoss: 0.9244 \n",
            "| Epoch [ 48/ 60] Iter[ 76/ 84]\t\tLoss: 0.1980 \n",
            "| Epoch [ 48/ 60] Iter[ 81/ 84]\t\tLoss: 0.2063 \n",
            "\n",
            "| Validation Epoch #48\t\t\tLoss: 0.2270 \n",
            "\n",
            "=> Training Epoch #49, LR=0.0000\n",
            "| Epoch [ 49/ 60] Iter[  1/ 84]\t\tLoss: 0.1457 \n",
            "| Epoch [ 49/ 60] Iter[  6/ 84]\t\tLoss: 0.6504 \n",
            "| Epoch [ 49/ 60] Iter[ 11/ 84]\t\tLoss: 7.1490 \n",
            "| Epoch [ 49/ 60] Iter[ 16/ 84]\t\tLoss: 0.2671 \n",
            "| Epoch [ 49/ 60] Iter[ 21/ 84]\t\tLoss: 0.4261 \n",
            "| Epoch [ 49/ 60] Iter[ 26/ 84]\t\tLoss: 0.2011 \n",
            "| Epoch [ 49/ 60] Iter[ 31/ 84]\t\tLoss: 1.2281 \n",
            "| Epoch [ 49/ 60] Iter[ 36/ 84]\t\tLoss: 0.4363 \n",
            "| Epoch [ 49/ 60] Iter[ 41/ 84]\t\tLoss: 1.0990 \n",
            "| Epoch [ 49/ 60] Iter[ 46/ 84]\t\tLoss: 0.3984 \n",
            "| Epoch [ 49/ 60] Iter[ 51/ 84]\t\tLoss: 0.2873 \n",
            "| Epoch [ 49/ 60] Iter[ 56/ 84]\t\tLoss: 0.7769 \n",
            "| Epoch [ 49/ 60] Iter[ 61/ 84]\t\tLoss: 6.3199 \n",
            "| Epoch [ 49/ 60] Iter[ 66/ 84]\t\tLoss: 3.6229 \n",
            "| Epoch [ 49/ 60] Iter[ 71/ 84]\t\tLoss: 6.2442 \n",
            "| Epoch [ 49/ 60] Iter[ 76/ 84]\t\tLoss: 1.0072 \n",
            "| Epoch [ 49/ 60] Iter[ 81/ 84]\t\tLoss: 0.4599 \n",
            "\n",
            "| Validation Epoch #49\t\t\tLoss: 0.2274 \n",
            "\n",
            "=> Training Epoch #50, LR=0.0000\n",
            "| Epoch [ 50/ 60] Iter[  1/ 84]\t\tLoss: 0.5151 \n",
            "| Epoch [ 50/ 60] Iter[  6/ 84]\t\tLoss: 0.1521 \n",
            "| Epoch [ 50/ 60] Iter[ 11/ 84]\t\tLoss: 0.0568 \n",
            "| Epoch [ 50/ 60] Iter[ 16/ 84]\t\tLoss: 6.4222 \n",
            "| Epoch [ 50/ 60] Iter[ 21/ 84]\t\tLoss: 0.5438 \n",
            "| Epoch [ 50/ 60] Iter[ 26/ 84]\t\tLoss: 0.3933 \n",
            "| Epoch [ 50/ 60] Iter[ 31/ 84]\t\tLoss: 6.1767 \n",
            "| Epoch [ 50/ 60] Iter[ 36/ 84]\t\tLoss: 0.0339 \n",
            "| Epoch [ 50/ 60] Iter[ 41/ 84]\t\tLoss: 0.8427 \n",
            "| Epoch [ 50/ 60] Iter[ 46/ 84]\t\tLoss: 0.6065 \n",
            "| Epoch [ 50/ 60] Iter[ 51/ 84]\t\tLoss: 0.8079 \n",
            "| Epoch [ 50/ 60] Iter[ 56/ 84]\t\tLoss: 0.2813 \n",
            "| Epoch [ 50/ 60] Iter[ 61/ 84]\t\tLoss: 0.4148 \n",
            "| Epoch [ 50/ 60] Iter[ 66/ 84]\t\tLoss: 0.2454 \n",
            "| Epoch [ 50/ 60] Iter[ 71/ 84]\t\tLoss: 0.4814 \n",
            "| Epoch [ 50/ 60] Iter[ 76/ 84]\t\tLoss: 0.4013 \n",
            "| Epoch [ 50/ 60] Iter[ 81/ 84]\t\tLoss: 1.0714 \n",
            "\n",
            "| Validation Epoch #50\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #51, LR=0.0000\n",
            "| Epoch [ 51/ 60] Iter[  1/ 84]\t\tLoss: 0.3891 \n",
            "| Epoch [ 51/ 60] Iter[  6/ 84]\t\tLoss: 0.2545 \n",
            "| Epoch [ 51/ 60] Iter[ 11/ 84]\t\tLoss: 0.5539 \n",
            "| Epoch [ 51/ 60] Iter[ 16/ 84]\t\tLoss: 0.5171 \n",
            "| Epoch [ 51/ 60] Iter[ 21/ 84]\t\tLoss: 1.5387 \n",
            "| Epoch [ 51/ 60] Iter[ 26/ 84]\t\tLoss: 0.1848 \n",
            "| Epoch [ 51/ 60] Iter[ 31/ 84]\t\tLoss: 0.3342 \n",
            "| Epoch [ 51/ 60] Iter[ 36/ 84]\t\tLoss: 0.4276 \n",
            "| Epoch [ 51/ 60] Iter[ 41/ 84]\t\tLoss: 0.4687 \n",
            "| Epoch [ 51/ 60] Iter[ 46/ 84]\t\tLoss: 1.2390 \n",
            "| Epoch [ 51/ 60] Iter[ 51/ 84]\t\tLoss: 0.3989 \n",
            "| Epoch [ 51/ 60] Iter[ 56/ 84]\t\tLoss: 0.0585 \n",
            "| Epoch [ 51/ 60] Iter[ 61/ 84]\t\tLoss: 1.5083 \n",
            "| Epoch [ 51/ 60] Iter[ 66/ 84]\t\tLoss: 1.0101 \n",
            "| Epoch [ 51/ 60] Iter[ 71/ 84]\t\tLoss: 0.8574 \n",
            "| Epoch [ 51/ 60] Iter[ 76/ 84]\t\tLoss: 0.9735 \n",
            "| Epoch [ 51/ 60] Iter[ 81/ 84]\t\tLoss: 0.3348 \n",
            "\n",
            "| Validation Epoch #51\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #52, LR=0.0000\n",
            "| Epoch [ 52/ 60] Iter[  1/ 84]\t\tLoss: 1.1961 \n",
            "| Epoch [ 52/ 60] Iter[  6/ 84]\t\tLoss: 7.2223 \n",
            "| Epoch [ 52/ 60] Iter[ 11/ 84]\t\tLoss: 0.0823 \n",
            "| Epoch [ 52/ 60] Iter[ 16/ 84]\t\tLoss: 0.5010 \n",
            "| Epoch [ 52/ 60] Iter[ 21/ 84]\t\tLoss: 6.3527 \n",
            "| Epoch [ 52/ 60] Iter[ 26/ 84]\t\tLoss: 0.3515 \n",
            "| Epoch [ 52/ 60] Iter[ 31/ 84]\t\tLoss: 0.7276 \n",
            "| Epoch [ 52/ 60] Iter[ 36/ 84]\t\tLoss: 0.4279 \n",
            "| Epoch [ 52/ 60] Iter[ 41/ 84]\t\tLoss: 0.2525 \n",
            "| Epoch [ 52/ 60] Iter[ 46/ 84]\t\tLoss: 0.2576 \n",
            "| Epoch [ 52/ 60] Iter[ 51/ 84]\t\tLoss: 1.5907 \n",
            "| Epoch [ 52/ 60] Iter[ 56/ 84]\t\tLoss: 0.8910 \n",
            "| Epoch [ 52/ 60] Iter[ 61/ 84]\t\tLoss: 0.4150 \n",
            "| Epoch [ 52/ 60] Iter[ 66/ 84]\t\tLoss: 0.3862 \n",
            "| Epoch [ 52/ 60] Iter[ 71/ 84]\t\tLoss: 0.2548 \n",
            "| Epoch [ 52/ 60] Iter[ 76/ 84]\t\tLoss: 0.4613 \n",
            "| Epoch [ 52/ 60] Iter[ 81/ 84]\t\tLoss: 6.3803 \n",
            "\n",
            "| Validation Epoch #52\t\t\tLoss: 0.2272 \n",
            "\n",
            "=> Training Epoch #53, LR=0.0000\n",
            "| Epoch [ 53/ 60] Iter[  1/ 84]\t\tLoss: 0.2445 \n",
            "| Epoch [ 53/ 60] Iter[  6/ 84]\t\tLoss: 0.3345 \n",
            "| Epoch [ 53/ 60] Iter[ 11/ 84]\t\tLoss: 0.3203 \n",
            "| Epoch [ 53/ 60] Iter[ 16/ 84]\t\tLoss: 0.7938 \n",
            "| Epoch [ 53/ 60] Iter[ 21/ 84]\t\tLoss: 0.5922 \n",
            "| Epoch [ 53/ 60] Iter[ 26/ 84]\t\tLoss: 0.0874 \n",
            "| Epoch [ 53/ 60] Iter[ 31/ 84]\t\tLoss: 0.4836 \n",
            "| Epoch [ 53/ 60] Iter[ 36/ 84]\t\tLoss: 0.6331 \n",
            "| Epoch [ 53/ 60] Iter[ 41/ 84]\t\tLoss: 0.4726 \n",
            "| Epoch [ 53/ 60] Iter[ 46/ 84]\t\tLoss: 1.1309 \n",
            "| Epoch [ 53/ 60] Iter[ 51/ 84]\t\tLoss: 0.9970 \n",
            "| Epoch [ 53/ 60] Iter[ 56/ 84]\t\tLoss: 0.4562 \n",
            "| Epoch [ 53/ 60] Iter[ 61/ 84]\t\tLoss: 6.0459 \n",
            "| Epoch [ 53/ 60] Iter[ 66/ 84]\t\tLoss: 0.2234 \n",
            "| Epoch [ 53/ 60] Iter[ 71/ 84]\t\tLoss: 0.1613 \n",
            "| Epoch [ 53/ 60] Iter[ 76/ 84]\t\tLoss: 0.2480 \n",
            "| Epoch [ 53/ 60] Iter[ 81/ 84]\t\tLoss: 0.8525 \n",
            "\n",
            "| Validation Epoch #53\t\t\tLoss: 0.2272 \n",
            "\n",
            "=> Training Epoch #54, LR=0.0000\n",
            "| Epoch [ 54/ 60] Iter[  1/ 84]\t\tLoss: 0.5392 \n",
            "| Epoch [ 54/ 60] Iter[  6/ 84]\t\tLoss: 0.6495 \n",
            "| Epoch [ 54/ 60] Iter[ 11/ 84]\t\tLoss: 0.4995 \n",
            "| Epoch [ 54/ 60] Iter[ 16/ 84]\t\tLoss: 0.9097 \n",
            "| Epoch [ 54/ 60] Iter[ 21/ 84]\t\tLoss: 0.6051 \n",
            "| Epoch [ 54/ 60] Iter[ 26/ 84]\t\tLoss: 0.5707 \n",
            "| Epoch [ 54/ 60] Iter[ 31/ 84]\t\tLoss: 0.5179 \n",
            "| Epoch [ 54/ 60] Iter[ 36/ 84]\t\tLoss: 0.2511 \n",
            "| Epoch [ 54/ 60] Iter[ 41/ 84]\t\tLoss: 0.0626 \n",
            "| Epoch [ 54/ 60] Iter[ 46/ 84]\t\tLoss: 0.6179 \n",
            "| Epoch [ 54/ 60] Iter[ 51/ 84]\t\tLoss: 5.7286 \n",
            "| Epoch [ 54/ 60] Iter[ 56/ 84]\t\tLoss: 0.8338 \n",
            "| Epoch [ 54/ 60] Iter[ 61/ 84]\t\tLoss: 1.1169 \n",
            "| Epoch [ 54/ 60] Iter[ 66/ 84]\t\tLoss: 0.5973 \n",
            "| Epoch [ 54/ 60] Iter[ 71/ 84]\t\tLoss: 0.7641 \n",
            "| Epoch [ 54/ 60] Iter[ 76/ 84]\t\tLoss: 0.2656 \n",
            "| Epoch [ 54/ 60] Iter[ 81/ 84]\t\tLoss: 0.9844 \n",
            "\n",
            "| Validation Epoch #54\t\t\tLoss: 0.2272 \n",
            "\n",
            "=> Training Epoch #55, LR=0.0000\n",
            "| Epoch [ 55/ 60] Iter[  1/ 84]\t\tLoss: 0.2599 \n",
            "| Epoch [ 55/ 60] Iter[  6/ 84]\t\tLoss: 6.1503 \n",
            "| Epoch [ 55/ 60] Iter[ 11/ 84]\t\tLoss: 0.4093 \n",
            "| Epoch [ 55/ 60] Iter[ 16/ 84]\t\tLoss: 0.5860 \n",
            "| Epoch [ 55/ 60] Iter[ 21/ 84]\t\tLoss: 0.8137 \n",
            "| Epoch [ 55/ 60] Iter[ 26/ 84]\t\tLoss: 0.8627 \n",
            "| Epoch [ 55/ 60] Iter[ 31/ 84]\t\tLoss: 0.4834 \n",
            "| Epoch [ 55/ 60] Iter[ 36/ 84]\t\tLoss: 0.8140 \n",
            "| Epoch [ 55/ 60] Iter[ 41/ 84]\t\tLoss: 6.2591 \n",
            "| Epoch [ 55/ 60] Iter[ 46/ 84]\t\tLoss: 0.3796 \n",
            "| Epoch [ 55/ 60] Iter[ 51/ 84]\t\tLoss: 0.6374 \n",
            "| Epoch [ 55/ 60] Iter[ 56/ 84]\t\tLoss: 0.0833 \n",
            "| Epoch [ 55/ 60] Iter[ 61/ 84]\t\tLoss: 1.1358 \n",
            "| Epoch [ 55/ 60] Iter[ 66/ 84]\t\tLoss: 0.1506 \n",
            "| Epoch [ 55/ 60] Iter[ 71/ 84]\t\tLoss: 1.6632 \n",
            "| Epoch [ 55/ 60] Iter[ 76/ 84]\t\tLoss: 0.2104 \n",
            "| Epoch [ 55/ 60] Iter[ 81/ 84]\t\tLoss: 0.3357 \n",
            "\n",
            "| Validation Epoch #55\t\t\tLoss: 0.2272 \n",
            "\n",
            "=> Training Epoch #56, LR=0.0000\n",
            "| Epoch [ 56/ 60] Iter[  1/ 84]\t\tLoss: 0.9659 \n",
            "| Epoch [ 56/ 60] Iter[  6/ 84]\t\tLoss: 0.3135 \n",
            "| Epoch [ 56/ 60] Iter[ 11/ 84]\t\tLoss: 0.9859 \n",
            "| Epoch [ 56/ 60] Iter[ 16/ 84]\t\tLoss: 0.4118 \n",
            "| Epoch [ 56/ 60] Iter[ 21/ 84]\t\tLoss: 0.2350 \n",
            "| Epoch [ 56/ 60] Iter[ 26/ 84]\t\tLoss: 0.3596 \n",
            "| Epoch [ 56/ 60] Iter[ 31/ 84]\t\tLoss: 0.7323 \n",
            "| Epoch [ 56/ 60] Iter[ 36/ 84]\t\tLoss: 0.8092 \n",
            "| Epoch [ 56/ 60] Iter[ 41/ 84]\t\tLoss: 0.5610 \n",
            "| Epoch [ 56/ 60] Iter[ 46/ 84]\t\tLoss: 0.4588 \n",
            "| Epoch [ 56/ 60] Iter[ 51/ 84]\t\tLoss: 0.9219 \n",
            "| Epoch [ 56/ 60] Iter[ 56/ 84]\t\tLoss: 5.6224 \n",
            "| Epoch [ 56/ 60] Iter[ 61/ 84]\t\tLoss: 0.3706 \n",
            "| Epoch [ 56/ 60] Iter[ 66/ 84]\t\tLoss: 0.3484 \n",
            "| Epoch [ 56/ 60] Iter[ 71/ 84]\t\tLoss: 0.3027 \n",
            "| Epoch [ 56/ 60] Iter[ 76/ 84]\t\tLoss: 0.4555 \n",
            "| Epoch [ 56/ 60] Iter[ 81/ 84]\t\tLoss: 0.4460 \n",
            "\n",
            "| Validation Epoch #56\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #57, LR=0.0000\n",
            "| Epoch [ 57/ 60] Iter[  1/ 84]\t\tLoss: 0.0513 \n",
            "| Epoch [ 57/ 60] Iter[  6/ 84]\t\tLoss: 0.9059 \n",
            "| Epoch [ 57/ 60] Iter[ 11/ 84]\t\tLoss: 6.5347 \n",
            "| Epoch [ 57/ 60] Iter[ 16/ 84]\t\tLoss: 0.2585 \n",
            "| Epoch [ 57/ 60] Iter[ 21/ 84]\t\tLoss: 0.2098 \n",
            "| Epoch [ 57/ 60] Iter[ 26/ 84]\t\tLoss: 0.5303 \n",
            "| Epoch [ 57/ 60] Iter[ 31/ 84]\t\tLoss: 1.0616 \n",
            "| Epoch [ 57/ 60] Iter[ 36/ 84]\t\tLoss: 0.1717 \n",
            "| Epoch [ 57/ 60] Iter[ 41/ 84]\t\tLoss: 0.6548 \n",
            "| Epoch [ 57/ 60] Iter[ 46/ 84]\t\tLoss: 0.3948 \n",
            "| Epoch [ 57/ 60] Iter[ 51/ 84]\t\tLoss: 1.3518 \n",
            "| Epoch [ 57/ 60] Iter[ 56/ 84]\t\tLoss: 0.4240 \n",
            "| Epoch [ 57/ 60] Iter[ 61/ 84]\t\tLoss: 2.5995 \n",
            "| Epoch [ 57/ 60] Iter[ 66/ 84]\t\tLoss: 0.5691 \n",
            "| Epoch [ 57/ 60] Iter[ 71/ 84]\t\tLoss: 6.1573 \n",
            "| Epoch [ 57/ 60] Iter[ 76/ 84]\t\tLoss: 0.9617 \n",
            "| Epoch [ 57/ 60] Iter[ 81/ 84]\t\tLoss: 0.2330 \n",
            "\n",
            "| Validation Epoch #57\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #58, LR=0.0000\n",
            "| Epoch [ 58/ 60] Iter[  1/ 84]\t\tLoss: 0.1002 \n",
            "| Epoch [ 58/ 60] Iter[  6/ 84]\t\tLoss: 0.2438 \n",
            "| Epoch [ 58/ 60] Iter[ 11/ 84]\t\tLoss: 0.5755 \n",
            "| Epoch [ 58/ 60] Iter[ 16/ 84]\t\tLoss: 0.6544 \n",
            "| Epoch [ 58/ 60] Iter[ 21/ 84]\t\tLoss: 1.0592 \n",
            "| Epoch [ 58/ 60] Iter[ 26/ 84]\t\tLoss: 0.2567 \n",
            "| Epoch [ 58/ 60] Iter[ 31/ 84]\t\tLoss: 6.2643 \n",
            "| Epoch [ 58/ 60] Iter[ 36/ 84]\t\tLoss: 1.0913 \n",
            "| Epoch [ 58/ 60] Iter[ 41/ 84]\t\tLoss: 0.5169 \n",
            "| Epoch [ 58/ 60] Iter[ 46/ 84]\t\tLoss: 0.4441 \n",
            "| Epoch [ 58/ 60] Iter[ 51/ 84]\t\tLoss: 6.3731 \n",
            "| Epoch [ 58/ 60] Iter[ 56/ 84]\t\tLoss: 0.1317 \n",
            "| Epoch [ 58/ 60] Iter[ 61/ 84]\t\tLoss: 0.1503 \n",
            "| Epoch [ 58/ 60] Iter[ 66/ 84]\t\tLoss: 0.5372 \n",
            "| Epoch [ 58/ 60] Iter[ 71/ 84]\t\tLoss: 2.7114 \n",
            "| Epoch [ 58/ 60] Iter[ 76/ 84]\t\tLoss: 6.2528 \n",
            "| Epoch [ 58/ 60] Iter[ 81/ 84]\t\tLoss: 0.3630 \n",
            "\n",
            "| Validation Epoch #58\t\t\tLoss: 0.2273 \n",
            "\n",
            "=> Training Epoch #59, LR=0.0000\n",
            "| Epoch [ 59/ 60] Iter[  1/ 84]\t\tLoss: 0.7389 \n",
            "| Epoch [ 59/ 60] Iter[  6/ 84]\t\tLoss: 6.0804 \n",
            "| Epoch [ 59/ 60] Iter[ 11/ 84]\t\tLoss: 0.0798 \n",
            "| Epoch [ 59/ 60] Iter[ 16/ 84]\t\tLoss: 0.5055 \n",
            "| Epoch [ 59/ 60] Iter[ 21/ 84]\t\tLoss: 0.4402 \n",
            "| Epoch [ 59/ 60] Iter[ 26/ 84]\t\tLoss: 0.8583 \n",
            "| Epoch [ 59/ 60] Iter[ 31/ 84]\t\tLoss: 1.3272 \n",
            "| Epoch [ 59/ 60] Iter[ 36/ 84]\t\tLoss: 1.0480 \n",
            "| Epoch [ 59/ 60] Iter[ 41/ 84]\t\tLoss: 0.7388 \n",
            "| Epoch [ 59/ 60] Iter[ 46/ 84]\t\tLoss: 5.7107 \n",
            "| Epoch [ 59/ 60] Iter[ 51/ 84]\t\tLoss: 3.9608 \n",
            "| Epoch [ 59/ 60] Iter[ 56/ 84]\t\tLoss: 1.0492 \n",
            "| Epoch [ 59/ 60] Iter[ 61/ 84]\t\tLoss: 0.4033 \n",
            "| Epoch [ 59/ 60] Iter[ 66/ 84]\t\tLoss: 0.5610 \n",
            "| Epoch [ 59/ 60] Iter[ 71/ 84]\t\tLoss: 0.3574 \n",
            "| Epoch [ 59/ 60] Iter[ 76/ 84]\t\tLoss: 0.6613 \n",
            "| Epoch [ 59/ 60] Iter[ 81/ 84]\t\tLoss: 1.0945 \n",
            "\n",
            "| Validation Epoch #59\t\t\tLoss: 0.2273 \n",
            "O valor médio para o caso de 50 treinamentos é: 1.60%\n",
            "O valor médio para o caso de 50 treinamentos é: 2.26%\n",
            "O valor médio para o caso de 100 treinamentos é: 1.69%\n",
            "O valor médio para o caso de 100 treinamentos é: 3.95%\n",
            "O valor médio para o caso de 150 treinamentos é: 1.32%\n",
            "O valor médio para o caso de 150 treinamentos é: 2.09%\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_epochs=60\n",
        "#lr=0.001\n",
        "loss_training=[]\n",
        "loss_test=[]\n",
        "def weights_init_uniform(m):\n",
        "      classname = m.__class__.__name__\n",
        "      # for every Linear layer in a model..\n",
        "      if classname.find('Linear') != -1:\n",
        "          # apply a uniform distribution to the weights and a bias=0\n",
        "          m.weight.data.uniform_(0.0, 1.0)\n",
        "          m.bias.data.fill_(0)\n",
        "\n",
        "rede_neural_150 = SimpleNetwork()\n",
        "rede_neural_150.apply(weights_init_uniform)\n",
        "#rede_neural_150= rede_neural_150.cuda()\n",
        "\n",
        "rede_neural_100 = SimpleNetwork()\n",
        "rede_neural_100.apply(weights_init_uniform)\n",
        "#rede_neural_100= rede_neural_100.cuda()\n",
        "\n",
        "rede_neural_50 = SimpleNetwork()\n",
        "rede_neural_50.apply(weights_init_uniform)\n",
        "#rede_neural_50= rede_neural_50.cuda()\n",
        "\n",
        "# x_train_data=torch.tensor(x_train_data, dtype=torch.float32)\n",
        "# y_train_data=torch.tensor(y_train_data, dtype=torch.float32)\n",
        "\n",
        "#optimizer = optim.Adam(rede_neural_150.parameters(), lr)\n",
        "for epoch in range(n_epochs):\n",
        "    #rede_neural_150.train()\n",
        "    loss_training_value,__=train(epoch,rede_neural_150,train_loader_150)\n",
        "    __,loss_test_value =test(epoch,rede_neural_150,test_loader)\n",
        "    # y_train_prediction=rede_neural_150(x_train_data)\n",
        "    # loss=criterion(y_train_prediction,y_train_data)\n",
        "    # optimizer.zero_grad()\n",
        "    # loss.backward()\n",
        "    # optimizer.step()\n",
        "    # if epoch%10 == 0 :\n",
        "    #   print(f'[epoch:{epoch}]: The loss value for training part={loss}')\n",
        "\n",
        "    loss_training.append(loss_training_value)\n",
        "    loss_test.append(loss_test_value)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "\n",
        "    loss_training_value,__=train(epoch,rede_neural_100,train_loader_100)\n",
        "    __,loss_test_value =test(epoch,rede_neural_100,test_loader)\n",
        "\n",
        "\n",
        "\n",
        "    loss_training.append(loss_training_value)\n",
        "    loss_test.append(loss_test_value)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    loss_training_value,__=train(epoch,rede_neural_50,train_loader_50)\n",
        "    __,loss_test_value =test(epoch,rede_neural_50,test_loader)\n",
        "\n",
        "    loss_training.append(loss_training_value)\n",
        "    loss_test.append(loss_test_value)\n",
        "\n",
        "\n",
        "y150_net_predict = rede_neural_150(torch.from_numpy(x_test_data).to(torch.float32)).cpu().detach().numpy()\n",
        "#y150_net_predict = rede_neural_150(torch.tensor(x_test_data,dtype=torch.float32))\n",
        "#print(y150_net_predict)\n",
        "y150_net_predict_fopt=np.transpose(y150_net_predict)[0]\n",
        "y150_net_predict_fwpt=np.transpose(y150_net_predict)[1]\n",
        "\n",
        "\n",
        "y100_net_predict = rede_neural_100(torch.from_numpy(x_test_data).to(torch.float32)).cpu().detach().numpy()\n",
        "y100_net_predict_fopt=y100_net_predict.transpose()[0]\n",
        "y100_net_predict_fwpt=y100_net_predict.transpose()[1]\n",
        "\n",
        "y50_net_predict = rede_neural_50(torch.from_numpy(x_test_data).to(torch.float32)).cpu().detach().numpy()\n",
        "y50_net_predict_fopt=y50_net_predict.transpose()[0]\n",
        "y50_net_predict_fwpt=y50_net_predict.transpose()[1]\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,4.8))\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "axs[0].scatter(np.sort(y_test_fopt)[10:],np.arange(len(np.sort(y_test_fopt)))[10:]/float(len(np.sort(y_test_fopt))), label='sim',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(y50_net_predict_fopt),np.arange(len(np.sort(y50_net_predict_fopt)))/float(len(np.sort(y50_net_predict_fopt))), label='t50',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(y100_net_predict_fopt),np.arange(len(np.sort(y100_net_predict_fopt)))/float(len(np.sort(y100_net_predict_fopt))), label='t100',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(y150_net_predict_fopt),np.arange(len(np.sort(y150_net_predict_fopt)))/float(len(np.sort(y150_net_predict_fopt))), label='t150',linewidths=0.3)\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "axs[0].set_xlabel(\"FOPT \")\n",
        "axs[0].set_ylabel(\"CDF\")\n",
        "#axs[0].set_ylim([0,1.02])\n",
        "#axs[0].set_xlim([min(y_test_fopt),-min(y_test_fopt)])\n",
        "\n",
        "#axs[0].set_xlim([-1,1])\n",
        "axs[0].set_title(\"CDF da Prod. Total de petróleo\")\n",
        "\n",
        "\n",
        "axs[1].scatter(np.sort(y_test_fwpt)[10:],np.arange(len(np.sort(y_test_fwpt)))[10:]/float(len(np.sort(y_test_fwpt))), label='sim',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(y50_net_predict_fwpt),np.arange(len(np.sort(y50_net_predict_fwpt)))/float(len(np.sort(y50_net_predict_fwpt))), label='t50',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(y100_net_predict_fwpt),np.arange(len(np.sort(y100_net_predict_fwpt)))/float(len(np.sort(y100_net_predict_fwpt))), label='t100',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(y150_net_predict_fwpt),np.arange(len(np.sort(y150_net_predict_fwpt)))/float(len(np.sort(y150_net_predict_fwpt))), label='t150',linewidths=0.3)\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "axs[1].set_xlabel(\"FWPT \")\n",
        "axs[1].set_ylabel(\"CDF\")\n",
        "#axs[1].set_ylim([0,1.02])\n",
        "\n",
        "axs[1].set_xlim([-max(y_test_fwpt),max(y_test_fwpt)])\n",
        "axs[1].set_title(\"CDF da Prod. Total de água\")\n",
        "\n",
        "RE_50_fopt =np.abs( np.abs(y_test_fopt - y50_net_predict_fopt) / y_test_fopt)\n",
        "RE_100_fopt = np.abs(np.abs(y_test_fopt -  y100_net_predict_fopt) / y_test_fopt)\n",
        "RE_150_fopt =np.abs( np.abs(y_test_fopt -  y150_net_predict_fopt) / y_test_fopt)\n",
        "\n",
        "RE_50_fwpt =np.abs( np.abs(y_test_fwpt -  y50_net_predict_fwpt) / y_test_fwpt)\n",
        "RE_100_fwpt = np.abs(np.abs(y_test_fwpt - y100_net_predict_fwpt) / y_test_fwpt)\n",
        "RE_150_fwpt =np.abs( np.abs(y_test_fwpt - y150_net_predict_fwpt) / y_test_fwpt)\n",
        "\n",
        "\n",
        "print('O valor médio para o caso de 50 treinamentos é: {:.2f}%'.format(np.mean(RE_50_fopt)))\n",
        "print('O valor médio para o caso de 50 treinamentos é: {:.2f}%'.format(np.mean(RE_50_fwpt)))\n",
        "\n",
        "print('O valor médio para o caso de 100 treinamentos é: {:.2f}%'.format(np.mean(RE_100_fopt)))\n",
        "print('O valor médio para o caso de 100 treinamentos é: {:.2f}%'.format(np.mean(RE_100_fwpt)))\n",
        "\n",
        "print('O valor médio para o caso de 150 treinamentos é: {:.2f}%'.format(np.mean(RE_150_fopt)))\n",
        "print('O valor médio para o caso de 150 treinamentos é: {:.2f}%'.format(np.mean(RE_150_fwpt)))\n",
        "\n",
        "\n",
        "# y150_net_predict = rede_neural_150(torch.from_numpy(x_train_data).to(torch.float32).cuda()).cpu().detach().numpy()\n",
        "# print(y150_net_predict_fopt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 922,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "O valor médio para o caso de 50 treinamentos é: 1.60%\n",
            "O valor médio para o caso de 50 treinamentos é: 2.26%\n",
            "500\n",
            "teste [[-0.69050201  0.68648865  0.62472514 ...  1.4430374   1.42401904\n",
            "   1.4430374 ]\n",
            " [ 1.29687186  0.74110498  0.80056784 ...  0.73462434  0.77182503\n",
            "   0.73462434]\n",
            " [ 1.1864622  -1.40071169 -1.39746587 ... -1.39061484 -1.37109815\n",
            "  -1.39061484]\n",
            " ...\n",
            " [-1.02173098  0.74110498  0.80056784 ... -1.39061484 -1.37109815\n",
            "  -1.39061484]\n",
            " [-1.57377928  0.74110498  0.80056784 ...  0.02621128  0.02646044\n",
            "   0.02621128]\n",
            " [ 1.1864622  -1.40071169 -1.39746587 ...  0.73462434  0.77182503\n",
            "   0.73462434]]\n",
            "[ 0.77527663  0.19047397  0.38399086 ... -0.12361028  0.75641792\n",
            "  0.54658234]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print('O valor médio para o caso de 50 treinamentos é: {:.2f}%'.format(np.mean(RE_50_fopt)))\n",
        "print('O valor médio para o caso de 50 treinamentos é: {:.2f}%'.format(np.mean(RE_50_fwpt)))\n",
        "fig, axs = plt.subplots(1,2,figsize=(10,4.8))\n",
        "fig.tight_layout()\n",
        "axs[0].scatter(np.sort(y_test_fopt)[10:],np.arange(len(np.sort(y_test_fopt)))[10:]/float(len(np.sort(y_test_fopt))), label='sim',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(y50_net_predict_fopt),np.arange(len(np.sort(y50_net_predict_fopt)))/float(len(np.sort(y50_net_predict_fopt))), label='t50',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(y100_net_predict_fopt),np.arange(len(np.sort(y100_net_predict_fopt)))/float(len(np.sort(y100_net_predict_fopt))), label='t100',linewidths=0.3)\n",
        "axs[0].scatter(np.sort(y150_net_predict_fopt),np.arange(len(np.sort(y150_net_predict_fopt)))/float(len(np.sort(y150_net_predict_fopt))), label='t150',linewidths=0.3)\n",
        "axs[0].legend()\n",
        "axs[0].grid()\n",
        "axs[0].set_xlabel(\"FOPT \")\n",
        "axs[0].set_ylabel(\"CDF\")\n",
        "axs[0].set_ylim([0,1.02])\n",
        "axs[0].set_xlim([-2,2])\n",
        "axs[0].set_title(\"CDF da Prod. Total de petróleo\")\n",
        "\n",
        "axs[1].scatter(np.sort(y_test_fwpt)[10:],np.arange(len(np.sort(y_test_fwpt)))[10:]/float(len(np.sort(y_test_fwpt))), label='sim',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(y50_net_predict_fwpt),np.arange(len(np.sort(y50_net_predict_fwpt)))/float(len(np.sort(y50_net_predict_fwpt))), label='t50',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(y100_net_predict_fwpt),np.arange(len(np.sort(y100_net_predict_fwpt)))/float(len(np.sort(y100_net_predict_fwpt))), label='t100',linewidths=0.3)\n",
        "axs[1].scatter(np.sort(y150_net_predict_fwpt),np.arange(len(np.sort(y150_net_predict_fwpt)))/float(len(np.sort(y150_net_predict_fwpt))), label='t150',linewidths=0.3)\n",
        "#axs[1].ecdf(y150_net_predict_fwpt)\n",
        "axs[1].legend()\n",
        "axs[1].grid()\n",
        "axs[1].set_xlabel(\"FWPT \")\n",
        "axs[1].set_ylabel(\"CDF\")\n",
        "axs[1].set_ylim([0,1.02])\n",
        "\n",
        "axs[1].set_xlim([-max(y_test_fwpt),max(y_test_fwpt)])\n",
        "axs[1].set_title(\"CDF da Prod. Total de água\")\n",
        "print(qte_training)\n",
        "print('teste',x_data)\n",
        "print(y_test_fopt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
